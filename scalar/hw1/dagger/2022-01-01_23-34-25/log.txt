#!/bin/bash
# NOTE: This script is used for better grouping of experiment data, and the data 
# generated by this script does not follow the homework submission specifications.  

export PYTHONPATH=.

video_log_freq=5
eval_batch_size=10000
num_agent_train_steps_per_iter=5000
n_iter=10

DATA_DIR=./data
EXP_DIR=${DATA_DIR}/dagger/`date +%Y-%m-%d_%H-%M-%S`
LOG_PATH=${EXP_DIR}/log.txt
# clear other data files
rm -rf $(find ${DATA_DIR} -maxdepth 1 -name '*q2_*' 2> /dev/null)
# create data directory for this experiment and logfile
mkdir -p $EXP_DIR; touch $LOG_PATH
# dump experiment commands and hyperparameters (this file) to logfile
cat $0 >> $LOG_PATH; echo "\n\n" >> $LOG_PATH

for env in Ant Humanoid Walker2d HalfCheetah Hopper
do 
    python -m cs285.scripts.run_hw1 \
        --expert_policy_file cs285/policies/experts/${env}.pkl \
        --env_name ${env}-v2 --exp_name dagger_${env} --n_iter ${n_iter} \
        --do_dagger --expert_data cs285/expert_data/expert_data_${env}-v2.pkl \
        --eval_batch_size ${eval_batch_size} \
        --num_agent_train_steps_per_iter ${num_agent_train_steps_per_iter} \
        --video_log_freq ${video_log_freq} >> $LOG_PATH
done

mv $(find ${DATA_DIR} -maxdepth 1 -name '*q2_*' 2> /dev/null) $EXP_DIR



########################
logging outputs to  /home/zzh/Desktop/CS285/cs285-homework-fall2021/hw1/cs285/scripts/../../data/q2_dagger_Ant_Ant-v2_2022-01-01_23-34-26
########################
Using GPU id 0
Loading expert policy from... cs285/policies/experts/Ant.pkl
obs (1, 111) (1, 111)
Done restoring expert policy...


********** Iteration 0 ************

Collecting data to be used for training...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4677.927734375
Eval_StdReturn : 126.8599853515625
Eval_MaxReturn : 4909.62158203125
Eval_MinReturn : 4424.70947265625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4713.6533203125
Train_StdReturn : 12.196533203125
Train_MaxReturn : 4725.849609375
Train_MinReturn : 4701.45654296875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 12.155273675918579
Training Loss : -2.446242570877075
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4641.388671875
Eval_StdReturn : 146.99090576171875
Eval_MaxReturn : 4987.25830078125
Eval_MinReturn : 4388.2265625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4648.345703125
Train_StdReturn : 0.0
Train_MaxReturn : 4648.345703125
Train_MinReturn : 4648.345703125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 1000
TimeSinceStart : 23.907205820083618
Training Loss : -2.5832512378692627
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4778.4599609375
Eval_StdReturn : 72.35332489013672
Eval_MaxReturn : 4912.56640625
Eval_MinReturn : 4691.96875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4808.54443359375
Train_StdReturn : 0.0
Train_MaxReturn : 4808.54443359375
Train_MinReturn : 4808.54443359375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 2000
TimeSinceStart : 35.97342610359192
Training Loss : -2.3791513442993164
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4502.55517578125
Eval_StdReturn : 920.5220947265625
Eval_MaxReturn : 4999.84814453125
Eval_MinReturn : 1609.20654296875
Eval_AverageEpLen : 942.0909090909091
Train_AverageReturn : 4621.50146484375
Train_StdReturn : 0.0
Train_MaxReturn : 4621.50146484375
Train_MinReturn : 4621.50146484375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 3000
TimeSinceStart : 48.081841230392456
Training Loss : -2.6366963386535645
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4708.6337890625
Eval_StdReturn : 103.82581329345703
Eval_MaxReturn : 4925.35400390625
Eval_MinReturn : 4489.896484375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4950.9873046875
Train_StdReturn : 0.0
Train_MaxReturn : 4950.9873046875
Train_MinReturn : 4950.9873046875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 4000
TimeSinceStart : 60.29540967941284
Training Loss : -2.582793712615967
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...

Collecting train rollouts to be used for saving videos...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Collecting video rollouts eval

Saving train rollouts as videos...
Eval_AverageReturn : 4745.68603515625
Eval_StdReturn : 52.22336959838867
Eval_MaxReturn : 4845.44921875
Eval_MinReturn : 4689.615234375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4621.9521484375
Train_StdReturn : 0.0
Train_MaxReturn : 4621.9521484375
Train_MinReturn : 4621.9521484375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 5000
TimeSinceStart : 77.30693125724792
Training Loss : -2.597874641418457
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4309.40185546875
Eval_StdReturn : 1066.8944091796875
Eval_MaxReturn : 4912.9404296875
Eval_MinReturn : 1345.3369140625
Eval_AverageEpLen : 903.5
Train_AverageReturn : 4693.2041015625
Train_StdReturn : 0.0
Train_MaxReturn : 4693.2041015625
Train_MinReturn : 4693.2041015625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 6000
TimeSinceStart : 90.12623524665833
Training Loss : -2.538632392883301
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4450.03955078125
Eval_StdReturn : 500.6977844238281
Eval_MaxReturn : 4738.31787109375
Eval_MinReturn : 2905.739501953125
Eval_AverageEpLen : 964.5454545454545
Train_AverageReturn : 4460.34619140625
Train_StdReturn : 0.0
Train_MaxReturn : 4460.34619140625
Train_MinReturn : 4460.34619140625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 7000
TimeSinceStart : 102.90166640281677
Training Loss : -2.6104729175567627
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4707.8466796875
Eval_StdReturn : 116.02566528320312
Eval_MaxReturn : 4880.4267578125
Eval_MinReturn : 4432.5751953125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4682.44189453125
Train_StdReturn : 0.0
Train_MaxReturn : 4682.44189453125
Train_MinReturn : 4682.44189453125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 8000
TimeSinceStart : 115.0140974521637
Training Loss : -2.600330114364624
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4739.0126953125
Eval_StdReturn : 61.60883712768555
Eval_MaxReturn : 4827.97265625
Eval_MinReturn : 4604.33984375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4739.25
Train_StdReturn : 0.0
Train_MaxReturn : 4739.25
Train_MinReturn : 4739.25
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 9000
TimeSinceStart : 127.29545736312866
Training Loss : -2.4575753211975098
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...


Found 4 GPUs for rendering. Using device 0.
########################
logging outputs to  /home/zzh/Desktop/CS285/cs285-homework-fall2021/hw1/cs285/scripts/../../data/q2_dagger_Humanoid_Humanoid-v2_2022-01-01_23-36-37
########################
Using GPU id 0
Loading expert policy from... cs285/policies/experts/Humanoid.pkl
obs (1, 376) (1, 376)
Done restoring expert policy...


********** Iteration 0 ************

Collecting data to be used for training...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 291.07781982421875
Eval_StdReturn : 30.324705123901367
Eval_MaxReturn : 405.86663818359375
Eval_MinReturn : 224.61758422851562
Eval_AverageEpLen : 53.833333333333336
Train_AverageReturn : 10344.517578125
Train_StdReturn : 20.9814453125
Train_MaxReturn : 10365.4990234375
Train_MinReturn : 10323.5361328125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 13.765896320343018
Training Loss : 0.1651475578546524
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 353.3219299316406
Eval_StdReturn : 104.70117950439453
Eval_MaxReturn : 905.2037353515625
Eval_MinReturn : 172.98147583007812
Eval_AverageEpLen : 65.66666666666667
Train_AverageReturn : 286.3415832519531
Train_StdReturn : 23.491863250732422
Train_MaxReturn : 344.14471435546875
Train_MinReturn : 238.34652709960938
Train_AverageEpLen : 52.73684210526316
Train_EnvstepsSoFar : 1002
TimeSinceStart : 27.381996631622314
Training Loss : 0.3759532868862152
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 313.457275390625
Eval_StdReturn : 23.59294891357422
Eval_MaxReturn : 390.4229431152344
Eval_MinReturn : 254.58883666992188
Eval_AverageEpLen : 57.47428571428571
Train_AverageReturn : 374.6206359863281
Train_StdReturn : 118.06147003173828
Train_MaxReturn : 738.8174438476562
Train_MinReturn : 224.61903381347656
Train_AverageEpLen : 69.66666666666667
Train_EnvstepsSoFar : 2047
TimeSinceStart : 41.33132743835449
Training Loss : 0.5467990636825562
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 291.41705322265625
Eval_StdReturn : 53.36784362792969
Eval_MaxReturn : 534.253662109375
Eval_MinReturn : 193.3857421875
Eval_AverageEpLen : 52.98941798941799
Train_AverageReturn : 302.7569885253906
Train_StdReturn : 17.849252700805664
Train_MaxReturn : 348.80340576171875
Train_MinReturn : 267.0437927246094
Train_AverageEpLen : 55.94444444444444
Train_EnvstepsSoFar : 3054
TimeSinceStart : 55.06045436859131
Training Loss : 0.5170094966888428
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 321.52264404296875
Eval_StdReturn : 40.99192810058594
Eval_MaxReturn : 496.560302734375
Eval_MinReturn : 226.77847290039062
Eval_AverageEpLen : 58.50877192982456
Train_AverageReturn : 301.0790100097656
Train_StdReturn : 47.029354095458984
Train_MaxReturn : 392.0211181640625
Train_MinReturn : 232.4257354736328
Train_AverageEpLen : 54.421052631578945
Train_EnvstepsSoFar : 4088
TimeSinceStart : 68.95885157585144
Training Loss : 0.5382992029190063
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...

Collecting train rollouts to be used for saving videos...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Collecting video rollouts eval

Saving train rollouts as videos...
Eval_AverageReturn : 336.4987487792969
Eval_StdReturn : 50.261253356933594
Eval_MaxReturn : 601.9056396484375
Eval_MinReturn : 220.8479461669922
Eval_AverageEpLen : 59.97604790419162
Train_AverageReturn : 326.2229919433594
Train_StdReturn : 46.26999282836914
Train_MaxReturn : 417.12127685546875
Train_MinReturn : 241.8565673828125
Train_AverageEpLen : 59.411764705882355
Train_EnvstepsSoFar : 5098
TimeSinceStart : 86.75582456588745
Training Loss : 0.5099533796310425
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 358.7681884765625
Eval_StdReturn : 78.27796173095703
Eval_MaxReturn : 668.9142456054688
Eval_MinReturn : 235.14959716796875
Eval_AverageEpLen : 63.15723270440252
Train_AverageReturn : 322.1082763671875
Train_StdReturn : 35.50320053100586
Train_MaxReturn : 396.8714904785156
Train_MinReturn : 272.5691833496094
Train_AverageEpLen : 57.611111111111114
Train_EnvstepsSoFar : 6135
TimeSinceStart : 100.92290043830872
Training Loss : 0.5949633121490479
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 371.98516845703125
Eval_StdReturn : 70.8738784790039
Eval_MaxReturn : 738.1651000976562
Eval_MinReturn : 255.02755737304688
Eval_AverageEpLen : 64.93506493506493
Train_AverageReturn : 348.970947265625
Train_StdReturn : 74.55958557128906
Train_MaxReturn : 587.0650024414062
Train_MinReturn : 255.76576232910156
Train_AverageEpLen : 62.11764705882353
Train_EnvstepsSoFar : 7191
TimeSinceStart : 115.02261590957642
Training Loss : 0.5719987750053406
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 378.7444763183594
Eval_StdReturn : 85.80258178710938
Eval_MaxReturn : 759.7353515625
Eval_MinReturn : 243.50750732421875
Eval_AverageEpLen : 65.4640522875817
Train_AverageReturn : 429.50341796875
Train_StdReturn : 96.92408752441406
Train_MaxReturn : 639.5869750976562
Train_MinReturn : 310.82305908203125
Train_AverageEpLen : 72.85714285714286
Train_EnvstepsSoFar : 8211
TimeSinceStart : 129.68614172935486
Training Loss : 0.5550409555435181
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 372.60003662109375
Eval_StdReturn : 81.1655044555664
Eval_MaxReturn : 845.5700073242188
Eval_MinReturn : 239.25221252441406
Eval_AverageEpLen : 64.59354838709677
Train_AverageReturn : 406.9309997558594
Train_StdReturn : 125.4524154663086
Train_MaxReturn : 763.7083740234375
Train_MinReturn : 286.4322509765625
Train_AverageEpLen : 69.26666666666667
Train_EnvstepsSoFar : 9250
TimeSinceStart : 144.13925433158875
Training Loss : 0.5506694316864014
Initial_DataCollection_AverageReturn : 10344.517578125
Done logging...


Found 4 GPUs for rendering. Using device 0.
########################
logging outputs to  /home/zzh/Desktop/CS285/cs285-homework-fall2021/hw1/cs285/scripts/../../data/q2_dagger_Walker2d_Walker2d-v2_2022-01-01_23-39-04
########################
Using GPU id 0
Loading expert policy from... cs285/policies/experts/Walker2d.pkl
obs (1, 17) (1, 17)
Done restoring expert policy...


********** Iteration 0 ************

Collecting data to be used for training...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 5013.5361328125
Eval_StdReturn : 898.9456787109375
Eval_MaxReturn : 5521.576171875
Eval_MinReturn : 2317.072265625
Eval_AverageEpLen : 934.0
Train_AverageReturn : 5566.845703125
Train_StdReturn : 9.237548828125
Train_MaxReturn : 5576.08349609375
Train_MinReturn : 5557.6083984375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 11.263497352600098
Training Loss : -1.3312077522277832
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4631.24951171875
Eval_StdReturn : 1595.0740966796875
Eval_MaxReturn : 5603.0634765625
Eval_MinReturn : 1138.5919189453125
Eval_AverageEpLen : 855.3333333333334
Train_AverageReturn : 5450.193359375
Train_StdReturn : 0.0
Train_MaxReturn : 5450.193359375
Train_MinReturn : 5450.193359375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 1000
TimeSinceStart : 21.741931438446045
Training Loss : -1.1695194244384766
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3280.849365234375
Eval_StdReturn : 2352.343994140625
Eval_MaxReturn : 5503.39013671875
Eval_MinReturn : 239.10824584960938
Eval_AverageEpLen : 635.2352941176471
Train_AverageReturn : 5492.9052734375
Train_StdReturn : 0.0
Train_MaxReturn : 5492.9052734375
Train_MinReturn : 5492.9052734375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 2000
TimeSinceStart : 33.32597756385803
Training Loss : -1.1483248472213745
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 5000.6513671875
Eval_StdReturn : 1501.1287841796875
Eval_MaxReturn : 5579.681640625
Eval_MinReturn : 256.2024230957031
Eval_AverageEpLen : 919.1818181818181
Train_AverageReturn : 5363.7255859375
Train_StdReturn : 0.0
Train_MaxReturn : 5363.7255859375
Train_MinReturn : 5363.7255859375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 3000
TimeSinceStart : 44.0524263381958
Training Loss : -1.3074957132339478
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 5513.3193359375
Eval_StdReturn : 46.430145263671875
Eval_MaxReturn : 5606.3515625
Eval_MinReturn : 5441.306640625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 2375.802001953125
Train_StdReturn : 2290.8203125
Train_MaxReturn : 5559.11279296875
Train_MinReturn : 262.9124755859375
Train_AverageEpLen : 477.6666666666667
Train_EnvstepsSoFar : 4433
TimeSinceStart : 55.2170147895813
Training Loss : -1.31271493434906
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...

Collecting train rollouts to be used for saving videos...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Collecting video rollouts eval

Saving train rollouts as videos...
Eval_AverageReturn : 5408.1943359375
Eval_StdReturn : 52.47981643676758
Eval_MaxReturn : 5476.6171875
Eval_MinReturn : 5287.0888671875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 5503.7998046875
Train_StdReturn : 0.0
Train_MaxReturn : 5503.7998046875
Train_MinReturn : 5503.7998046875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 5433
TimeSinceStart : 70.47270703315735
Training Loss : -1.3093386888504028
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 5427.56298828125
Eval_StdReturn : 74.65702056884766
Eval_MaxReturn : 5549.2138671875
Eval_MinReturn : 5289.1884765625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4831.5595703125
Train_StdReturn : 573.7021484375
Train_MaxReturn : 5405.26171875
Train_MinReturn : 4257.857421875
Train_AverageEpLen : 920.0
Train_EnvstepsSoFar : 7273
TimeSinceStart : 81.73872232437134
Training Loss : -1.3990775346755981
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 5491.70068359375
Eval_StdReturn : 42.3912467956543
Eval_MaxReturn : 5556.69140625
Eval_MinReturn : 5426.328125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 5434.7333984375
Train_StdReturn : 0.0
Train_MaxReturn : 5434.7333984375
Train_MinReturn : 5434.7333984375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 8273
TimeSinceStart : 92.43850755691528
Training Loss : -1.242974042892456
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 5534.9130859375
Eval_StdReturn : 86.95103454589844
Eval_MaxReturn : 5625.3779296875
Eval_MinReturn : 5318.673828125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 5288.923828125
Train_StdReturn : 0.0
Train_MaxReturn : 5288.923828125
Train_MinReturn : 5288.923828125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 9273
TimeSinceStart : 103.56487107276917
Training Loss : -1.4081919193267822
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 5493.4580078125
Eval_StdReturn : 29.38899803161621
Eval_MaxReturn : 5555.59228515625
Eval_MinReturn : 5431.21728515625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 5554.11328125
Train_StdReturn : 0.0
Train_MaxReturn : 5554.11328125
Train_MinReturn : 5554.11328125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 10273
TimeSinceStart : 114.92524480819702
Training Loss : -1.3020715713500977
Initial_DataCollection_AverageReturn : 5566.845703125
Done logging...


Found 4 GPUs for rendering. Using device 0.
########################
logging outputs to  /home/zzh/Desktop/CS285/cs285-homework-fall2021/hw1/cs285/scripts/../../data/q2_dagger_HalfCheetah_HalfCheetah-v2_2022-01-01_23-41-03
########################
Using GPU id 0
Loading expert policy from... cs285/policies/experts/HalfCheetah.pkl
obs (1, 17) (1, 17)
Done restoring expert policy...


********** Iteration 0 ************

Collecting data to be used for training...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4134.45361328125
Eval_StdReturn : 61.3955192565918
Eval_MaxReturn : 4244.86328125
Eval_MinReturn : 4042.0498046875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4205.7783203125
Train_StdReturn : 83.038818359375
Train_MaxReturn : 4288.81689453125
Train_MinReturn : 4122.7392578125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 10.000257015228271
Training Loss : -2.190131664276123
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4107.5986328125
Eval_StdReturn : 80.11780548095703
Eval_MaxReturn : 4221.71337890625
Eval_MinReturn : 3975.5458984375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4022.4111328125
Train_StdReturn : 0.0
Train_MaxReturn : 4022.4111328125
Train_MinReturn : 4022.4111328125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 1000
TimeSinceStart : 20.109281301498413
Training Loss : -2.0643794536590576
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4076.49462890625
Eval_StdReturn : 58.72236251831055
Eval_MaxReturn : 4215.21923828125
Eval_MinReturn : 4006.667236328125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4157.74609375
Train_StdReturn : 0.0
Train_MaxReturn : 4157.74609375
Train_MinReturn : 4157.74609375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 2000
TimeSinceStart : 30.21363139152527
Training Loss : -1.9889099597930908
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4032.65771484375
Eval_StdReturn : 66.26763153076172
Eval_MaxReturn : 4114.068359375
Eval_MinReturn : 3918.295166015625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4084.946044921875
Train_StdReturn : 0.0
Train_MaxReturn : 4084.946044921875
Train_MinReturn : 4084.946044921875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 3000
TimeSinceStart : 40.28037357330322
Training Loss : -2.0832903385162354
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4104.6787109375
Eval_StdReturn : 59.99592208862305
Eval_MaxReturn : 4227.5869140625
Eval_MinReturn : 4033.23974609375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4147.7626953125
Train_StdReturn : 0.0
Train_MaxReturn : 4147.7626953125
Train_MinReturn : 4147.7626953125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 4000
TimeSinceStart : 50.36778163909912
Training Loss : -2.165299415588379
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...

Collecting train rollouts to be used for saving videos...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Collecting video rollouts eval

Saving train rollouts as videos...
Eval_AverageReturn : 4081.44384765625
Eval_StdReturn : 75.60936737060547
Eval_MaxReturn : 4188.71875
Eval_MinReturn : 3919.92529296875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4151.111328125
Train_StdReturn : 0.0
Train_MaxReturn : 4151.111328125
Train_MinReturn : 4151.111328125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 5000
TimeSinceStart : 64.90186524391174
Training Loss : -2.3117501735687256
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4171.49609375
Eval_StdReturn : 74.75437927246094
Eval_MaxReturn : 4282.9833984375
Eval_MinReturn : 4042.6875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4146.8740234375
Train_StdReturn : 0.0
Train_MaxReturn : 4146.8740234375
Train_MinReturn : 4146.8740234375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 6000
TimeSinceStart : 75.15681838989258
Training Loss : -2.2503373622894287
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4104.98193359375
Eval_StdReturn : 140.6469268798828
Eval_MaxReturn : 4288.1591796875
Eval_MinReturn : 3802.792236328125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4178.5478515625
Train_StdReturn : 0.0
Train_MaxReturn : 4178.5478515625
Train_MinReturn : 4178.5478515625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 7000
TimeSinceStart : 85.33999300003052
Training Loss : -2.331141710281372
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4138.7177734375
Eval_StdReturn : 49.63003921508789
Eval_MaxReturn : 4214.09423828125
Eval_MinReturn : 4064.287841796875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4072.70556640625
Train_StdReturn : 0.0
Train_MaxReturn : 4072.70556640625
Train_MinReturn : 4072.70556640625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 8000
TimeSinceStart : 95.42049622535706
Training Loss : -2.0905447006225586
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 4113.4365234375
Eval_StdReturn : 60.47414016723633
Eval_MaxReturn : 4218.802734375
Eval_MinReturn : 4036.26220703125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4125.5625
Train_StdReturn : 0.0
Train_MaxReturn : 4125.5625
Train_MinReturn : 4125.5625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 9000
TimeSinceStart : 105.69491147994995
Training Loss : -2.224766254425049
Initial_DataCollection_AverageReturn : 4205.7783203125
Done logging...


Found 4 GPUs for rendering. Using device 0.
########################
logging outputs to  /home/zzh/Desktop/CS285/cs285-homework-fall2021/hw1/cs285/scripts/../../data/q2_dagger_Hopper_Hopper-v2_2022-01-01_23-42-53
########################
Using GPU id 0
Loading expert policy from... cs285/policies/experts/Hopper.pkl
obs (1, 11) (1, 11)
Done restoring expert policy...


********** Iteration 0 ************

Collecting data to be used for training...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 807.9420776367188
Eval_StdReturn : 351.1786804199219
Eval_MaxReturn : 1872.687255859375
Eval_MinReturn : 421.4342346191406
Eval_AverageEpLen : 261.5128205128205
Train_AverageReturn : 3772.67041015625
Train_StdReturn : 1.9483642578125
Train_MaxReturn : 3774.61865234375
Train_MinReturn : 3770.721923828125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 11.298965692520142
Training Loss : -2.0189826488494873
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 2230.10107421875
Eval_StdReturn : 677.2194213867188
Eval_MaxReturn : 3592.28857421875
Eval_MinReturn : 1446.914794921875
Eval_AverageEpLen : 598.3333333333334
Train_AverageReturn : 767.1707763671875
Train_StdReturn : 223.9349365234375
Train_MaxReturn : 1091.385009765625
Train_MinReturn : 418.52911376953125
Train_AverageEpLen : 253.6
Train_EnvstepsSoFar : 1268
TimeSinceStart : 22.089191198349
Training Loss : -1.4740667343139648
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3610.593017578125
Eval_StdReturn : 551.103271484375
Eval_MaxReturn : 3837.14501953125
Eval_MinReturn : 1868.8798828125
Eval_AverageEpLen : 954.7272727272727
Train_AverageReturn : 3258.05224609375
Train_StdReturn : 111.396484375
Train_MaxReturn : 3369.44873046875
Train_MinReturn : 3146.65576171875
Train_AverageEpLen : 860.5
Train_EnvstepsSoFar : 2989
TimeSinceStart : 32.85121703147888
Training Loss : -1.2687162160873413
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3777.759765625
Eval_StdReturn : 3.2604241371154785
Eval_MaxReturn : 3782.33251953125
Eval_MinReturn : 3772.6982421875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3781.97705078125
Train_StdReturn : 0.0
Train_MaxReturn : 3781.97705078125
Train_MinReturn : 3781.97705078125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 3989
TimeSinceStart : 43.16851806640625
Training Loss : -1.5005879402160645
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3766.487060546875
Eval_StdReturn : 3.593681812286377
Eval_MaxReturn : 3773.25634765625
Eval_MinReturn : 3760.64404296875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3778.0439453125
Train_StdReturn : 0.0
Train_MaxReturn : 3778.0439453125
Train_MinReturn : 3778.0439453125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 4989
TimeSinceStart : 53.658156871795654
Training Loss : -1.6222642660140991
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...

Collecting train rollouts to be used for saving videos...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Collecting video rollouts eval

Saving train rollouts as videos...
Eval_AverageReturn : 3777.640625
Eval_StdReturn : 3.6903367042541504
Eval_MaxReturn : 3784.9482421875
Eval_MinReturn : 3771.01904296875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3768.40625
Train_StdReturn : 0.0
Train_MaxReturn : 3768.40625
Train_MinReturn : 3768.40625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 5989
TimeSinceStart : 68.7223207950592
Training Loss : -1.7670717239379883
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3778.42822265625
Eval_StdReturn : 3.8249905109405518
Eval_MaxReturn : 3785.38427734375
Eval_MinReturn : 3771.57568359375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3779.19482421875
Train_StdReturn : 0.0
Train_MaxReturn : 3779.19482421875
Train_MinReturn : 3779.19482421875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 6989
TimeSinceStart : 80.07339334487915
Training Loss : -1.9098225831985474
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3782.60546875
Eval_StdReturn : 4.380570411682129
Eval_MaxReturn : 3794.799072265625
Eval_MinReturn : 3778.37841796875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3775.22509765625
Train_StdReturn : 0.0
Train_MaxReturn : 3775.22509765625
Train_MinReturn : 3775.22509765625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 7989
TimeSinceStart : 90.70171999931335
Training Loss : -1.91391122341156
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3776.946533203125
Eval_StdReturn : 4.108597278594971
Eval_MaxReturn : 3782.5771484375
Eval_MinReturn : 3769.12890625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3785.13671875
Train_StdReturn : 0.0
Train_MaxReturn : 3785.13671875
Train_MinReturn : 3785.13671875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 8989
TimeSinceStart : 101.38786363601685
Training Loss : -1.8721473217010498
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 3777.71435546875
Eval_StdReturn : 3.1202311515808105
Eval_MaxReturn : 3783.046875
Eval_MinReturn : 3772.90380859375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3774.563720703125
Train_StdReturn : 0.0
Train_MaxReturn : 3774.563720703125
Train_MinReturn : 3774.563720703125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 9989
TimeSinceStart : 112.20064377784729
Training Loss : -2.0174508094787598
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...


Found 4 GPUs for rendering. Using device 0.
