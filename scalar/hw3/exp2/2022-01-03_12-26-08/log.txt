#!/bin/bash
export PYTHONPATH=.

DATA_DIR=./data
EXP_DIR=${DATA_DIR}/exp2/`date +%Y-%m-%d_%H-%M-%S`
LOG_PATH=${EXP_DIR}/log.txt
# clear other data files
rm -rf $(find ${DATA_DIR} -maxdepth 1 -name '*q2_*' 2> /dev/null)
# create data directory for this experiment and logfile
mkdir -p $EXP_DIR; touch $LOG_PATH
# dump experiment commands and hyperparameters (this file) to logfile
cat $0 >> $LOG_PATH; echo "\n\n" >> $LOG_PATH

for seed in 1 2 3
do
   python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_dqn_${seed} --seed ${seed} >> $LOG_PATH 
   python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_doubledqn_${seed} --double_q --seed ${seed} >> $LOG_PATH
done

# move data file to experiment data directory
mv $(find ${DATA_DIR} -maxdepth 1 -name '*q2_*' 2> /dev/null) $EXP_DIR






LOGGING TO:  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_dqn_1_LunarLander-v3_03-01-2022_12-26-09 



########################
logging outputs to  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_dqn_1_LunarLander-v3_03-01-2022_12-26-09
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001049
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.001049041748046875
Done logging...




********** Iteration 1000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 7000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 8000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 9000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 10000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -241.571285
best mean reward -inf
running time 16.331336
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -241.57128490921392
TimeSinceStart : 16.33133625984192
Training Loss : 3.312045097351074
Done logging...




********** Iteration 11000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 12000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 13000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 14000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 15000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 16000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 17000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 18000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 19000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 20000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -190.546302
best mean reward -190.546302
running time 37.123316
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -190.54630156087717
Train_BestReturn : -190.54630156087717
TimeSinceStart : 37.123316049575806
Training Loss : 1.2240753173828125
Done logging...




********** Iteration 21000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 22000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 23000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 24000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 25000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 26000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 27000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 28000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 29000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 30000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -170.019178
best mean reward -170.019178
running time 67.636920
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -170.01917764098766
Train_BestReturn : -170.01917764098766
TimeSinceStart : 67.63691997528076
Training Loss : 0.30576860904693604
Done logging...




********** Iteration 31000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 32000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 33000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 34000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 35000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 36000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 37000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 38000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 39000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 40000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -138.861504
best mean reward -138.861504
running time 96.363670
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -138.86150430051114
Train_BestReturn : -138.86150430051114
TimeSinceStart : 96.36366963386536
Training Loss : 0.46430307626724243
Done logging...




********** Iteration 41000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 42000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 43000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 44000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 45000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 46000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 47000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 48000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 49000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 50000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -126.354853
best mean reward -126.354853
running time 124.572174
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -126.35485280428034
Train_BestReturn : -126.35485280428034
TimeSinceStart : 124.57217383384705
Training Loss : 1.1112208366394043
Done logging...




********** Iteration 51000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 52000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 53000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 54000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 55000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 56000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 57000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 58000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 59000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 60000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -93.468936
best mean reward -93.468936
running time 148.809649
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -93.46893592186319
Train_BestReturn : -93.46893592186319
TimeSinceStart : 148.80964922904968
Training Loss : 1.6154299974441528
Done logging...




********** Iteration 61000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 62000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 63000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 64000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 65000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 66000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 67000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 68000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 69000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 70000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -76.944089
best mean reward -76.944089
running time 176.032667
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -76.94408912994523
Train_BestReturn : -76.94408912994523
TimeSinceStart : 176.03266668319702
Training Loss : 0.23274900019168854
Done logging...




********** Iteration 71000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 72000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 73000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 74000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 75000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 76000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 77000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 78000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 79000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 80000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -75.800022
best mean reward -75.800022
running time 203.439520
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -75.8000224233214
Train_BestReturn : -75.8000224233214
TimeSinceStart : 203.43951988220215
Training Loss : 0.22883260250091553
Done logging...




********** Iteration 81000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 82000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 83000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 84000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 85000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 86000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 87000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 88000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 89000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 90000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -52.316643
best mean reward -52.316643
running time 229.368267
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -52.31664348226783
Train_BestReturn : -52.31664348226783
TimeSinceStart : 229.3682668209076
Training Loss : 0.32160529494285583
Done logging...




********** Iteration 91000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 92000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 93000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 94000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 95000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 96000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 97000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 98000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 99000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -38.175204
best mean reward -38.175204
running time 254.877939
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -38.17520392362963
Train_BestReturn : -38.17520392362963
TimeSinceStart : 254.87793850898743
Training Loss : 0.20563144981861115
Done logging...




********** Iteration 101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -11.772072
best mean reward -11.772072
running time 280.551219
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -11.772071719525318
Train_BestReturn : -11.772071719525318
TimeSinceStart : 280.55121874809265
Training Loss : 0.7487544417381287
Done logging...




********** Iteration 111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) 5.272338
best mean reward 5.272338
running time 305.019717
Train_EnvstepsSoFar : 120001
Train_AverageReturn : 5.272337903773273
Train_BestReturn : 5.272337903773273
TimeSinceStart : 305.0197172164917
Training Loss : 0.2142019122838974
Done logging...




********** Iteration 121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) 48.217213
best mean reward 48.217213
running time 329.079904
Train_EnvstepsSoFar : 130001
Train_AverageReturn : 48.217212739119994
Train_BestReturn : 48.217212739119994
TimeSinceStart : 329.0799038410187
Training Loss : 0.16399705410003662
Done logging...




********** Iteration 131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) 58.483666
best mean reward 58.483666
running time 352.437579
Train_EnvstepsSoFar : 140001
Train_AverageReturn : 58.48366629569691
Train_BestReturn : 58.48366629569691
TimeSinceStart : 352.4375786781311
Training Loss : 0.1391291618347168
Done logging...




********** Iteration 141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) 71.189472
best mean reward 71.189472
running time 377.296093
Train_EnvstepsSoFar : 150001
Train_AverageReturn : 71.18947173196123
Train_BestReturn : 71.18947173196123
TimeSinceStart : 377.29609298706055
Training Loss : 1.8125317096710205
Done logging...




********** Iteration 151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 80.098957
best mean reward 80.098957
running time 400.516333
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 80.09895708954076
Train_BestReturn : 80.09895708954076
TimeSinceStart : 400.51633310317993
Training Loss : 0.20469436049461365
Done logging...




********** Iteration 161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 82.922939
best mean reward 82.922939
running time 425.506071
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 82.92293857129226
Train_BestReturn : 82.92293857129226
TimeSinceStart : 425.50607085227966
Training Loss : 0.32237982749938965
Done logging...




********** Iteration 171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 85.345764
best mean reward 85.345764
running time 449.724037
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 85.34576444069202
Train_BestReturn : 85.34576444069202
TimeSinceStart : 449.72403740882874
Training Loss : 0.47729212045669556
Done logging...




********** Iteration 181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 78.907779
best mean reward 85.345764
running time 474.726590
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 78.90777865252922
Train_BestReturn : 85.34576444069202
TimeSinceStart : 474.72659039497375
Training Loss : 0.11387565732002258
Done logging...




********** Iteration 191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 55.731984
best mean reward 85.345764
running time 498.584965
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 55.731983905876035
Train_BestReturn : 85.34576444069202
TimeSinceStart : 498.58496475219727
Training Loss : 0.4676303565502167
Done logging...




********** Iteration 201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 52.401108
best mean reward 85.345764
running time 521.474727
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 52.40110759536707
Train_BestReturn : 85.34576444069202
TimeSinceStart : 521.4747273921967
Training Loss : 0.981124758720398
Done logging...




********** Iteration 211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 47.499504
best mean reward 85.345764
running time 545.930106
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 47.49950358267259
Train_BestReturn : 85.34576444069202
TimeSinceStart : 545.9301059246063
Training Loss : 2.062882423400879
Done logging...




********** Iteration 221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 45.492254
best mean reward 85.345764
running time 568.614648
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 45.49225431643258
Train_BestReturn : 85.34576444069202
TimeSinceStart : 568.6146476268768
Training Loss : 1.7315449714660645
Done logging...




********** Iteration 231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 44.510193
best mean reward 85.345764
running time 590.394647
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 44.51019252941283
Train_BestReturn : 85.34576444069202
TimeSinceStart : 590.3946468830109
Training Loss : 1.6266443729400635
Done logging...




********** Iteration 241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 53.035362
best mean reward 85.345764
running time 612.491186
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 53.03536194973119
Train_BestReturn : 85.34576444069202
TimeSinceStart : 612.4911861419678
Training Loss : 0.18024450540542603
Done logging...




********** Iteration 251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 70.857845
best mean reward 85.345764
running time 635.481659
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 70.85784514378498
Train_BestReturn : 85.34576444069202
TimeSinceStart : 635.4816586971283
Training Loss : 6.700657844543457
Done logging...




********** Iteration 261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 77.049988
best mean reward 85.345764
running time 659.098722
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 77.0499878510666
Train_BestReturn : 85.34576444069202
TimeSinceStart : 659.0987215042114
Training Loss : 1.0771383047103882
Done logging...




********** Iteration 271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 88.832374
best mean reward 88.832374
running time 682.017023
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 88.8323742899475
Train_BestReturn : 88.8323742899475
TimeSinceStart : 682.0170233249664
Training Loss : 2.598501205444336
Done logging...




********** Iteration 281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 105.916085
best mean reward 105.916085
running time 704.389593
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 105.91608515525799
Train_BestReturn : 105.91608515525799
TimeSinceStart : 704.3895928859711
Training Loss : 0.1453615427017212
Done logging...




********** Iteration 291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 110.196679
best mean reward 110.196679
running time 725.834622
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 110.19667919321323
Train_BestReturn : 110.19667919321323
TimeSinceStart : 725.8346223831177
Training Loss : 1.1734569072723389
Done logging...




********** Iteration 301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 103.460285
best mean reward 110.196679
running time 747.305323
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 103.46028491365132
Train_BestReturn : 110.19667919321323
TimeSinceStart : 747.3053231239319
Training Loss : 0.4740605354309082
Done logging...




********** Iteration 311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 106.600676
best mean reward 110.196679
running time 769.726698
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 106.6006758500237
Train_BestReturn : 110.19667919321323
TimeSinceStart : 769.7266976833344
Training Loss : 0.229436457157135
Done logging...




********** Iteration 321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 113.803630
best mean reward 113.803630
running time 790.675975
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 113.80362950541456
Train_BestReturn : 113.80362950541456
TimeSinceStart : 790.6759748458862
Training Loss : 4.784247875213623
Done logging...




********** Iteration 331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 105.374188
best mean reward 113.803630
running time 813.403955
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 105.3741877204044
Train_BestReturn : 113.80362950541456
TimeSinceStart : 813.403954744339
Training Loss : 1.018913984298706
Done logging...




********** Iteration 341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 111.087856
best mean reward 113.803630
running time 835.478382
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 111.0878564973096
Train_BestReturn : 113.80362950541456
TimeSinceStart : 835.4783823490143
Training Loss : 0.6927046775817871
Done logging...




********** Iteration 351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 138.594898
best mean reward 138.594898
running time 855.271906
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 138.5948984045747
Train_BestReturn : 138.5948984045747
TimeSinceStart : 855.2719056606293
Training Loss : 0.5181748867034912
Done logging...




********** Iteration 361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 149.400399
best mean reward 149.400399
running time 877.222718
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 149.40039885194358
Train_BestReturn : 149.40039885194358
TimeSinceStart : 877.2227182388306
Training Loss : 2.6045007705688477
Done logging...




********** Iteration 371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 145.604132
best mean reward 149.400399
running time 899.842454
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 145.60413245190955
Train_BestReturn : 149.40039885194358
TimeSinceStart : 899.8424544334412
Training Loss : 0.27515894174575806
Done logging...




********** Iteration 381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 136.234769
best mean reward 149.400399
running time 923.710837
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 136.23476872783445
Train_BestReturn : 149.40039885194358
TimeSinceStart : 923.7108373641968
Training Loss : 0.40285179018974304
Done logging...




********** Iteration 391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 142.085710
best mean reward 149.400399
running time 947.474712
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 142.0857098362293
Train_BestReturn : 149.40039885194358
TimeSinceStart : 947.4747116565704
Training Loss : 2.2278101444244385
Done logging...




********** Iteration 401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 135.592310
best mean reward 149.400399
running time 969.358441
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 135.592310011086
Train_BestReturn : 149.40039885194358
TimeSinceStart : 969.3584411144257
Training Loss : 0.5775103569030762
Done logging...




********** Iteration 411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 153.891006
best mean reward 153.891006
running time 990.797888
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 153.89100598557525
Train_BestReturn : 153.89100598557525
TimeSinceStart : 990.7978875637054
Training Loss : 0.2090466022491455
Done logging...




********** Iteration 421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) 164.304116
best mean reward 164.304116
running time 1011.974598
Train_EnvstepsSoFar : 430001
Train_AverageReturn : 164.30411613993405
Train_BestReturn : 164.30411613993405
TimeSinceStart : 1011.9745979309082
Training Loss : 0.2759622633457184
Done logging...




********** Iteration 431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) 171.172793
best mean reward 171.172793
running time 1032.383392
Train_EnvstepsSoFar : 440001
Train_AverageReturn : 171.17279341636876
Train_BestReturn : 171.17279341636876
TimeSinceStart : 1032.3833920955658
Training Loss : 0.3941725492477417
Done logging...




********** Iteration 441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) 168.697426
best mean reward 171.172793
running time 1051.864890
Train_EnvstepsSoFar : 450001
Train_AverageReturn : 168.69742587526943
Train_BestReturn : 171.17279341636876
TimeSinceStart : 1051.8648903369904
Training Loss : 1.140019416809082
Done logging...




********** Iteration 451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) 162.046567
best mean reward 171.172793
running time 1070.946474
Train_EnvstepsSoFar : 460001
Train_AverageReturn : 162.04656678116646
Train_BestReturn : 171.17279341636876
TimeSinceStart : 1070.9464740753174
Training Loss : 0.9652659296989441
Done logging...




********** Iteration 461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) 137.358490
best mean reward 171.172793
running time 1089.877976
Train_EnvstepsSoFar : 470001
Train_AverageReturn : 137.3584904270732
Train_BestReturn : 171.17279341636876
TimeSinceStart : 1089.877976179123
Training Loss : 0.13189350068569183
Done logging...




********** Iteration 471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) 131.663241
best mean reward 171.172793
running time 1109.072068
Train_EnvstepsSoFar : 480001
Train_AverageReturn : 131.66324115584337
Train_BestReturn : 171.17279341636876
TimeSinceStart : 1109.072067975998
Training Loss : 0.13743093609809875
Done logging...




********** Iteration 481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) 92.242819
best mean reward 171.172793
running time 1128.082396
Train_EnvstepsSoFar : 490001
Train_AverageReturn : 92.24281929552981
Train_BestReturn : 171.17279341636876
TimeSinceStart : 1128.0823962688446
Training Loss : 0.9341617822647095
Done logging...




********** Iteration 491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 499000 ************

Training agent...

Training agent using sampled data from replay buffer...



LOGGING TO:  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_doubledqn_1_LunarLander-v3_03-01-2022_12-45-18 



########################
logging outputs to  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_doubledqn_1_LunarLander-v3_03-01-2022_12-45-18
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.000327
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0003273487091064453
Done logging...




********** Iteration 1000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 7000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 8000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 9000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 10000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -237.667192
best mean reward -inf
running time 17.491664
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -237.66719167496586
TimeSinceStart : 17.491663694381714
Training Loss : 0.23171007633209229
Done logging...




********** Iteration 11000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 12000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 13000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 14000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 15000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 16000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 17000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 18000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 19000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 20000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -201.909964
best mean reward -201.909964
running time 41.875978
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -201.90996425370756
Train_BestReturn : -201.90996425370756
TimeSinceStart : 41.875977993011475
Training Loss : 0.8850998282432556
Done logging...




********** Iteration 21000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 22000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 23000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 24000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 25000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 26000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 27000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 28000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 29000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 30000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -171.148274
best mean reward -171.148274
running time 70.183585
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -171.14827350474886
Train_BestReturn : -171.14827350474886
TimeSinceStart : 70.18358516693115
Training Loss : 0.4514082968235016
Done logging...




********** Iteration 31000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 32000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 33000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 34000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 35000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 36000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 37000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 38000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 39000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 40000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -163.015228
best mean reward -163.015228
running time 99.342319
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -163.01522753594026
Train_BestReturn : -163.01522753594026
TimeSinceStart : 99.34231853485107
Training Loss : 0.4535982310771942
Done logging...




********** Iteration 41000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 42000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 43000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 44000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 45000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 46000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 47000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 48000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 49000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 50000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -149.271434
best mean reward -149.271434
running time 129.261061
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -149.27143381900433
Train_BestReturn : -149.27143381900433
TimeSinceStart : 129.26106071472168
Training Loss : 0.425680547952652
Done logging...




********** Iteration 51000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 52000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 53000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 54000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 55000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 56000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 57000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 58000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 59000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 60000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -130.532478
best mean reward -130.532478
running time 157.304074
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -130.53247848092167
Train_BestReturn : -130.53247848092167
TimeSinceStart : 157.30407404899597
Training Loss : 0.8821145296096802
Done logging...




********** Iteration 61000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 62000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 63000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 64000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 65000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 66000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 67000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 68000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 69000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 70000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -111.004490
best mean reward -111.004490
running time 185.106218
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -111.00448981939107
Train_BestReturn : -111.00448981939107
TimeSinceStart : 185.10621809959412
Training Loss : 0.23027737438678741
Done logging...




********** Iteration 71000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 72000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 73000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 74000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 75000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 76000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 77000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 78000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 79000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 80000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -102.063719
best mean reward -102.063719
running time 213.872493
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -102.0637194965658
Train_BestReturn : -102.0637194965658
TimeSinceStart : 213.87249326705933
Training Loss : 0.3282391428947449
Done logging...




********** Iteration 81000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 82000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 83000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 84000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 85000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 86000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 87000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 88000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 89000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 90000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -78.148904
best mean reward -78.148904
running time 242.744540
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -78.14890360753833
Train_BestReturn : -78.14890360753833
TimeSinceStart : 242.74453997612
Training Loss : 0.2956586480140686
Done logging...




********** Iteration 91000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 92000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 93000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 94000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 95000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 96000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 97000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 98000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 99000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -64.485111
best mean reward -64.485111
running time 269.843184
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -64.48511119204186
Train_BestReturn : -64.48511119204186
TimeSinceStart : 269.84318375587463
Training Loss : 0.24408382177352905
Done logging...




********** Iteration 101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -31.800384
best mean reward -31.800384
running time 295.965855
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -31.800383769269253
Train_BestReturn : -31.800383769269253
TimeSinceStart : 295.9658546447754
Training Loss : 0.14368319511413574
Done logging...




********** Iteration 111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -5.874646
best mean reward -5.874646
running time 322.842937
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -5.874645810390291
Train_BestReturn : -5.874645810390291
TimeSinceStart : 322.84293699264526
Training Loss : 0.12762488424777985
Done logging...




********** Iteration 121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) 20.933967
best mean reward 20.933967
running time 349.341238
Train_EnvstepsSoFar : 130001
Train_AverageReturn : 20.93396655575309
Train_BestReturn : 20.93396655575309
TimeSinceStart : 349.3412380218506
Training Loss : 0.16148047149181366
Done logging...




********** Iteration 131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) 51.399976
best mean reward 51.399976
running time 376.940460
Train_EnvstepsSoFar : 140001
Train_AverageReturn : 51.399976155164275
Train_BestReturn : 51.399976155164275
TimeSinceStart : 376.94045972824097
Training Loss : 0.06395164132118225
Done logging...




********** Iteration 141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) 53.036052
best mean reward 53.036052
running time 404.450271
Train_EnvstepsSoFar : 150001
Train_AverageReturn : 53.03605194212949
Train_BestReturn : 53.03605194212949
TimeSinceStart : 404.45027136802673
Training Loss : 0.044274795800447464
Done logging...




********** Iteration 151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 53.793281
best mean reward 53.793281
running time 430.089489
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 53.79328121413544
Train_BestReturn : 53.79328121413544
TimeSinceStart : 430.08948850631714
Training Loss : 0.16371650993824005
Done logging...




********** Iteration 161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 64.609010
best mean reward 64.609010
running time 455.014277
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 64.60901019327768
Train_BestReturn : 64.60901019327768
TimeSinceStart : 455.0142774581909
Training Loss : 0.3745114207267761
Done logging...




********** Iteration 171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 62.555984
best mean reward 64.609010
running time 482.681241
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 62.55598390666672
Train_BestReturn : 64.60901019327768
TimeSinceStart : 482.68124127388
Training Loss : 0.06569220870733261
Done logging...




********** Iteration 181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 72.389665
best mean reward 72.389665
running time 508.082954
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 72.38966508562142
Train_BestReturn : 72.38966508562142
TimeSinceStart : 508.0829544067383
Training Loss : 0.11776808649301529
Done logging...




********** Iteration 191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 79.645135
best mean reward 79.645135
running time 533.376011
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 79.64513479485473
Train_BestReturn : 79.64513479485473
TimeSinceStart : 533.3760108947754
Training Loss : 0.11941951513290405
Done logging...




********** Iteration 201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 70.555890
best mean reward 79.645135
running time 561.569063
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 70.55589011106612
Train_BestReturn : 79.64513479485473
TimeSinceStart : 561.5690629482269
Training Loss : 1.0150630474090576
Done logging...




********** Iteration 211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 78.959058
best mean reward 79.645135
running time 586.709288
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 78.95905805553177
Train_BestReturn : 79.64513479485473
TimeSinceStart : 586.7092876434326
Training Loss : 0.5750131607055664
Done logging...




********** Iteration 221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 89.375896
best mean reward 89.375896
running time 610.935735
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 89.37589615468337
Train_BestReturn : 89.37589615468337
TimeSinceStart : 610.9357354640961
Training Loss : 0.6733856201171875
Done logging...




********** Iteration 231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 83.006611
best mean reward 89.375896
running time 636.201854
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 83.0066105501014
Train_BestReturn : 89.37589615468337
TimeSinceStart : 636.2018537521362
Training Loss : 0.23786520957946777
Done logging...




********** Iteration 241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 92.893199
best mean reward 92.893199
running time 662.743390
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 92.89319919790547
Train_BestReturn : 92.89319919790547
TimeSinceStart : 662.7433896064758
Training Loss : 0.1731906533241272
Done logging...




********** Iteration 251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 111.514261
best mean reward 111.514261
running time 688.549659
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 111.51426098781621
Train_BestReturn : 111.51426098781621
TimeSinceStart : 688.5496594905853
Training Loss : 0.08627170324325562
Done logging...




********** Iteration 261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 120.740360
best mean reward 120.740360
running time 711.253824
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 120.74036026181933
Train_BestReturn : 120.74036026181933
TimeSinceStart : 711.2538242340088
Training Loss : 0.16796737909317017
Done logging...




********** Iteration 271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 123.373515
best mean reward 123.373515
running time 732.974526
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 123.37351538940962
Train_BestReturn : 123.37351538940962
TimeSinceStart : 732.9745259284973
Training Loss : 0.18623444437980652
Done logging...




********** Iteration 281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 123.418139
best mean reward 123.418139
running time 755.640913
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 123.41813893796329
Train_BestReturn : 123.41813893796329
TimeSinceStart : 755.6409134864807
Training Loss : 0.14528413116931915
Done logging...




********** Iteration 291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 126.363388
best mean reward 126.363388
running time 777.321082
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 126.36338768291712
Train_BestReturn : 126.36338768291712
TimeSinceStart : 777.3210816383362
Training Loss : 0.2840855121612549
Done logging...




********** Iteration 301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 130.481145
best mean reward 130.481145
running time 798.814753
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 130.48114486603524
Train_BestReturn : 130.48114486603524
TimeSinceStart : 798.8147525787354
Training Loss : 1.4853262901306152
Done logging...




********** Iteration 311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 124.402035
best mean reward 130.481145
running time 821.311173
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 124.4020351167838
Train_BestReturn : 130.48114486603524
TimeSinceStart : 821.3111729621887
Training Loss : 0.12869766354560852
Done logging...




********** Iteration 321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 121.700354
best mean reward 130.481145
running time 842.621634
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 121.70035427841368
Train_BestReturn : 130.48114486603524
TimeSinceStart : 842.6216337680817
Training Loss : 0.5267210006713867
Done logging...




********** Iteration 331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 119.585165
best mean reward 130.481145
running time 863.625006
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 119.58516498213731
Train_BestReturn : 130.48114486603524
TimeSinceStart : 863.6250059604645
Training Loss : 0.31739914417266846
Done logging...




********** Iteration 341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 96.973498
best mean reward 130.481145
running time 884.747265
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 96.97349819060335
Train_BestReturn : 130.48114486603524
TimeSinceStart : 884.747264623642
Training Loss : 0.2289041131734848
Done logging...




********** Iteration 351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 100.044719
best mean reward 130.481145
running time 905.825545
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 100.04471899357148
Train_BestReturn : 130.48114486603524
TimeSinceStart : 905.8255453109741
Training Loss : 0.5390881299972534
Done logging...




********** Iteration 361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 118.911045
best mean reward 130.481145
running time 926.590579
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 118.91104537227757
Train_BestReturn : 130.48114486603524
TimeSinceStart : 926.5905792713165
Training Loss : 0.23252999782562256
Done logging...




********** Iteration 371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 109.466215
best mean reward 130.481145
running time 947.455028
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 109.46621461286482
Train_BestReturn : 130.48114486603524
TimeSinceStart : 947.4550280570984
Training Loss : 1.6893107891082764
Done logging...




********** Iteration 381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 119.912384
best mean reward 130.481145
running time 968.033432
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 119.91238380674564
Train_BestReturn : 130.48114486603524
TimeSinceStart : 968.0334317684174
Training Loss : 0.1467071920633316
Done logging...




********** Iteration 391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 106.339324
best mean reward 130.481145
running time 988.796801
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 106.33932384036109
Train_BestReturn : 130.48114486603524
TimeSinceStart : 988.7968013286591
Training Loss : 0.9420580267906189
Done logging...




********** Iteration 401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 73.381151
best mean reward 130.481145
running time 1009.513957
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 73.38115086454778
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1009.5139565467834
Training Loss : 9.020115852355957
Done logging...




********** Iteration 411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 93.458691
best mean reward 130.481145
running time 1030.248917
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 93.45869117748616
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1030.2489168643951
Training Loss : 7.484272480010986
Done logging...




********** Iteration 421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) 97.389739
best mean reward 130.481145
running time 1050.944917
Train_EnvstepsSoFar : 430001
Train_AverageReturn : 97.38973852943064
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1050.9449174404144
Training Loss : 8.101933479309082
Done logging...




********** Iteration 431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) 103.448276
best mean reward 130.481145
running time 1071.550035
Train_EnvstepsSoFar : 440001
Train_AverageReturn : 103.4482762152151
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1071.5500345230103
Training Loss : 2.7896506786346436
Done logging...




********** Iteration 441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) 91.700578
best mean reward 130.481145
running time 1092.506164
Train_EnvstepsSoFar : 450001
Train_AverageReturn : 91.70057828892857
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1092.5061638355255
Training Loss : 4.066956043243408
Done logging...




********** Iteration 451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) 93.221221
best mean reward 130.481145
running time 1112.870738
Train_EnvstepsSoFar : 460001
Train_AverageReturn : 93.2212208271094
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1112.8707377910614
Training Loss : 0.1615479290485382
Done logging...




********** Iteration 461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) 101.205463
best mean reward 130.481145
running time 1135.044936
Train_EnvstepsSoFar : 470001
Train_AverageReturn : 101.20546343829868
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1135.0449364185333
Training Loss : 3.0413784980773926
Done logging...




********** Iteration 471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) 99.556597
best mean reward 130.481145
running time 1157.925795
Train_EnvstepsSoFar : 480001
Train_AverageReturn : 99.55659727574589
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1157.925794839859
Training Loss : 3.6637089252471924
Done logging...




********** Iteration 481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) 86.702101
best mean reward 130.481145
running time 1181.611016
Train_EnvstepsSoFar : 490001
Train_AverageReturn : 86.7021005042867
Train_BestReturn : 130.48114486603524
TimeSinceStart : 1181.6110155582428
Training Loss : 0.8761781454086304
Done logging...




********** Iteration 491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 499000 ************

Training agent...

Training agent using sampled data from replay buffer...



LOGGING TO:  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_dqn_2_LunarLander-v3_03-01-2022_13-05-26 



########################
logging outputs to  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_dqn_2_LunarLander-v3_03-01-2022_13-05-26
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.000218
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.00021839141845703125
Done logging...




********** Iteration 1000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 7000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 8000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 9000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 10000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -270.149572
best mean reward -inf
running time 17.096532
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -270.14957215716817
TimeSinceStart : 17.096532106399536
Training Loss : 3.8649916648864746
Done logging...




********** Iteration 11000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 12000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 13000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 14000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 15000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 16000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 17000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 18000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 19000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 20000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -228.102578
best mean reward -inf
running time 41.272287
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -228.10257837659822
TimeSinceStart : 41.272287130355835
Training Loss : 0.7779744863510132
Done logging...




********** Iteration 21000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 22000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 23000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 24000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 25000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 26000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 27000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 28000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 29000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 30000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -207.477189
best mean reward -207.477189
running time 68.671570
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -207.47718924334978
Train_BestReturn : -207.47718924334978
TimeSinceStart : 68.67157030105591
Training Loss : 0.3660483956336975
Done logging...




********** Iteration 31000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 32000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 33000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 34000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 35000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 36000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 37000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 38000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 39000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 40000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -187.326335
best mean reward -187.326335
running time 98.207836
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -187.3263348742403
Train_BestReturn : -187.3263348742403
TimeSinceStart : 98.20783615112305
Training Loss : 0.8699965476989746
Done logging...




********** Iteration 41000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 42000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 43000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 44000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 45000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 46000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 47000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 48000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 49000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 50000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -167.236661
best mean reward -167.236661
running time 126.770289
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -167.23666109320493
Train_BestReturn : -167.23666109320493
TimeSinceStart : 126.77028894424438
Training Loss : 0.40243351459503174
Done logging...




********** Iteration 51000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 52000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 53000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 54000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 55000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 56000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 57000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 58000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 59000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 60000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -153.999453
best mean reward -153.999453
running time 154.809272
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -153.99945342181633
Train_BestReturn : -153.99945342181633
TimeSinceStart : 154.80927228927612
Training Loss : 0.6921833753585815
Done logging...




********** Iteration 61000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 62000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 63000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 64000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 65000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 66000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 67000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 68000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 69000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 70000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -142.174420
best mean reward -142.174420
running time 183.796427
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -142.17441997889114
Train_BestReturn : -142.17441997889114
TimeSinceStart : 183.7964265346527
Training Loss : 0.20140743255615234
Done logging...




********** Iteration 71000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 72000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 73000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 74000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 75000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 76000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 77000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 78000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 79000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 80000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -118.726195
best mean reward -118.726195
running time 212.476751
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -118.72619503065458
Train_BestReturn : -118.72619503065458
TimeSinceStart : 212.47675132751465
Training Loss : 0.21154288947582245
Done logging...




********** Iteration 81000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 82000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 83000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 84000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 85000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 86000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 87000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 88000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 89000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 90000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -104.667946
best mean reward -104.667946
running time 238.583037
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -104.66794570208111
Train_BestReturn : -104.66794570208111
TimeSinceStart : 238.5830373764038
Training Loss : 0.659936249256134
Done logging...




********** Iteration 91000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 92000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 93000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 94000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 95000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 96000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 97000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 98000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 99000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -98.231208
best mean reward -98.231208
running time 266.046212
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -98.23120838651927
Train_BestReturn : -98.23120838651927
TimeSinceStart : 266.04621171951294
Training Loss : 0.07904836535453796
Done logging...




********** Iteration 101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -89.422258
best mean reward -89.422258
running time 293.502457
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -89.42225841953234
Train_BestReturn : -89.42225841953234
TimeSinceStart : 293.5024573802948
Training Loss : 0.2308671623468399
Done logging...




********** Iteration 111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -81.353216
best mean reward -81.353216
running time 320.570683
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -81.35321587683902
Train_BestReturn : -81.35321587683902
TimeSinceStart : 320.5706830024719
Training Loss : 0.1375327706336975
Done logging...




********** Iteration 121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) -60.567956
best mean reward -60.567956
running time 347.501956
Train_EnvstepsSoFar : 130001
Train_AverageReturn : -60.567956061202636
Train_BestReturn : -60.567956061202636
TimeSinceStart : 347.5019555091858
Training Loss : 0.7955428957939148
Done logging...




********** Iteration 131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) -36.650610
best mean reward -36.650610
running time 373.732774
Train_EnvstepsSoFar : 140001
Train_AverageReturn : -36.650609634900526
Train_BestReturn : -36.650609634900526
TimeSinceStart : 373.73277378082275
Training Loss : 0.09041132032871246
Done logging...




********** Iteration 141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) -4.971220
best mean reward -4.971220
running time 398.821001
Train_EnvstepsSoFar : 150001
Train_AverageReturn : -4.971220292928742
Train_BestReturn : -4.971220292928742
TimeSinceStart : 398.821001291275
Training Loss : 0.1394319087266922
Done logging...




********** Iteration 151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 30.820238
best mean reward 30.820238
running time 424.229010
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 30.820238043981547
Train_BestReturn : 30.820238043981547
TimeSinceStart : 424.2290096282959
Training Loss : 0.10331133008003235
Done logging...




********** Iteration 161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 68.859749
best mean reward 68.859749
running time 447.190017
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 68.85974860021491
Train_BestReturn : 68.85974860021491
TimeSinceStart : 447.190016746521
Training Loss : 0.1287105828523636
Done logging...




********** Iteration 171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 87.622035
best mean reward 87.622035
running time 472.489565
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 87.62203491148031
Train_BestReturn : 87.62203491148031
TimeSinceStart : 472.4895646572113
Training Loss : 0.3316107988357544
Done logging...




********** Iteration 181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 100.065612
best mean reward 100.065612
running time 496.619652
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 100.06561186972978
Train_BestReturn : 100.06561186972978
TimeSinceStart : 496.6196520328522
Training Loss : 0.10165654122829437
Done logging...




********** Iteration 191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 103.297305
best mean reward 103.297305
running time 520.711886
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 103.29730450680809
Train_BestReturn : 103.29730450680809
TimeSinceStart : 520.7118861675262
Training Loss : 0.6623286008834839
Done logging...




********** Iteration 201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 106.145941
best mean reward 106.145941
running time 543.797789
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 106.14594065076724
Train_BestReturn : 106.14594065076724
TimeSinceStart : 543.7977886199951
Training Loss : 0.9350921511650085
Done logging...




********** Iteration 211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 112.091642
best mean reward 112.091642
running time 568.842249
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 112.09164180258044
Train_BestReturn : 112.09164180258044
TimeSinceStart : 568.8422491550446
Training Loss : 0.2482801377773285
Done logging...




********** Iteration 221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 113.305877
best mean reward 113.305877
running time 592.128965
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 113.30587672055427
Train_BestReturn : 113.30587672055427
TimeSinceStart : 592.1289649009705
Training Loss : 0.2007858008146286
Done logging...




********** Iteration 231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 114.225264
best mean reward 114.225264
running time 615.661295
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 114.2252638620941
Train_BestReturn : 114.2252638620941
TimeSinceStart : 615.6612951755524
Training Loss : 1.0766841173171997
Done logging...




********** Iteration 241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 110.470295
best mean reward 114.225264
running time 639.612342
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 110.47029532657784
Train_BestReturn : 114.2252638620941
TimeSinceStart : 639.6123423576355
Training Loss : 0.30432745814323425
Done logging...




********** Iteration 251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 112.393745
best mean reward 114.225264
running time 665.147576
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 112.39374479227816
Train_BestReturn : 114.2252638620941
TimeSinceStart : 665.1475756168365
Training Loss : 0.05537267029285431
Done logging...




********** Iteration 261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 108.998322
best mean reward 114.225264
running time 688.798667
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 108.99832185254417
Train_BestReturn : 114.2252638620941
TimeSinceStart : 688.798666715622
Training Loss : 0.7488162517547607
Done logging...




********** Iteration 271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 106.559901
best mean reward 114.225264
running time 714.205554
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 106.55990054129117
Train_BestReturn : 114.2252638620941
TimeSinceStart : 714.2055542469025
Training Loss : 0.12762902677059174
Done logging...




********** Iteration 281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 111.829885
best mean reward 114.225264
running time 739.231041
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 111.82988463144542
Train_BestReturn : 114.2252638620941
TimeSinceStart : 739.2310411930084
Training Loss : 0.1508876085281372
Done logging...




********** Iteration 291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 118.191068
best mean reward 118.191068
running time 762.951985
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 118.19106821976342
Train_BestReturn : 118.19106821976342
TimeSinceStart : 762.9519846439362
Training Loss : 0.1449700891971588
Done logging...




********** Iteration 301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 132.133524
best mean reward 132.133524
running time 787.562147
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 132.13352389077446
Train_BestReturn : 132.13352389077446
TimeSinceStart : 787.5621473789215
Training Loss : 0.11961852759122849
Done logging...




********** Iteration 311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 118.760600
best mean reward 132.133524
running time 815.184277
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 118.76060012355182
Train_BestReturn : 132.13352389077446
TimeSinceStart : 815.1842772960663
Training Loss : 1.5782525539398193
Done logging...




********** Iteration 321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 123.891751
best mean reward 132.133524
running time 840.710329
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 123.89175114433347
Train_BestReturn : 132.13352389077446
TimeSinceStart : 840.710328578949
Training Loss : 0.2317027896642685
Done logging...




********** Iteration 331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 131.924772
best mean reward 132.133524
running time 863.307834
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 131.92477184662897
Train_BestReturn : 132.13352389077446
TimeSinceStart : 863.3078343868256
Training Loss : 0.10960051417350769
Done logging...




********** Iteration 341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 143.762342
best mean reward 143.762342
running time 884.450942
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 143.76234224384478
Train_BestReturn : 143.76234224384478
TimeSinceStart : 884.4509420394897
Training Loss : 0.09088746458292007
Done logging...




********** Iteration 351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 140.633343
best mean reward 143.762342
running time 906.024078
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 140.63334299337293
Train_BestReturn : 143.76234224384478
TimeSinceStart : 906.0240778923035
Training Loss : 0.1384270191192627
Done logging...




********** Iteration 361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 159.393150
best mean reward 159.393150
running time 926.476788
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 159.39315014905137
Train_BestReturn : 159.39315014905137
TimeSinceStart : 926.4767880439758
Training Loss : 0.3664569854736328
Done logging...




********** Iteration 371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 175.289223
best mean reward 175.289223
running time 946.721445
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 175.2892230801351
Train_BestReturn : 175.2892230801351
TimeSinceStart : 946.721444606781
Training Loss : 0.1445593386888504
Done logging...




********** Iteration 381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 176.822135
best mean reward 176.822135
running time 967.195544
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 176.8221352039211
Train_BestReturn : 176.8221352039211
TimeSinceStart : 967.1955444812775
Training Loss : 0.11615218222141266
Done logging...




********** Iteration 391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 184.138702
best mean reward 184.138702
running time 986.755352
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 184.1387017176789
Train_BestReturn : 184.1387017176789
TimeSinceStart : 986.7553522586823
Training Loss : 0.07370392233133316
Done logging...




********** Iteration 401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 188.242330
best mean reward 188.242330
running time 1006.436011
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 188.24232968219167
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1006.4360110759735
Training Loss : 0.13772515952587128
Done logging...




********** Iteration 411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 185.019670
best mean reward 188.242330
running time 1026.129227
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 185.01967036663677
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1026.1292271614075
Training Loss : 0.1875438243150711
Done logging...




********** Iteration 421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) 186.883530
best mean reward 188.242330
running time 1045.732590
Train_EnvstepsSoFar : 430001
Train_AverageReturn : 186.88352951648682
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1045.7325899600983
Training Loss : 0.6777468919754028
Done logging...




********** Iteration 431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) 182.412191
best mean reward 188.242330
running time 1065.094866
Train_EnvstepsSoFar : 440001
Train_AverageReturn : 182.4121913756236
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1065.0948655605316
Training Loss : 1.5206518173217773
Done logging...




********** Iteration 441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) 151.992738
best mean reward 188.242330
running time 1084.385782
Train_EnvstepsSoFar : 450001
Train_AverageReturn : 151.99273821408465
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1084.3857822418213
Training Loss : 0.39547106623649597
Done logging...




********** Iteration 451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) 161.930255
best mean reward 188.242330
running time 1103.752584
Train_EnvstepsSoFar : 460001
Train_AverageReturn : 161.9302550608607
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1103.7525844573975
Training Loss : 0.3590911328792572
Done logging...




********** Iteration 461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) 147.926471
best mean reward 188.242330
running time 1123.142673
Train_EnvstepsSoFar : 470001
Train_AverageReturn : 147.9264711914542
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1123.142672777176
Training Loss : 0.2490677535533905
Done logging...




********** Iteration 471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) 140.958516
best mean reward 188.242330
running time 1142.513859
Train_EnvstepsSoFar : 480001
Train_AverageReturn : 140.95851611512876
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1142.513858795166
Training Loss : 0.16809554398059845
Done logging...




********** Iteration 481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) 103.683146
best mean reward 188.242330
running time 1161.578031
Train_EnvstepsSoFar : 490001
Train_AverageReturn : 103.68314570384965
Train_BestReturn : 188.24232968219167
TimeSinceStart : 1161.5780308246613
Training Loss : 2.3130457401275635
Done logging...




********** Iteration 491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 499000 ************

Training agent...

Training agent using sampled data from replay buffer...



LOGGING TO:  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_doubledqn_2_LunarLander-v3_03-01-2022_13-25-09 



########################
logging outputs to  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_doubledqn_2_LunarLander-v3_03-01-2022_13-25-09
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.000327
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0003266334533691406
Done logging...




********** Iteration 1000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 7000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 8000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 9000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 10000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -219.338526
best mean reward -inf
running time 18.915231
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -219.33852570481966
TimeSinceStart : 18.915230751037598
Training Loss : 0.8718059062957764
Done logging...




********** Iteration 11000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 12000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 13000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 14000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 15000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 16000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 17000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 18000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 19000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 20000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -194.258355
best mean reward -inf
running time 43.734309
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -194.25835485622414
TimeSinceStart : 43.73430919647217
Training Loss : 0.43757906556129456
Done logging...




********** Iteration 21000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 22000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 23000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 24000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 25000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 26000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 27000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 28000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 29000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 30000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -188.284575
best mean reward -188.284575
running time 73.167298
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -188.28457492214807
Train_BestReturn : -188.28457492214807
TimeSinceStart : 73.16729760169983
Training Loss : 0.4434139132499695
Done logging...




********** Iteration 31000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 32000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 33000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 34000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 35000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 36000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 37000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 38000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 39000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 40000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -172.544113
best mean reward -172.544113
running time 101.316125
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -172.54411260751058
Train_BestReturn : -172.54411260751058
TimeSinceStart : 101.31612539291382
Training Loss : 0.47109898924827576
Done logging...




********** Iteration 41000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 42000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 43000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 44000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 45000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 46000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 47000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 48000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 49000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 50000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -159.091802
best mean reward -159.091802
running time 132.358126
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -159.0918017600431
Train_BestReturn : -159.0918017600431
TimeSinceStart : 132.35812616348267
Training Loss : 0.4595411419868469
Done logging...




********** Iteration 51000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 52000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 53000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 54000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 55000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 56000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 57000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 58000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 59000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 60000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -138.291806
best mean reward -138.291806
running time 162.198556
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -138.29180569297333
Train_BestReturn : -138.29180569297333
TimeSinceStart : 162.19855570793152
Training Loss : 0.1639830470085144
Done logging...




********** Iteration 61000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 62000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 63000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 64000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 65000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 66000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 67000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 68000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 69000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 70000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -125.532908
best mean reward -125.532908
running time 190.006712
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -125.5329082718867
Train_BestReturn : -125.5329082718867
TimeSinceStart : 190.00671243667603
Training Loss : 0.3812854588031769
Done logging...




********** Iteration 71000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 72000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 73000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 74000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 75000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 76000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 77000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 78000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 79000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 80000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -101.636772
best mean reward -101.636772
running time 220.884643
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -101.63677188912581
Train_BestReturn : -101.63677188912581
TimeSinceStart : 220.88464260101318
Training Loss : 0.16621369123458862
Done logging...




********** Iteration 81000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 82000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 83000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 84000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 85000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 86000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 87000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 88000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 89000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 90000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -76.785720
best mean reward -76.785720
running time 248.406200
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -76.78572037888665
Train_BestReturn : -76.78572037888665
TimeSinceStart : 248.4061996936798
Training Loss : 0.21934005618095398
Done logging...




********** Iteration 91000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 92000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 93000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 94000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 95000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 96000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 97000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 98000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 99000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -62.414031
best mean reward -62.414031
running time 276.636308
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -62.41403143937744
Train_BestReturn : -62.41403143937744
TimeSinceStart : 276.63630843162537
Training Loss : 0.11722113192081451
Done logging...




********** Iteration 101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -45.718426
best mean reward -45.718426
running time 307.224603
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -45.718426241775326
Train_BestReturn : -45.718426241775326
TimeSinceStart : 307.2246026992798
Training Loss : 0.08685287833213806
Done logging...




********** Iteration 111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -9.956759
best mean reward -9.956759
running time 332.874947
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -9.956758922286594
Train_BestReturn : -9.956758922286594
TimeSinceStart : 332.87494707107544
Training Loss : 0.29718202352523804
Done logging...




********** Iteration 121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) 5.821554
best mean reward 5.821554
running time 360.474451
Train_EnvstepsSoFar : 130001
Train_AverageReturn : 5.821553882718381
Train_BestReturn : 5.821553882718381
TimeSinceStart : 360.4744510650635
Training Loss : 0.11973787099123001
Done logging...




********** Iteration 131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) 26.575881
best mean reward 26.575881
running time 389.648465
Train_EnvstepsSoFar : 140001
Train_AverageReturn : 26.57588095774552
Train_BestReturn : 26.57588095774552
TimeSinceStart : 389.6484651565552
Training Loss : 1.1752204895019531
Done logging...




********** Iteration 141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) 30.152340
best mean reward 30.152340
running time 417.182279
Train_EnvstepsSoFar : 150001
Train_AverageReturn : 30.15233950247715
Train_BestReturn : 30.15233950247715
TimeSinceStart : 417.18227887153625
Training Loss : 0.09966227412223816
Done logging...




********** Iteration 151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 61.819707
best mean reward 61.819707
running time 440.944051
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 61.819707166615935
Train_BestReturn : 61.819707166615935
TimeSinceStart : 440.9440505504608
Training Loss : 0.4171120226383209
Done logging...




********** Iteration 161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 67.228841
best mean reward 67.228841
running time 466.868402
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 67.2288412915007
Train_BestReturn : 67.2288412915007
TimeSinceStart : 466.8684024810791
Training Loss : 1.1515381336212158
Done logging...




********** Iteration 171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 73.048506
best mean reward 73.048506
running time 492.486153
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 73.04850561976409
Train_BestReturn : 73.04850561976409
TimeSinceStart : 492.4861526489258
Training Loss : 3.196450710296631
Done logging...




********** Iteration 181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 88.478207
best mean reward 88.478207
running time 516.034636
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 88.4782073294827
Train_BestReturn : 88.4782073294827
TimeSinceStart : 516.0346360206604
Training Loss : 0.4861438274383545
Done logging...




********** Iteration 191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 87.152722
best mean reward 88.478207
running time 541.712353
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 87.15272188615556
Train_BestReturn : 88.4782073294827
TimeSinceStart : 541.712352514267
Training Loss : 0.27990615367889404
Done logging...




********** Iteration 201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 95.981446
best mean reward 95.981446
running time 568.184480
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 95.9814462078935
Train_BestReturn : 95.9814462078935
TimeSinceStart : 568.1844804286957
Training Loss : 0.09614995121955872
Done logging...




********** Iteration 211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 99.132307
best mean reward 99.132307
running time 592.483653
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 99.1323069888047
Train_BestReturn : 99.1323069888047
TimeSinceStart : 592.4836525917053
Training Loss : 0.09602013975381851
Done logging...




********** Iteration 221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 91.264705
best mean reward 99.132307
running time 617.214332
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 91.26470462232936
Train_BestReturn : 99.1323069888047
TimeSinceStart : 617.2143316268921
Training Loss : 0.4734423756599426
Done logging...




********** Iteration 231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 97.579539
best mean reward 99.132307
running time 642.589611
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 97.57953870216603
Train_BestReturn : 99.1323069888047
TimeSinceStart : 642.5896108150482
Training Loss : 0.9143670797348022
Done logging...




********** Iteration 241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 99.543769
best mean reward 99.543769
running time 668.322881
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 99.54376875520347
Train_BestReturn : 99.54376875520347
TimeSinceStart : 668.3228807449341
Training Loss : 1.0730739831924438
Done logging...




********** Iteration 251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 103.735947
best mean reward 103.735947
running time 693.887019
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 103.73594717903273
Train_BestReturn : 103.73594717903273
TimeSinceStart : 693.8870191574097
Training Loss : 0.1908348947763443
Done logging...




********** Iteration 261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 105.937568
best mean reward 105.937568
running time 718.618910
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 105.93756772180157
Train_BestReturn : 105.93756772180157
TimeSinceStart : 718.6189103126526
Training Loss : 2.926363229751587
Done logging...




********** Iteration 271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 119.692317
best mean reward 119.692317
running time 742.059010
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 119.69231734587409
Train_BestReturn : 119.69231734587409
TimeSinceStart : 742.059009552002
Training Loss : 0.10820142179727554
Done logging...




********** Iteration 281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 117.479226
best mean reward 119.692317
running time 766.495643
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 117.47922640715919
Train_BestReturn : 119.69231734587409
TimeSinceStart : 766.4956426620483
Training Loss : 0.33562925457954407
Done logging...




********** Iteration 291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 118.731008
best mean reward 119.692317
running time 790.262519
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 118.73100753073444
Train_BestReturn : 119.69231734587409
TimeSinceStart : 790.2625193595886
Training Loss : 0.34865471720695496
Done logging...




********** Iteration 301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 105.755340
best mean reward 119.692317
running time 813.122140
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 105.7553400007681
Train_BestReturn : 119.69231734587409
TimeSinceStart : 813.1221399307251
Training Loss : 1.088172435760498
Done logging...




********** Iteration 311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 113.351975
best mean reward 119.692317
running time 834.988844
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 113.35197547475711
Train_BestReturn : 119.69231734587409
TimeSinceStart : 834.9888436794281
Training Loss : 0.2959008514881134
Done logging...




********** Iteration 321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 106.735016
best mean reward 119.692317
running time 856.593591
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 106.73501616590957
Train_BestReturn : 119.69231734587409
TimeSinceStart : 856.5935914516449
Training Loss : 1.2898091077804565
Done logging...




********** Iteration 331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 108.391445
best mean reward 119.692317
running time 878.026050
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 108.3914451485562
Train_BestReturn : 119.69231734587409
TimeSinceStart : 878.0260498523712
Training Loss : 0.1877390444278717
Done logging...




********** Iteration 341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 126.519704
best mean reward 126.519704
running time 899.257685
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 126.51970445404
Train_BestReturn : 126.51970445404
TimeSinceStart : 899.2576851844788
Training Loss : 1.539779543876648
Done logging...




********** Iteration 351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 114.258730
best mean reward 126.519704
running time 920.652897
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 114.25873008281293
Train_BestReturn : 126.51970445404
TimeSinceStart : 920.6528968811035
Training Loss : 1.8806054592132568
Done logging...




********** Iteration 361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 110.169432
best mean reward 126.519704
running time 941.690586
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 110.16943239713473
Train_BestReturn : 126.51970445404
TimeSinceStart : 941.6905860900879
Training Loss : 3.5348119735717773
Done logging...




********** Iteration 371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 112.186050
best mean reward 126.519704
running time 962.619339
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 112.18604963807717
Train_BestReturn : 126.51970445404
TimeSinceStart : 962.6193387508392
Training Loss : 0.3222380578517914
Done logging...




********** Iteration 381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 116.333849
best mean reward 126.519704
running time 983.419178
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 116.33384906080721
Train_BestReturn : 126.51970445404
TimeSinceStart : 983.4191777706146
Training Loss : 1.63316011428833
Done logging...




********** Iteration 391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 112.100781
best mean reward 126.519704
running time 1004.745060
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 112.10078084760855
Train_BestReturn : 126.51970445404
TimeSinceStart : 1004.7450597286224
Training Loss : 6.787250995635986
Done logging...




********** Iteration 401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 125.055466
best mean reward 126.519704
running time 1025.661729
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 125.05546577703691
Train_BestReturn : 126.51970445404
TimeSinceStart : 1025.6617288589478
Training Loss : 0.1863887906074524
Done logging...




********** Iteration 411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 124.110825
best mean reward 126.519704
running time 1046.661494
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 124.11082519964144
Train_BestReturn : 126.51970445404
TimeSinceStart : 1046.6614937782288
Training Loss : 0.108768031001091
Done logging...




********** Iteration 421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) 138.080775
best mean reward 138.080775
running time 1067.538291
Train_EnvstepsSoFar : 430001
Train_AverageReturn : 138.0807747321422
Train_BestReturn : 138.0807747321422
TimeSinceStart : 1067.5382907390594
Training Loss : 0.3554264008998871
Done logging...




********** Iteration 431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) 156.060935
best mean reward 156.060935
running time 1088.377476
Train_EnvstepsSoFar : 440001
Train_AverageReturn : 156.0609347641064
Train_BestReturn : 156.0609347641064
TimeSinceStart : 1088.3774757385254
Training Loss : 0.22098001837730408
Done logging...




********** Iteration 441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) 154.171948
best mean reward 156.060935
running time 1109.305931
Train_EnvstepsSoFar : 450001
Train_AverageReturn : 154.17194775968122
Train_BestReturn : 156.0609347641064
TimeSinceStart : 1109.30593085289
Training Loss : 0.761633574962616
Done logging...




********** Iteration 451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) 147.050437
best mean reward 156.060935
running time 1130.255409
Train_EnvstepsSoFar : 460001
Train_AverageReturn : 147.0504367268129
Train_BestReturn : 156.0609347641064
TimeSinceStart : 1130.2554094791412
Training Loss : 0.20320460200309753
Done logging...




********** Iteration 461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) 141.768363
best mean reward 156.060935
running time 1151.257086
Train_EnvstepsSoFar : 470001
Train_AverageReturn : 141.76836309644116
Train_BestReturn : 156.0609347641064
TimeSinceStart : 1151.257086277008
Training Loss : 1.6162360906600952
Done logging...




********** Iteration 471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) 93.647937
best mean reward 156.060935
running time 1172.593908
Train_EnvstepsSoFar : 480001
Train_AverageReturn : 93.64793716113599
Train_BestReturn : 156.0609347641064
TimeSinceStart : 1172.5939083099365
Training Loss : 0.3352097272872925
Done logging...




********** Iteration 481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) 66.974421
best mean reward 156.060935
running time 1193.298252
Train_EnvstepsSoFar : 490001
Train_AverageReturn : 66.97442130713881
Train_BestReturn : 156.0609347641064
TimeSinceStart : 1193.2982518672943
Training Loss : 0.5737305879592896
Done logging...




********** Iteration 491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 499000 ************

Training agent...

Training agent using sampled data from replay buffer...



LOGGING TO:  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_dqn_3_LunarLander-v3_03-01-2022_13-45-25 



########################
logging outputs to  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_dqn_3_LunarLander-v3_03-01-2022_13-45-25
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.000323
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0003228187561035156
Done logging...




********** Iteration 1000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 7000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 8000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 9000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 10000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -224.350242
best mean reward -inf
running time 16.602529
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -224.3502422789268
TimeSinceStart : 16.60252857208252
Training Loss : 4.160402297973633
Done logging...




********** Iteration 11000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 12000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 13000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 14000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 15000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 16000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 17000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 18000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 19000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 20000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -177.637741
best mean reward -177.637741
running time 37.895034
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -177.6377410750569
Train_BestReturn : -177.6377410750569
TimeSinceStart : 37.895034074783325
Training Loss : 0.8271803259849548
Done logging...




********** Iteration 21000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 22000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 23000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 24000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 25000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 26000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 27000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 28000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 29000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 30000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -168.861939
best mean reward -168.861939
running time 65.867688
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -168.86193864381897
Train_BestReturn : -168.86193864381897
TimeSinceStart : 65.86768770217896
Training Loss : 0.3658446967601776
Done logging...




********** Iteration 31000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 32000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 33000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 34000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 35000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 36000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 37000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 38000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 39000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 40000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -154.219911
best mean reward -154.219911
running time 93.969082
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -154.21991132684983
Train_BestReturn : -154.21991132684983
TimeSinceStart : 93.96908211708069
Training Loss : 0.5439980030059814
Done logging...




********** Iteration 41000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 42000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 43000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 44000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 45000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 46000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 47000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 48000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 49000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 50000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -145.618287
best mean reward -145.618287
running time 120.309595
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -145.61828669387538
Train_BestReturn : -145.61828669387538
TimeSinceStart : 120.3095953464508
Training Loss : 0.25546276569366455
Done logging...




********** Iteration 51000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 52000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 53000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 54000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 55000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 56000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 57000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 58000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 59000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 60000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -131.005341
best mean reward -131.005341
running time 147.900961
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -131.0053414320587
Train_BestReturn : -131.0053414320587
TimeSinceStart : 147.90096139907837
Training Loss : 0.3466273248195648
Done logging...




********** Iteration 61000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 62000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 63000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 64000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 65000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 66000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 67000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 68000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 69000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 70000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -125.130200
best mean reward -125.130200
running time 174.675398
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -125.13020013645121
Train_BestReturn : -125.13020013645121
TimeSinceStart : 174.67539811134338
Training Loss : 0.27238473296165466
Done logging...




********** Iteration 71000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 72000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 73000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 74000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 75000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 76000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 77000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 78000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 79000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 80000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -113.706933
best mean reward -113.706933
running time 202.639215
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -113.70693327872051
Train_BestReturn : -113.70693327872051
TimeSinceStart : 202.63921451568604
Training Loss : 0.2387927770614624
Done logging...




********** Iteration 81000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 82000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 83000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 84000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 85000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 86000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 87000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 88000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 89000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 90000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -108.347674
best mean reward -108.347674
running time 233.773494
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -108.34767372264687
Train_BestReturn : -108.34767372264687
TimeSinceStart : 233.7734944820404
Training Loss : 0.16580578684806824
Done logging...




********** Iteration 91000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 92000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 93000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 94000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 95000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 96000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 97000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 98000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 99000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -99.770512
best mean reward -99.770512
running time 262.216330
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -99.77051219231149
Train_BestReturn : -99.77051219231149
TimeSinceStart : 262.2163302898407
Training Loss : 0.14738509058952332
Done logging...




********** Iteration 101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -96.561814
best mean reward -96.561814
running time 293.620190
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -96.56181392087386
Train_BestReturn : -96.56181392087386
TimeSinceStart : 293.6201899051666
Training Loss : 0.12332470715045929
Done logging...




********** Iteration 111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -85.923255
best mean reward -85.923255
running time 322.712115
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -85.92325451013554
Train_BestReturn : -85.92325451013554
TimeSinceStart : 322.7121148109436
Training Loss : 0.21237534284591675
Done logging...




********** Iteration 121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) -69.464765
best mean reward -69.464765
running time 348.523936
Train_EnvstepsSoFar : 130001
Train_AverageReturn : -69.46476472781441
Train_BestReturn : -69.46476472781441
TimeSinceStart : 348.52393555641174
Training Loss : 1.2323284149169922
Done logging...




********** Iteration 131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) -46.090484
best mean reward -46.090484
running time 373.794400
Train_EnvstepsSoFar : 140001
Train_AverageReturn : -46.0904836437114
Train_BestReturn : -46.0904836437114
TimeSinceStart : 373.79439973831177
Training Loss : 0.135927215218544
Done logging...




********** Iteration 141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) -16.077596
best mean reward -16.077596
running time 398.085286
Train_EnvstepsSoFar : 150001
Train_AverageReturn : -16.07759584837136
Train_BestReturn : -16.07759584837136
TimeSinceStart : 398.0852859020233
Training Loss : 0.21066123247146606
Done logging...




********** Iteration 151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 8.285709
best mean reward 8.285709
running time 423.808262
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 8.285708989206974
Train_BestReturn : 8.285708989206974
TimeSinceStart : 423.8082616329193
Training Loss : 0.08540739119052887
Done logging...




********** Iteration 161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 44.984717
best mean reward 44.984717
running time 448.768097
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 44.98471746243215
Train_BestReturn : 44.98471746243215
TimeSinceStart : 448.7680971622467
Training Loss : 0.25826823711395264
Done logging...




********** Iteration 171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 73.180649
best mean reward 73.180649
running time 471.914315
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 73.18064860067356
Train_BestReturn : 73.18064860067356
TimeSinceStart : 471.9143154621124
Training Loss : 1.1973131895065308
Done logging...




********** Iteration 181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 92.301193
best mean reward 92.301193
running time 495.741082
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 92.30119291248756
Train_BestReturn : 92.30119291248756
TimeSinceStart : 495.7410817146301
Training Loss : 0.0878111720085144
Done logging...




********** Iteration 191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 94.487186
best mean reward 94.487186
running time 520.459405
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 94.48718607240852
Train_BestReturn : 94.48718607240852
TimeSinceStart : 520.4594051837921
Training Loss : 1.077290415763855
Done logging...




********** Iteration 201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 85.734077
best mean reward 94.487186
running time 548.100363
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 85.73407686090565
Train_BestReturn : 94.48718607240852
TimeSinceStart : 548.1003625392914
Training Loss : 0.2346571832895279
Done logging...




********** Iteration 211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 86.120096
best mean reward 94.487186
running time 570.534866
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 86.12009645995867
Train_BestReturn : 94.48718607240852
TimeSinceStart : 570.5348656177521
Training Loss : 0.2345786690711975
Done logging...




********** Iteration 221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 81.346629
best mean reward 94.487186
running time 594.675884
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 81.34662885473207
Train_BestReturn : 94.48718607240852
TimeSinceStart : 594.6758842468262
Training Loss : 0.12395676970481873
Done logging...




********** Iteration 231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 86.764763
best mean reward 94.487186
running time 617.862094
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 86.76476345949632
Train_BestReturn : 94.48718607240852
TimeSinceStart : 617.8620941638947
Training Loss : 0.17523881793022156
Done logging...




********** Iteration 241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 103.301176
best mean reward 103.301176
running time 641.336484
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 103.30117623418099
Train_BestReturn : 103.30117623418099
TimeSinceStart : 641.3364837169647
Training Loss : 0.20437785983085632
Done logging...




********** Iteration 251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 103.279774
best mean reward 103.301176
running time 666.217141
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 103.27977366570646
Train_BestReturn : 103.30117623418099
TimeSinceStart : 666.2171409130096
Training Loss : 0.1212044507265091
Done logging...




********** Iteration 261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 101.251738
best mean reward 103.301176
running time 690.455528
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 101.25173796965326
Train_BestReturn : 103.30117623418099
TimeSinceStart : 690.4555284976959
Training Loss : 2.276048183441162
Done logging...




********** Iteration 271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 88.724287
best mean reward 103.301176
running time 715.479940
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 88.72428672832079
Train_BestReturn : 103.30117623418099
TimeSinceStart : 715.4799404144287
Training Loss : 0.16342376172542572
Done logging...




********** Iteration 281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 64.641748
best mean reward 103.301176
running time 741.113857
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 64.64174804815063
Train_BestReturn : 103.30117623418099
TimeSinceStart : 741.1138565540314
Training Loss : 0.2990815043449402
Done logging...




********** Iteration 291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 49.881565
best mean reward 103.301176
running time 765.867896
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 49.88156495064888
Train_BestReturn : 103.30117623418099
TimeSinceStart : 765.8678956031799
Training Loss : 0.3108389377593994
Done logging...




********** Iteration 301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 55.706024
best mean reward 103.301176
running time 789.570560
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 55.70602386191161
Train_BestReturn : 103.30117623418099
TimeSinceStart : 789.5705597400665
Training Loss : 0.12704066932201385
Done logging...




********** Iteration 311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 61.158209
best mean reward 103.301176
running time 815.928799
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 61.15820916984747
Train_BestReturn : 103.30117623418099
TimeSinceStart : 815.9287989139557
Training Loss : 0.11274736374616623
Done logging...




********** Iteration 321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 89.968860
best mean reward 103.301176
running time 839.616223
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 89.96886038631618
Train_BestReturn : 103.30117623418099
TimeSinceStart : 839.616222858429
Training Loss : 0.16264237463474274
Done logging...




********** Iteration 331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 93.546664
best mean reward 103.301176
running time 861.805982
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 93.54666397500456
Train_BestReturn : 103.30117623418099
TimeSinceStart : 861.8059816360474
Training Loss : 0.4363645911216736
Done logging...




********** Iteration 341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 114.696231
best mean reward 114.696231
running time 882.194515
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 114.69623067606966
Train_BestReturn : 114.69623067606966
TimeSinceStart : 882.1945145130157
Training Loss : 0.21793261170387268
Done logging...




********** Iteration 351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 131.919532
best mean reward 131.919532
running time 903.033308
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 131.91953212481604
Train_BestReturn : 131.91953212481604
TimeSinceStart : 903.0333080291748
Training Loss : 1.0759731531143188
Done logging...




********** Iteration 361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 132.759332
best mean reward 132.759332
running time 922.912558
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 132.75933231003697
Train_BestReturn : 132.75933231003697
TimeSinceStart : 922.9125576019287
Training Loss : 0.1804799884557724
Done logging...




********** Iteration 371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 138.469324
best mean reward 138.469324
running time 942.981930
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 138.46932373849484
Train_BestReturn : 138.46932373849484
TimeSinceStart : 942.9819302558899
Training Loss : 1.5313900709152222
Done logging...




********** Iteration 381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 118.918719
best mean reward 138.469324
running time 962.604084
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 118.91871930968405
Train_BestReturn : 138.46932373849484
TimeSinceStart : 962.604083776474
Training Loss : 0.13342353701591492
Done logging...




********** Iteration 391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 110.973924
best mean reward 138.469324
running time 981.975049
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 110.97392427844014
Train_BestReturn : 138.46932373849484
TimeSinceStart : 981.9750485420227
Training Loss : 1.252561092376709
Done logging...




********** Iteration 401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 97.481168
best mean reward 138.469324
running time 1001.540648
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 97.48116810829556
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1001.5406484603882
Training Loss : 7.637635231018066
Done logging...




********** Iteration 411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 18.391365
best mean reward 138.469324
running time 1020.703524
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 18.391365040592174
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1020.7035236358643
Training Loss : 1.2784833908081055
Done logging...




********** Iteration 421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) -1.616904
best mean reward 138.469324
running time 1039.884945
Train_EnvstepsSoFar : 430001
Train_AverageReturn : -1.616903795144775
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1039.8849453926086
Training Loss : 0.15276561677455902
Done logging...




********** Iteration 431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) -69.528701
best mean reward 138.469324
running time 1058.769245
Train_EnvstepsSoFar : 440001
Train_AverageReturn : -69.5287008166231
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1058.7692453861237
Training Loss : 1.245084285736084
Done logging...




********** Iteration 441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) -116.560772
best mean reward 138.469324
running time 1078.037295
Train_EnvstepsSoFar : 450001
Train_AverageReturn : -116.56077208154024
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1078.037294626236
Training Loss : 0.3204982578754425
Done logging...




********** Iteration 451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) -116.105976
best mean reward 138.469324
running time 1097.219920
Train_EnvstepsSoFar : 460001
Train_AverageReturn : -116.10597558859861
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1097.2199203968048
Training Loss : 7.1736578941345215
Done logging...




********** Iteration 461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) -142.634195
best mean reward 138.469324
running time 1116.508567
Train_EnvstepsSoFar : 470001
Train_AverageReturn : -142.6341947674888
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1116.5085670948029
Training Loss : 0.6721913814544678
Done logging...




********** Iteration 471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) -156.556709
best mean reward 138.469324
running time 1135.436856
Train_EnvstepsSoFar : 480001
Train_AverageReturn : -156.5567092468622
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1135.4368557929993
Training Loss : 1.5728574991226196
Done logging...




********** Iteration 481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) -170.701007
best mean reward 138.469324
running time 1154.454547
Train_EnvstepsSoFar : 490001
Train_AverageReturn : -170.70100697362977
Train_BestReturn : 138.46932373849484
TimeSinceStart : 1154.454547405243
Training Loss : 24.610610961914062
Done logging...




********** Iteration 491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 499000 ************

Training agent...

Training agent using sampled data from replay buffer...



LOGGING TO:  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_doubledqn_3_LunarLander-v3_03-01-2022_14-05-01 



########################
logging outputs to  /home/zzh/Desktop/hw3/cs285/scripts/../../data/q2_doubledqn_3_LunarLander-v3_03-01-2022_14-05-01
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.000320
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0003199577331542969
Done logging...




********** Iteration 1000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 7000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 8000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 9000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 10000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -235.585834
best mean reward -inf
running time 17.423560
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -235.58583406686375
TimeSinceStart : 17.42355990409851
Training Loss : 0.19480249285697937
Done logging...




********** Iteration 11000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 12000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 13000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 14000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 15000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 16000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 17000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 18000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 19000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 20000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -188.344926
best mean reward -188.344926
running time 40.503769
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -188.34492595567986
Train_BestReturn : -188.34492595567986
TimeSinceStart : 40.503769397735596
Training Loss : 0.5564441680908203
Done logging...




********** Iteration 21000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 22000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 23000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 24000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 25000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 26000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 27000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 28000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 29000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 30000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -166.523306
best mean reward -166.523306
running time 68.927084
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -166.5233055499528
Train_BestReturn : -166.5233055499528
TimeSinceStart : 68.92708373069763
Training Loss : 3.1966335773468018
Done logging...




********** Iteration 31000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 32000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 33000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 34000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 35000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 36000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 37000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 38000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 39000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 40000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -154.550761
best mean reward -154.550761
running time 97.076187
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -154.55076089740297
Train_BestReturn : -154.55076089740297
TimeSinceStart : 97.07618737220764
Training Loss : 0.5297034382820129
Done logging...




********** Iteration 41000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 42000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 43000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 44000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 45000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 46000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 47000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 48000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 49000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 50000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -140.597704
best mean reward -140.597704
running time 127.663183
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -140.59770418676243
Train_BestReturn : -140.59770418676243
TimeSinceStart : 127.6631829738617
Training Loss : 0.347760945558548
Done logging...




********** Iteration 51000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 52000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 53000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 54000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 55000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 56000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 57000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 58000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 59000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 60000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -118.350352
best mean reward -118.350352
running time 158.052560
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -118.35035244652043
Train_BestReturn : -118.35035244652043
TimeSinceStart : 158.05256009101868
Training Loss : 0.3095954656600952
Done logging...




********** Iteration 61000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 62000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 63000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 64000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 65000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 66000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 67000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 68000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 69000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 70000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -98.303213
best mean reward -98.303213
running time 184.388792
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -98.30321257623264
Train_BestReturn : -98.30321257623264
TimeSinceStart : 184.3887917995453
Training Loss : 0.44906407594680786
Done logging...




********** Iteration 71000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 72000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 73000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 74000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 75000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 76000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 77000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 78000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 79000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 80000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -86.089528
best mean reward -86.089528
running time 212.612238
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -86.08952799373088
Train_BestReturn : -86.08952799373088
TimeSinceStart : 212.61223793029785
Training Loss : 0.9672636389732361
Done logging...




********** Iteration 81000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 82000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 83000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 84000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 85000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 86000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 87000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 88000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 89000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 90000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -59.904425
best mean reward -59.904425
running time 239.501429
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -59.90442533376365
Train_BestReturn : -59.90442533376365
TimeSinceStart : 239.50142884254456
Training Loss : 0.2822650671005249
Done logging...




********** Iteration 91000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 92000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 93000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 94000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 95000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 96000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 97000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 98000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 99000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -53.560023
best mean reward -53.560023
running time 268.230114
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -53.56002308581519
Train_BestReturn : -53.56002308581519
TimeSinceStart : 268.2301139831543
Training Loss : 0.3080879747867584
Done logging...




********** Iteration 101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -42.254101
best mean reward -42.254101
running time 297.208357
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -42.254100798792976
Train_BestReturn : -42.254100798792976
TimeSinceStart : 297.2083566188812
Training Loss : 0.17161954939365387
Done logging...




********** Iteration 111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -13.911470
best mean reward -13.911470
running time 323.603258
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -13.911470343902426
Train_BestReturn : -13.911470343902426
TimeSinceStart : 323.603257894516
Training Loss : 0.8951051235198975
Done logging...




********** Iteration 121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) -4.647821
best mean reward -4.647821
running time 350.716033
Train_EnvstepsSoFar : 130001
Train_AverageReturn : -4.6478213114748375
Train_BestReturn : -4.6478213114748375
TimeSinceStart : 350.7160325050354
Training Loss : 1.2265433073043823
Done logging...




********** Iteration 131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) 4.598932
best mean reward 4.598932
running time 379.042856
Train_EnvstepsSoFar : 140001
Train_AverageReturn : 4.598931774490567
Train_BestReturn : 4.598931774490567
TimeSinceStart : 379.0428555011749
Training Loss : 0.27187633514404297
Done logging...




********** Iteration 141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) 18.733112
best mean reward 18.733112
running time 406.550784
Train_EnvstepsSoFar : 150001
Train_AverageReturn : 18.733111723692044
Train_BestReturn : 18.733111723692044
TimeSinceStart : 406.5507843494415
Training Loss : 0.08431348204612732
Done logging...




********** Iteration 151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 44.967186
best mean reward 44.967186
running time 431.713060
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 44.967185891023576
Train_BestReturn : 44.967185891023576
TimeSinceStart : 431.7130603790283
Training Loss : 0.13173991441726685
Done logging...




********** Iteration 161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 50.220959
best mean reward 50.220959
running time 458.683999
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 50.22095915083369
Train_BestReturn : 50.22095915083369
TimeSinceStart : 458.6839985847473
Training Loss : 0.18755711615085602
Done logging...




********** Iteration 171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 56.987490
best mean reward 56.987490
running time 485.277031
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 56.98748969804461
Train_BestReturn : 56.98748969804461
TimeSinceStart : 485.2770311832428
Training Loss : 0.25668907165527344
Done logging...




********** Iteration 181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 69.080675
best mean reward 69.080675
running time 512.802010
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 69.08067496542436
Train_BestReturn : 69.08067496542436
TimeSinceStart : 512.8020100593567
Training Loss : 0.18324404954910278
Done logging...




********** Iteration 191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 68.029951
best mean reward 69.080675
running time 540.177613
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 68.02995097082803
Train_BestReturn : 69.08067496542436
TimeSinceStart : 540.1776130199432
Training Loss : 1.4304215908050537
Done logging...




********** Iteration 201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 68.577731
best mean reward 69.080675
running time 567.556515
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 68.57773141488258
Train_BestReturn : 69.08067496542436
TimeSinceStart : 567.5565147399902
Training Loss : 2.784241199493408
Done logging...




********** Iteration 211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 59.057771
best mean reward 69.080675
running time 595.815933
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 59.05777131476334
Train_BestReturn : 69.08067496542436
TimeSinceStart : 595.8159332275391
Training Loss : 0.19068637490272522
Done logging...




********** Iteration 221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 71.153129
best mean reward 71.153129
running time 621.827217
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 71.15312918218484
Train_BestReturn : 71.15312918218484
TimeSinceStart : 621.8272171020508
Training Loss : 3.042762279510498
Done logging...




********** Iteration 231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 75.162958
best mean reward 75.162958
running time 650.562569
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 75.16295798296296
Train_BestReturn : 75.16295798296296
TimeSinceStart : 650.5625691413879
Training Loss : 1.0459364652633667
Done logging...




********** Iteration 241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 66.161945
best mean reward 75.162958
running time 679.044123
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 66.1619453257996
Train_BestReturn : 75.16295798296296
TimeSinceStart : 679.04412317276
Training Loss : 1.6396193504333496
Done logging...




********** Iteration 251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 75.390190
best mean reward 75.390190
running time 704.007472
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 75.39018950296628
Train_BestReturn : 75.39018950296628
TimeSinceStart : 704.007472038269
Training Loss : 0.1016744077205658
Done logging...




********** Iteration 261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 80.238363
best mean reward 80.238363
running time 729.832070
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 80.23836312273573
Train_BestReturn : 80.23836312273573
TimeSinceStart : 729.8320698738098
Training Loss : 0.19428379833698273
Done logging...




********** Iteration 271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 81.814271
best mean reward 81.814271
running time 757.780694
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 81.8142711696751
Train_BestReturn : 81.8142711696751
TimeSinceStart : 757.7806944847107
Training Loss : 0.16801394522190094
Done logging...




********** Iteration 281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 95.975221
best mean reward 95.975221
running time 782.164118
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 95.97522119234497
Train_BestReturn : 95.97522119234497
TimeSinceStart : 782.1641182899475
Training Loss : 0.8628315925598145
Done logging...




********** Iteration 291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 109.508520
best mean reward 109.508520
running time 805.421298
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 109.50852023883199
Train_BestReturn : 109.50852023883199
TimeSinceStart : 805.4212980270386
Training Loss : 0.6623549461364746
Done logging...




********** Iteration 301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 126.718735
best mean reward 126.718735
running time 829.216677
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 126.7187351827636
Train_BestReturn : 126.7187351827636
TimeSinceStart : 829.2166769504547
Training Loss : 0.5951370000839233
Done logging...




********** Iteration 311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 145.465108
best mean reward 145.465108
running time 851.854490
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 145.46510752764001
Train_BestReturn : 145.46510752764001
TimeSinceStart : 851.8544902801514
Training Loss : 3.5547001361846924
Done logging...




********** Iteration 321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 170.865189
best mean reward 170.865189
running time 873.891157
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 170.8651894635742
Train_BestReturn : 170.8651894635742
TimeSinceStart : 873.89115691185
Training Loss : 0.2717705965042114
Done logging...




********** Iteration 331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 169.993339
best mean reward 170.865189
running time 896.058940
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 169.99333946326882
Train_BestReturn : 170.8651894635742
TimeSinceStart : 896.0589399337769
Training Loss : 0.28625577688217163
Done logging...




********** Iteration 341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 173.633383
best mean reward 173.633383
running time 918.017112
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 173.633382820476
Train_BestReturn : 173.633382820476
TimeSinceStart : 918.0171122550964
Training Loss : 0.9315055012702942
Done logging...




********** Iteration 351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 180.622903
best mean reward 180.622903
running time 940.266253
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 180.62290263486778
Train_BestReturn : 180.62290263486778
TimeSinceStart : 940.2662532329559
Training Loss : 0.5331844091415405
Done logging...




********** Iteration 361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 190.163092
best mean reward 190.163092
running time 962.359870
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 190.16309249834939
Train_BestReturn : 190.16309249834939
TimeSinceStart : 962.3598701953888
Training Loss : 1.0605583190917969
Done logging...




********** Iteration 371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 198.589529
best mean reward 198.589529
running time 983.996879
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 198.5895287316297
Train_BestReturn : 198.5895287316297
TimeSinceStart : 983.996878862381
Training Loss : 0.3388681411743164
Done logging...




********** Iteration 381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 182.461960
best mean reward 198.589529
running time 1005.657685
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 182.46195950043509
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1005.6576850414276
Training Loss : 2.621823787689209
Done logging...




********** Iteration 391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 169.986092
best mean reward 198.589529
running time 1026.968301
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 169.9860919618242
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1026.9683010578156
Training Loss : 0.1532077193260193
Done logging...




********** Iteration 401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 177.990721
best mean reward 198.589529
running time 1048.540795
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 177.99072083404414
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1048.5407950878143
Training Loss : 0.22497880458831787
Done logging...




********** Iteration 411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 181.748077
best mean reward 198.589529
running time 1069.291095
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 181.74807716174712
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1069.2910947799683
Training Loss : 1.1142839193344116
Done logging...




********** Iteration 421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) 176.219911
best mean reward 198.589529
running time 1090.092082
Train_EnvstepsSoFar : 430001
Train_AverageReturn : 176.2199112953888
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1090.0920820236206
Training Loss : 1.9090406894683838
Done logging...




********** Iteration 431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) 168.044554
best mean reward 198.589529
running time 1110.496322
Train_EnvstepsSoFar : 440001
Train_AverageReturn : 168.04455373767019
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1110.4963219165802
Training Loss : 1.0427155494689941
Done logging...




********** Iteration 441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) 142.717008
best mean reward 198.589529
running time 1131.008887
Train_EnvstepsSoFar : 450001
Train_AverageReturn : 142.7170080296149
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1131.0088872909546
Training Loss : 0.227056622505188
Done logging...




********** Iteration 451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) 139.640151
best mean reward 198.589529
running time 1151.573981
Train_EnvstepsSoFar : 460001
Train_AverageReturn : 139.64015086823562
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1151.5739810466766
Training Loss : 0.11012867838144302
Done logging...




********** Iteration 461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) 121.158819
best mean reward 198.589529
running time 1172.223087
Train_EnvstepsSoFar : 470001
Train_AverageReturn : 121.15881879366643
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1172.2230868339539
Training Loss : 2.720614433288574
Done logging...




********** Iteration 471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) 126.222435
best mean reward 198.589529
running time 1192.724304
Train_EnvstepsSoFar : 480001
Train_AverageReturn : 126.22243513806299
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1192.7243044376373
Training Loss : 4.069728851318359
Done logging...




********** Iteration 481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) 109.561423
best mean reward 198.589529
running time 1213.302739
Train_EnvstepsSoFar : 490001
Train_AverageReturn : 109.56142333870962
Train_BestReturn : 198.5895287316297
TimeSinceStart : 1213.3027393817902
Training Loss : 0.2161121666431427
Done logging...




********** Iteration 491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 499000 ************

Training agent...

Training agent using sampled data from replay buffer...
