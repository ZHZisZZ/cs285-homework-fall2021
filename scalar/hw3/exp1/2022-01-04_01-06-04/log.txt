#!/bin/bash
export PYTHONPATH=.

DATA_DIR=./data
EXP_DIR=${DATA_DIR}/exp1/`date +%Y-%m-%d_%H-%M-%S`
LOG_PATH=${EXP_DIR}/log.txt
# clear other data files
rm -rf $(find ${DATA_DIR} -maxdepth 1 -name '*q1*' 2> /dev/null)
# create data directory for this experiment and logfile
mkdir -p $EXP_DIR; touch $LOG_PATH
# dump experiment commands and hyperparameters (this file) to logfile
cat $0 >> $LOG_PATH; echo "\n\n" >> $LOG_PATH

python cs285/scripts/run_hw3_dqn.py \
    --env_name MsPacman-v0 --exp_name q1_mspacman \
    --video_log_freq 500000 >> $LOG_PATH

# python cs285/scripts/run_hw3_dqn.py \
#     --env_name PongNoFrameskip-v4 --exp_name q1_pong \
#     --video_log_freq 500000 >> $LOG_PATH

# move data file to experiment data directory
mv $(find ${DATA_DIR} -maxdepth 1 -name '*q1*' 2> /dev/null) $EXP_DIR






LOGGING TO:  /home/zzh/Desktop/CS285/cs285-homework-fall2021/hw3/cs285/scripts/../../data/q1_mspacman_MsPacman-v0_2022-01-04_01-06-04 



########################
logging outputs to  /home/zzh/Desktop/CS285/cs285-homework-fall2021/hw3/cs285/scripts/../../data/q1_mspacman_MsPacman-v0_2022-01-04_01-06-04
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 2.563483
Train_EnvstepsSoFar : 1
TimeSinceStart : 2.5634829998016357
Done logging...




********** Iteration 1000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 7000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 8000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 9000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 10000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) 428.305085
best mean reward -inf
running time 30.405827
Train_EnvstepsSoFar : 10001
Train_AverageReturn : 428.3050847457627
TimeSinceStart : 30.405827283859253
Done logging...




********** Iteration 11000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 12000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 13000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 14000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 15000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 16000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 17000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 18000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 19000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 20000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) 440.700000
best mean reward 440.700000
running time 57.565544
Train_EnvstepsSoFar : 20001
Train_AverageReturn : 440.7
Train_BestReturn : 440.7
TimeSinceStart : 57.56554388999939
Done logging...




********** Iteration 21000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 22000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 23000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 24000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 25000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 26000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 27000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 28000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 29000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 30000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) 447.800000
best mean reward 447.800000
running time 84.775668
Train_EnvstepsSoFar : 30001
Train_AverageReturn : 447.8
Train_BestReturn : 447.8
TimeSinceStart : 84.77566814422607
Done logging...




********** Iteration 31000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 32000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 33000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 34000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 35000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 36000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 37000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 38000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 39000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 40000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) 398.600000
best mean reward 447.800000
running time 112.594730
Train_EnvstepsSoFar : 40001
Train_AverageReturn : 398.6
Train_BestReturn : 447.8
TimeSinceStart : 112.59472990036011
Done logging...




********** Iteration 41000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 42000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 43000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 44000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 45000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 46000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 47000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 48000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 49000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 50000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) 417.300000
best mean reward 447.800000
running time 140.518503
Train_EnvstepsSoFar : 50001
Train_AverageReturn : 417.3
Train_BestReturn : 447.8
TimeSinceStart : 140.51850295066833
Done logging...




********** Iteration 51000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 52000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 53000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 54000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 55000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 56000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 57000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 58000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 59000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 60000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) 448.200000
best mean reward 448.200000
running time 183.314321
Train_EnvstepsSoFar : 60001
Train_AverageReturn : 448.2
Train_BestReturn : 448.2
TimeSinceStart : 183.31432127952576
Training Loss : 0.09299100935459137
Done logging...




********** Iteration 61000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 62000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 63000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 64000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 65000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 66000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 67000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 68000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 69000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 70000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) 448.400000
best mean reward 448.400000
running time 224.751579
Train_EnvstepsSoFar : 70001
Train_AverageReturn : 448.4
Train_BestReturn : 448.4
TimeSinceStart : 224.75157856941223
Training Loss : 0.054596275091171265
Done logging...




********** Iteration 71000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 72000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 73000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 74000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 75000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 76000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 77000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 78000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 79000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 80000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) 449.500000
best mean reward 449.500000
running time 268.153502
Train_EnvstepsSoFar : 80001
Train_AverageReturn : 449.5
Train_BestReturn : 449.5
TimeSinceStart : 268.15350222587585
Training Loss : 0.0857803076505661
Done logging...




********** Iteration 81000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 82000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 83000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 84000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 85000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 86000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 87000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 88000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 89000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 90000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) 459.500000
best mean reward 459.500000
running time 312.123248
Train_EnvstepsSoFar : 90001
Train_AverageReturn : 459.5
Train_BestReturn : 459.5
TimeSinceStart : 312.1232476234436
Training Loss : 0.10334094613790512
Done logging...




********** Iteration 91000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 92000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 93000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 94000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 95000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 96000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 97000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 98000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 99000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) 471.900000
best mean reward 471.900000
running time 356.168981
Train_EnvstepsSoFar : 100001
Train_AverageReturn : 471.9
Train_BestReturn : 471.9
TimeSinceStart : 356.1689805984497
Training Loss : 0.06729985773563385
Done logging...




********** Iteration 101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) 448.400000
best mean reward 471.900000
running time 399.000919
Train_EnvstepsSoFar : 110001
Train_AverageReturn : 448.4
Train_BestReturn : 471.9
TimeSinceStart : 399.00091886520386
Training Loss : 0.06268138438463211
Done logging...




********** Iteration 111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) 437.300000
best mean reward 471.900000
running time 441.853129
Train_EnvstepsSoFar : 120001
Train_AverageReturn : 437.3
Train_BestReturn : 471.9
TimeSinceStart : 441.85312938690186
Training Loss : 0.057599522173404694
Done logging...




********** Iteration 121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) 499.800000
best mean reward 499.800000
running time 485.024777
Train_EnvstepsSoFar : 130001
Train_AverageReturn : 499.8
Train_BestReturn : 499.8
TimeSinceStart : 485.024777173996
Training Loss : 0.053617365658283234
Done logging...




********** Iteration 131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) 478.200000
best mean reward 499.800000
running time 528.175507
Train_EnvstepsSoFar : 140001
Train_AverageReturn : 478.2
Train_BestReturn : 499.8
TimeSinceStart : 528.1755065917969
Training Loss : 0.04058193415403366
Done logging...




********** Iteration 141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) 449.000000
best mean reward 499.800000
running time 571.357010
Train_EnvstepsSoFar : 150001
Train_AverageReturn : 449.0
Train_BestReturn : 499.8
TimeSinceStart : 571.3570096492767
Training Loss : 0.15998122096061707
Done logging...




********** Iteration 151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 476.200000
best mean reward 499.800000
running time 614.518090
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 476.2
Train_BestReturn : 499.8
TimeSinceStart : 614.5180900096893
Training Loss : 0.14376476407051086
Done logging...




********** Iteration 161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 514.600000
best mean reward 514.600000
running time 658.243215
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 514.6
Train_BestReturn : 514.6
TimeSinceStart : 658.2432148456573
Training Loss : 0.0570138618350029
Done logging...




********** Iteration 171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 531.800000
best mean reward 531.800000
running time 702.555682
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 531.8
Train_BestReturn : 531.8
TimeSinceStart : 702.5556824207306
Training Loss : 0.11665366590023041
Done logging...




********** Iteration 181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 527.900000
best mean reward 531.800000
running time 747.526852
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 527.9
Train_BestReturn : 531.8
TimeSinceStart : 747.5268518924713
Training Loss : 0.12735512852668762
Done logging...




********** Iteration 191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 486.900000
best mean reward 531.800000
running time 794.728345
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 486.9
Train_BestReturn : 531.8
TimeSinceStart : 794.7283451557159
Training Loss : 0.22963106632232666
Done logging...




********** Iteration 201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 505.600000
best mean reward 531.800000
running time 838.111019
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 505.6
Train_BestReturn : 531.8
TimeSinceStart : 838.1110191345215
Training Loss : 0.05945843830704689
Done logging...




********** Iteration 211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 578.600000
best mean reward 578.600000
running time 881.534902
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 578.6
Train_BestReturn : 578.6
TimeSinceStart : 881.5349020957947
Training Loss : 0.14844228327274323
Done logging...




********** Iteration 221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 559.400000
best mean reward 578.600000
running time 925.002429
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 559.4
Train_BestReturn : 578.6
TimeSinceStart : 925.0024287700653
Training Loss : 0.23167350888252258
Done logging...




********** Iteration 231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 574.100000
best mean reward 578.600000
running time 968.402946
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 574.1
Train_BestReturn : 578.6
TimeSinceStart : 968.4029459953308
Training Loss : 0.16139692068099976
Done logging...




********** Iteration 241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 624.300000
best mean reward 624.300000
running time 1013.029508
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 624.3
Train_BestReturn : 624.3
TimeSinceStart : 1013.0295076370239
Training Loss : 0.20726493000984192
Done logging...




********** Iteration 251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 657.400000
best mean reward 657.400000
running time 1056.452278
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 657.4
Train_BestReturn : 657.4
TimeSinceStart : 1056.4522778987885
Training Loss : 0.15453031659126282
Done logging...




********** Iteration 261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 602.900000
best mean reward 657.400000
running time 1102.899554
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 602.9
Train_BestReturn : 657.4
TimeSinceStart : 1102.899554491043
Training Loss : 0.08297719061374664
Done logging...




********** Iteration 271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 622.000000
best mean reward 657.400000
running time 1146.596681
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 622.0
Train_BestReturn : 657.4
TimeSinceStart : 1146.59668135643
Training Loss : 0.276652455329895
Done logging...




********** Iteration 281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 677.200000
best mean reward 677.200000
running time 1193.685440
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 677.2
Train_BestReturn : 677.2
TimeSinceStart : 1193.6854403018951
Training Loss : 0.20883281528949738
Done logging...




********** Iteration 291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 658.400000
best mean reward 677.200000
running time 1237.997648
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 658.4
Train_BestReturn : 677.2
TimeSinceStart : 1237.99764752388
Training Loss : 0.22759315371513367
Done logging...




********** Iteration 301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 610.400000
best mean reward 677.200000
running time 1281.733083
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 610.4
Train_BestReturn : 677.2
TimeSinceStart : 1281.733083486557
Training Loss : 0.34995537996292114
Done logging...




********** Iteration 311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 644.600000
best mean reward 677.200000
running time 1325.526603
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 644.6
Train_BestReturn : 677.2
TimeSinceStart : 1325.5266032218933
Training Loss : 0.19251704216003418
Done logging...




********** Iteration 321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 717.600000
best mean reward 717.600000
running time 1369.168235
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 717.6
Train_BestReturn : 717.6
TimeSinceStart : 1369.1682353019714
Training Loss : 0.2177271693944931
Done logging...




********** Iteration 331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 721.000000
best mean reward 721.000000
running time 1412.970864
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 721.0
Train_BestReturn : 721.0
TimeSinceStart : 1412.970864057541
Training Loss : 0.3704557716846466
Done logging...




********** Iteration 341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 783.200000
best mean reward 783.200000
running time 1456.818292
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 783.2
Train_BestReturn : 783.2
TimeSinceStart : 1456.8182916641235
Training Loss : 0.17358967661857605
Done logging...




********** Iteration 351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 798.700000
best mean reward 798.700000
running time 1501.453954
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 798.7
Train_BestReturn : 798.7
TimeSinceStart : 1501.453953742981
Training Loss : 0.18438546359539032
Done logging...




********** Iteration 361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 716.900000
best mean reward 798.700000
running time 1545.276923
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 716.9
Train_BestReturn : 798.7
TimeSinceStart : 1545.2769229412079
Training Loss : 0.4013458788394928
Done logging...




********** Iteration 371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 727.300000
best mean reward 798.700000
running time 1590.020454
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 727.3
Train_BestReturn : 798.7
TimeSinceStart : 1590.0204544067383
Training Loss : 0.24273446202278137
Done logging...




********** Iteration 381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 721.500000
best mean reward 798.700000
running time 1635.687348
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 721.5
Train_BestReturn : 798.7
TimeSinceStart : 1635.687348127365
Training Loss : 0.22023683786392212
Done logging...




********** Iteration 391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 762.900000
best mean reward 798.700000
running time 1681.397992
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 762.9
Train_BestReturn : 798.7
TimeSinceStart : 1681.397991657257
Training Loss : 0.32189321517944336
Done logging...




********** Iteration 401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 813.100000
best mean reward 813.100000
running time 1726.951937
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 813.1
Train_BestReturn : 813.1
TimeSinceStart : 1726.9519367218018
Training Loss : 0.22885125875473022
Done logging...




********** Iteration 411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 802.900000
best mean reward 813.100000
running time 1772.130777
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 802.9
Train_BestReturn : 813.1
TimeSinceStart : 1772.130776643753
Training Loss : 0.2532777786254883
Done logging...




********** Iteration 421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) 753.800000
best mean reward 813.100000
running time 1817.910269
Train_EnvstepsSoFar : 430001
Train_AverageReturn : 753.8
Train_BestReturn : 813.1
TimeSinceStart : 1817.910269021988
Training Loss : 0.16776803135871887
Done logging...




********** Iteration 431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) 704.900000
best mean reward 813.100000
running time 1863.862489
Train_EnvstepsSoFar : 440001
Train_AverageReturn : 704.9
Train_BestReturn : 813.1
TimeSinceStart : 1863.8624885082245
Training Loss : 0.20104624330997467
Done logging...




********** Iteration 441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) 750.500000
best mean reward 813.100000
running time 1910.083829
Train_EnvstepsSoFar : 450001
Train_AverageReturn : 750.5
Train_BestReturn : 813.1
TimeSinceStart : 1910.0838286876678
Training Loss : 0.2604890763759613
Done logging...




********** Iteration 451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) 781.700000
best mean reward 813.100000
running time 1954.466084
Train_EnvstepsSoFar : 460001
Train_AverageReturn : 781.7
Train_BestReturn : 813.1
TimeSinceStart : 1954.4660835266113
Training Loss : 0.46080654859542847
Done logging...




********** Iteration 461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) 798.400000
best mean reward 813.100000
running time 1999.676648
Train_EnvstepsSoFar : 470001
Train_AverageReturn : 798.4
Train_BestReturn : 813.1
TimeSinceStart : 1999.676647901535
Training Loss : 0.3320216238498688
Done logging...




********** Iteration 471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) 793.200000
best mean reward 813.100000
running time 2044.426129
Train_EnvstepsSoFar : 480001
Train_AverageReturn : 793.2
Train_BestReturn : 813.1
TimeSinceStart : 2044.4261286258698
Training Loss : 0.31570935249328613
Done logging...




********** Iteration 481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) 845.800000
best mean reward 845.800000
running time 2090.696429
Train_EnvstepsSoFar : 490001
Train_AverageReturn : 845.8
Train_BestReturn : 845.8
TimeSinceStart : 2090.696429014206
Training Loss : 0.2800624370574951
Done logging...




********** Iteration 491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 499000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 500000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 500001
mean reward (100 episodes) 862.500000
best mean reward 862.500000
running time 2139.245593
Train_EnvstepsSoFar : 500001
Train_AverageReturn : 862.5
Train_BestReturn : 862.5
TimeSinceStart : 2139.2455925941467
Training Loss : 0.2768288850784302
Done logging...




********** Iteration 501000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 502000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 503000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 504000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 505000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 506000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 507000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 508000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 509000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 510000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 510001
mean reward (100 episodes) 886.900000
best mean reward 886.900000
running time 2185.022483
Train_EnvstepsSoFar : 510001
Train_AverageReturn : 886.9
Train_BestReturn : 886.9
TimeSinceStart : 2185.022483110428
Training Loss : 0.19117343425750732
Done logging...




********** Iteration 511000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 512000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 513000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 514000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 515000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 516000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 517000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 518000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 519000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 520000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 520001
mean reward (100 episodes) 895.300000
best mean reward 895.300000
running time 2231.364451
Train_EnvstepsSoFar : 520001
Train_AverageReturn : 895.3
Train_BestReturn : 895.3
TimeSinceStart : 2231.364450931549
Training Loss : 0.316409707069397
Done logging...




********** Iteration 521000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 522000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 523000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 524000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 525000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 526000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 527000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 528000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 529000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 530000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 530001
mean reward (100 episodes) 932.500000
best mean reward 932.500000
running time 2278.036638
Train_EnvstepsSoFar : 530001
Train_AverageReturn : 932.5
Train_BestReturn : 932.5
TimeSinceStart : 2278.036637544632
Training Loss : 0.21579691767692566
Done logging...




********** Iteration 531000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 532000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 533000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 534000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 535000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 536000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 537000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 538000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 539000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 540000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 540001
mean reward (100 episodes) 903.900000
best mean reward 932.500000
running time 2324.827489
Train_EnvstepsSoFar : 540001
Train_AverageReturn : 903.9
Train_BestReturn : 932.5
TimeSinceStart : 2324.827489376068
Training Loss : 0.35511407256126404
Done logging...




********** Iteration 541000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 542000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 543000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 544000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 545000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 546000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 547000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 548000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 549000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 550000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 550001
mean reward (100 episodes) 941.300000
best mean reward 941.300000
running time 2370.567166
Train_EnvstepsSoFar : 550001
Train_AverageReturn : 941.3
Train_BestReturn : 941.3
TimeSinceStart : 2370.56716632843
Training Loss : 0.5137243270874023
Done logging...




********** Iteration 551000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 552000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 553000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 554000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 555000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 556000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 557000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 558000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 559000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 560000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 560001
mean reward (100 episodes) 1023.400000
best mean reward 1023.400000
running time 2415.896245
Train_EnvstepsSoFar : 560001
Train_AverageReturn : 1023.4
Train_BestReturn : 1023.4
TimeSinceStart : 2415.8962450027466
Training Loss : 0.2981407642364502
Done logging...




********** Iteration 561000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 562000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 563000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 564000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 565000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 566000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 567000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 568000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 569000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 570000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 570001
mean reward (100 episodes) 990.700000
best mean reward 1023.400000
running time 2463.237967
Train_EnvstepsSoFar : 570001
Train_AverageReturn : 990.7
Train_BestReturn : 1023.4
TimeSinceStart : 2463.2379665374756
Training Loss : 0.4562656581401825
Done logging...




********** Iteration 571000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 572000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 573000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 574000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 575000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 576000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 577000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 578000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 579000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 580000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 580001
mean reward (100 episodes) 1021.400000
best mean reward 1023.400000
running time 2510.148999
Train_EnvstepsSoFar : 580001
Train_AverageReturn : 1021.4
Train_BestReturn : 1023.4
TimeSinceStart : 2510.148998975754
Training Loss : 0.7486435174942017
Done logging...




********** Iteration 581000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 582000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 583000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 584000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 585000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 586000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 587000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 588000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 589000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 590000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 590001
mean reward (100 episodes) 1049.600000
best mean reward 1049.600000
running time 2557.514826
Train_EnvstepsSoFar : 590001
Train_AverageReturn : 1049.6
Train_BestReturn : 1049.6
TimeSinceStart : 2557.51482629776
Training Loss : 0.5278670787811279
Done logging...




********** Iteration 591000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 592000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 593000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 594000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 595000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 596000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 597000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 598000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 599000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 600000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 600001
mean reward (100 episodes) 985.000000
best mean reward 1049.600000
running time 2607.093295
Train_EnvstepsSoFar : 600001
Train_AverageReturn : 985.0
Train_BestReturn : 1049.6
TimeSinceStart : 2607.0932948589325
Training Loss : 0.4893559515476227
Done logging...




********** Iteration 601000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 602000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 603000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 604000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 605000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 606000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 607000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 608000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 609000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 610000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 610001
mean reward (100 episodes) 955.500000
best mean reward 1049.600000
running time 2654.857647
Train_EnvstepsSoFar : 610001
Train_AverageReturn : 955.5
Train_BestReturn : 1049.6
TimeSinceStart : 2654.8576471805573
Training Loss : 0.6201448440551758
Done logging...




********** Iteration 611000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 612000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 613000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 614000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 615000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 616000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 617000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 618000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 619000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 620000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 620001
mean reward (100 episodes) 1001.900000
best mean reward 1049.600000
running time 2705.488600
Train_EnvstepsSoFar : 620001
Train_AverageReturn : 1001.9
Train_BestReturn : 1049.6
TimeSinceStart : 2705.488600254059
Training Loss : 0.4691135585308075
Done logging...




********** Iteration 621000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 622000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 623000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 624000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 625000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 626000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 627000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 628000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 629000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 630000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 630001
mean reward (100 episodes) 959.000000
best mean reward 1049.600000
running time 2755.238082
Train_EnvstepsSoFar : 630001
Train_AverageReturn : 959.0
Train_BestReturn : 1049.6
TimeSinceStart : 2755.2380821704865
Training Loss : 0.2021818906068802
Done logging...




********** Iteration 631000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 632000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 633000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 634000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 635000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 636000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 637000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 638000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 639000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 640000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 640001
mean reward (100 episodes) 951.300000
best mean reward 1049.600000
running time 2804.742592
Train_EnvstepsSoFar : 640001
Train_AverageReturn : 951.3
Train_BestReturn : 1049.6
TimeSinceStart : 2804.7425920963287
Training Loss : 0.7032293081283569
Done logging...




********** Iteration 641000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 642000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 643000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 644000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 645000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 646000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 647000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 648000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 649000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 650000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 650001
mean reward (100 episodes) 980.900000
best mean reward 1049.600000
running time 2852.818516
Train_EnvstepsSoFar : 650001
Train_AverageReturn : 980.9
Train_BestReturn : 1049.6
TimeSinceStart : 2852.8185155391693
Training Loss : 0.29363131523132324
Done logging...




********** Iteration 651000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 652000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 653000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 654000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 655000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 656000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 657000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 658000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 659000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 660000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 660001
mean reward (100 episodes) 990.600000
best mean reward 1049.600000
running time 2901.633760
Train_EnvstepsSoFar : 660001
Train_AverageReturn : 990.6
Train_BestReturn : 1049.6
TimeSinceStart : 2901.6337599754333
Training Loss : 0.2749135494232178
Done logging...




********** Iteration 661000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 662000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 663000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 664000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 665000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 666000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 667000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 668000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 669000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 670000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 670001
mean reward (100 episodes) 975.500000
best mean reward 1049.600000
running time 2949.252070
Train_EnvstepsSoFar : 670001
Train_AverageReturn : 975.5
Train_BestReturn : 1049.6
TimeSinceStart : 2949.252069711685
Training Loss : 0.5787261724472046
Done logging...




********** Iteration 671000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 672000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 673000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 674000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 675000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 676000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 677000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 678000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 679000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 680000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 680001
mean reward (100 episodes) 978.900000
best mean reward 1049.600000
running time 2995.319942
Train_EnvstepsSoFar : 680001
Train_AverageReturn : 978.9
Train_BestReturn : 1049.6
TimeSinceStart : 2995.3199424743652
Training Loss : 0.36937427520751953
Done logging...




********** Iteration 681000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 682000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 683000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 684000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 685000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 686000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 687000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 688000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 689000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 690000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 690001
mean reward (100 episodes) 946.800000
best mean reward 1049.600000
running time 3041.956638
Train_EnvstepsSoFar : 690001
Train_AverageReturn : 946.8
Train_BestReturn : 1049.6
TimeSinceStart : 3041.956637620926
Training Loss : 0.4163031578063965
Done logging...




********** Iteration 691000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 692000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 693000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 694000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 695000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 696000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 697000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 698000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 699000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 700000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 700001
mean reward (100 episodes) 1054.300000
best mean reward 1054.300000
running time 3088.912987
Train_EnvstepsSoFar : 700001
Train_AverageReturn : 1054.3
Train_BestReturn : 1054.3
TimeSinceStart : 3088.912986755371
Training Loss : 0.42845121026039124
Done logging...




********** Iteration 701000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 702000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 703000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 704000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 705000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 706000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 707000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 708000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 709000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 710000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 710001
mean reward (100 episodes) 1140.000000
best mean reward 1140.000000
running time 3134.183409
Train_EnvstepsSoFar : 710001
Train_AverageReturn : 1140.0
Train_BestReturn : 1140.0
TimeSinceStart : 3134.1834087371826
Training Loss : 0.31230008602142334
Done logging...




********** Iteration 711000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 712000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 713000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 714000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 715000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 716000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 717000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 718000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 719000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 720000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 720001
mean reward (100 episodes) 1195.800000
best mean reward 1195.800000
running time 3179.762572
Train_EnvstepsSoFar : 720001
Train_AverageReturn : 1195.8
Train_BestReturn : 1195.8
TimeSinceStart : 3179.762572288513
Training Loss : 0.21609920263290405
Done logging...




********** Iteration 721000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 722000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 723000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 724000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 725000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 726000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 727000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 728000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 729000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 730000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 730001
mean reward (100 episodes) 1222.600000
best mean reward 1222.600000
running time 3226.119788
Train_EnvstepsSoFar : 730001
Train_AverageReturn : 1222.6
Train_BestReturn : 1222.6
TimeSinceStart : 3226.119788169861
Training Loss : 0.2867887318134308
Done logging...




********** Iteration 731000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 732000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 733000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 734000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 735000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 736000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 737000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 738000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 739000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 740000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 740001
mean reward (100 episodes) 1272.200000
best mean reward 1272.200000
running time 3272.008276
Train_EnvstepsSoFar : 740001
Train_AverageReturn : 1272.2
Train_BestReturn : 1272.2
TimeSinceStart : 3272.008275985718
Training Loss : 0.5141927003860474
Done logging...




********** Iteration 741000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 742000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 743000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 744000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 745000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 746000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 747000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 748000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 749000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 750000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 750001
mean reward (100 episodes) 1240.400000
best mean reward 1272.200000
running time 3318.787742
Train_EnvstepsSoFar : 750001
Train_AverageReturn : 1240.4
Train_BestReturn : 1272.2
TimeSinceStart : 3318.7877423763275
Training Loss : 0.522606611251831
Done logging...




********** Iteration 751000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 752000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 753000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 754000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 755000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 756000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 757000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 758000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 759000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 760000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 760001
mean reward (100 episodes) 1278.800000
best mean reward 1278.800000
running time 3364.704849
Train_EnvstepsSoFar : 760001
Train_AverageReturn : 1278.8
Train_BestReturn : 1278.8
TimeSinceStart : 3364.7048494815826
Training Loss : 0.1443270444869995
Done logging...




********** Iteration 761000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 762000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 763000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 764000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 765000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 766000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 767000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 768000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 769000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 770000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 770001
mean reward (100 episodes) 1251.600000
best mean reward 1278.800000
running time 3411.683650
Train_EnvstepsSoFar : 770001
Train_AverageReturn : 1251.6
Train_BestReturn : 1278.8
TimeSinceStart : 3411.6836500167847
Training Loss : 0.5102173686027527
Done logging...




********** Iteration 771000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 772000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 773000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 774000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 775000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 776000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 777000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 778000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 779000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 780000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 780001
mean reward (100 episodes) 1134.600000
best mean reward 1278.800000
running time 3459.295202
Train_EnvstepsSoFar : 780001
Train_AverageReturn : 1134.6
Train_BestReturn : 1278.8
TimeSinceStart : 3459.295201778412
Training Loss : 0.35055333375930786
Done logging...




********** Iteration 781000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 782000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 783000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 784000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 785000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 786000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 787000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 788000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 789000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 790000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 790001
mean reward (100 episodes) 1193.400000
best mean reward 1278.800000
running time 3505.000763
Train_EnvstepsSoFar : 790001
Train_AverageReturn : 1193.4
Train_BestReturn : 1278.8
TimeSinceStart : 3505.0007627010345
Training Loss : 0.3194589614868164
Done logging...




********** Iteration 791000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 792000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 793000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 794000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 795000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 796000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 797000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 798000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 799000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 800000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 800001
mean reward (100 episodes) 1254.100000
best mean reward 1278.800000
running time 3551.371830
Train_EnvstepsSoFar : 800001
Train_AverageReturn : 1254.1
Train_BestReturn : 1278.8
TimeSinceStart : 3551.3718299865723
Training Loss : 0.8157084584236145
Done logging...




********** Iteration 801000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 802000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 803000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 804000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 805000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 806000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 807000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 808000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 809000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 810000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 810001
mean reward (100 episodes) 1339.700000
best mean reward 1339.700000
running time 3601.487807
Train_EnvstepsSoFar : 810001
Train_AverageReturn : 1339.7
Train_BestReturn : 1339.7
TimeSinceStart : 3601.487807035446
Training Loss : 0.3098195791244507
Done logging...




********** Iteration 811000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 812000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 813000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 814000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 815000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 816000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 817000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 818000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 819000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 820000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 820001
mean reward (100 episodes) 1346.200000
best mean reward 1346.200000
running time 3649.722214
Train_EnvstepsSoFar : 820001
Train_AverageReturn : 1346.2
Train_BestReturn : 1346.2
TimeSinceStart : 3649.722214460373
Training Loss : 0.4177453815937042
Done logging...




********** Iteration 821000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 822000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 823000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 824000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 825000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 826000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 827000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 828000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 829000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 830000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 830001
mean reward (100 episodes) 1332.800000
best mean reward 1346.200000
running time 3697.757777
Train_EnvstepsSoFar : 830001
Train_AverageReturn : 1332.8
Train_BestReturn : 1346.2
TimeSinceStart : 3697.757776737213
Training Loss : 0.4345289468765259
Done logging...




********** Iteration 831000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 832000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 833000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 834000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 835000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 836000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 837000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 838000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 839000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 840000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 840001
mean reward (100 episodes) 1327.600000
best mean reward 1346.200000
running time 3744.991155
Train_EnvstepsSoFar : 840001
Train_AverageReturn : 1327.6
Train_BestReturn : 1346.2
TimeSinceStart : 3744.991155385971
Training Loss : 0.3264620304107666
Done logging...




********** Iteration 841000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 842000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 843000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 844000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 845000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 846000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 847000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 848000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 849000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 850000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 850001
mean reward (100 episodes) 1283.700000
best mean reward 1346.200000
running time 3791.472929
Train_EnvstepsSoFar : 850001
Train_AverageReturn : 1283.7
Train_BestReturn : 1346.2
TimeSinceStart : 3791.4729294776917
Training Loss : 0.4773719310760498
Done logging...




********** Iteration 851000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 852000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 853000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 854000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 855000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 856000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 857000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 858000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 859000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 860000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 860001
mean reward (100 episodes) 1300.000000
best mean reward 1346.200000
running time 3841.226774
Train_EnvstepsSoFar : 860001
Train_AverageReturn : 1300.0
Train_BestReturn : 1346.2
TimeSinceStart : 3841.226773738861
Training Loss : 1.3170316219329834
Done logging...




********** Iteration 861000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 862000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 863000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 864000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 865000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 866000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 867000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 868000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 869000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 870000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 870001
mean reward (100 episodes) 1316.500000
best mean reward 1346.200000
running time 3890.001014
Train_EnvstepsSoFar : 870001
Train_AverageReturn : 1316.5
Train_BestReturn : 1346.2
TimeSinceStart : 3890.0010135173798
Training Loss : 0.4362531006336212
Done logging...




********** Iteration 871000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 872000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 873000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 874000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 875000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 876000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 877000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 878000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 879000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 880000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 880001
mean reward (100 episodes) 1400.000000
best mean reward 1400.000000
running time 3936.238044
Train_EnvstepsSoFar : 880001
Train_AverageReturn : 1400.0
Train_BestReturn : 1400.0
TimeSinceStart : 3936.238043785095
Training Loss : 0.2626786231994629
Done logging...




********** Iteration 881000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 882000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 883000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 884000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 885000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 886000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 887000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 888000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 889000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 890000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 890001
mean reward (100 episodes) 1408.600000
best mean reward 1408.600000
running time 3982.222338
Train_EnvstepsSoFar : 890001
Train_AverageReturn : 1408.6
Train_BestReturn : 1408.6
TimeSinceStart : 3982.2223377227783
Training Loss : 0.3137893080711365
Done logging...




********** Iteration 891000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 892000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 893000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 894000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 895000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 896000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 897000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 898000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 899000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 900000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 900001
mean reward (100 episodes) 1397.000000
best mean reward 1408.600000
running time 4028.516150
Train_EnvstepsSoFar : 900001
Train_AverageReturn : 1397.0
Train_BestReturn : 1408.6
TimeSinceStart : 4028.5161502361298
Training Loss : 0.5043793320655823
Done logging...




********** Iteration 901000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 902000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 903000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 904000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 905000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 906000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 907000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 908000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 909000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 910000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 910001
mean reward (100 episodes) 1354.900000
best mean reward 1408.600000
running time 4076.945210
Train_EnvstepsSoFar : 910001
Train_AverageReturn : 1354.9
Train_BestReturn : 1408.6
TimeSinceStart : 4076.9452102184296
Training Loss : 0.40534740686416626
Done logging...




********** Iteration 911000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 912000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 913000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 914000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 915000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 916000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 917000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 918000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 919000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 920000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 920001
mean reward (100 episodes) 1340.400000
best mean reward 1408.600000
running time 4123.048090
Train_EnvstepsSoFar : 920001
Train_AverageReturn : 1340.4
Train_BestReturn : 1408.6
TimeSinceStart : 4123.048090219498
Training Loss : 0.32126808166503906
Done logging...




********** Iteration 921000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 922000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 923000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 924000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 925000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 926000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 927000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 928000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 929000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 930000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 930001
mean reward (100 episodes) 1343.700000
best mean reward 1408.600000
running time 4169.540079
Train_EnvstepsSoFar : 930001
Train_AverageReturn : 1343.7
Train_BestReturn : 1408.6
TimeSinceStart : 4169.540079116821
Training Loss : 0.3916831314563751
Done logging...




********** Iteration 931000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 932000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 933000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 934000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 935000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 936000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 937000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 938000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 939000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 940000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 940001
mean reward (100 episodes) 1453.100000
best mean reward 1453.100000
running time 4218.595094
Train_EnvstepsSoFar : 940001
Train_AverageReturn : 1453.1
Train_BestReturn : 1453.1
TimeSinceStart : 4218.59509396553
Training Loss : 0.5149651765823364
Done logging...




********** Iteration 941000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 942000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 943000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 944000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 945000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 946000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 947000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 948000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 949000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 950000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 950001
mean reward (100 episodes) 1449.400000
best mean reward 1453.100000
running time 4269.940560
Train_EnvstepsSoFar : 950001
Train_AverageReturn : 1449.4
Train_BestReturn : 1453.1
TimeSinceStart : 4269.940559625626
Training Loss : 0.3572847247123718
Done logging...




********** Iteration 951000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 952000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 953000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 954000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 955000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 956000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 957000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 958000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 959000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 960000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 960001
mean reward (100 episodes) 1431.600000
best mean reward 1453.100000
running time 4321.252177
Train_EnvstepsSoFar : 960001
Train_AverageReturn : 1431.6
Train_BestReturn : 1453.1
TimeSinceStart : 4321.252177476883
Training Loss : 0.23290005326271057
Done logging...




********** Iteration 961000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 962000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 963000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 964000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 965000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 966000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 967000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 968000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 969000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 970000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 970001
mean reward (100 episodes) 1442.100000
best mean reward 1453.100000
running time 4373.276020
Train_EnvstepsSoFar : 970001
Train_AverageReturn : 1442.1
Train_BestReturn : 1453.1
TimeSinceStart : 4373.27601981163
Training Loss : 0.5454373955726624
Done logging...




********** Iteration 971000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 972000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 973000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 974000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 975000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 976000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 977000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 978000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 979000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 980000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 980001
mean reward (100 episodes) 1426.800000
best mean reward 1453.100000
running time 4424.846884
Train_EnvstepsSoFar : 980001
Train_AverageReturn : 1426.8
Train_BestReturn : 1453.1
TimeSinceStart : 4424.846884012222
Training Loss : 0.7408512830734253
Done logging...




********** Iteration 981000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 982000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 983000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 984000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 985000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 986000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 987000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 988000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 989000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 990000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 990001
mean reward (100 episodes) 1448.400000
best mean reward 1453.100000
running time 4474.753647
Train_EnvstepsSoFar : 990001
Train_AverageReturn : 1448.4
Train_BestReturn : 1453.1
TimeSinceStart : 4474.753646850586
Training Loss : 0.752628743648529
Done logging...




********** Iteration 991000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 992000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 993000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 994000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 995000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 996000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 997000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 998000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 999000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1000000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1000001
mean reward (100 episodes) 1470.900000
best mean reward 1470.900000
running time 4527.289263
Train_EnvstepsSoFar : 1000001
Train_AverageReturn : 1470.9
Train_BestReturn : 1470.9
TimeSinceStart : 4527.289263486862
Training Loss : 0.7307058572769165
Done logging...




********** Iteration 1001000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1002000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1003000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1004000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1005000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1006000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1007000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1008000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1009000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1010000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1010001
mean reward (100 episodes) 1510.700000
best mean reward 1510.700000
running time 4577.920198
Train_EnvstepsSoFar : 1010001
Train_AverageReturn : 1510.7
Train_BestReturn : 1510.7
TimeSinceStart : 4577.920198202133
Training Loss : 0.48331204056739807
Done logging...




********** Iteration 1011000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1012000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1013000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1014000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1015000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1016000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1017000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1018000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1019000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1020000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1020001
mean reward (100 episodes) 1486.700000
best mean reward 1510.700000
running time 4630.376744
Train_EnvstepsSoFar : 1020001
Train_AverageReturn : 1486.7
Train_BestReturn : 1510.7
TimeSinceStart : 4630.376743555069
Training Loss : 0.390117883682251
Done logging...




********** Iteration 1021000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1022000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1023000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1024000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1025000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1026000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1027000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1028000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1029000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1030000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1030001
mean reward (100 episodes) 1469.600000
best mean reward 1510.700000
running time 4682.745817
Train_EnvstepsSoFar : 1030001
Train_AverageReturn : 1469.6
Train_BestReturn : 1510.7
TimeSinceStart : 4682.745817422867
Training Loss : 1.2361077070236206
Done logging...




********** Iteration 1031000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1032000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1033000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1034000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1035000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1036000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1037000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1038000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1039000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1040000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1040001
mean reward (100 episodes) 1440.100000
best mean reward 1510.700000
running time 4733.846073
Train_EnvstepsSoFar : 1040001
Train_AverageReturn : 1440.1
Train_BestReturn : 1510.7
TimeSinceStart : 4733.846073150635
Training Loss : 0.4215904176235199
Done logging...




********** Iteration 1041000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1042000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1043000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1044000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1045000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1046000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1047000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1048000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1049000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1050000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1050001
mean reward (100 episodes) 1498.600000
best mean reward 1510.700000
running time 4784.040346
Train_EnvstepsSoFar : 1050001
Train_AverageReturn : 1498.6
Train_BestReturn : 1510.7
TimeSinceStart : 4784.04034614563
Training Loss : 0.44537052512168884
Done logging...




********** Iteration 1051000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1052000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1053000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1054000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1055000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1056000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1057000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1058000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1059000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1060000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1060001
mean reward (100 episodes) 1500.200000
best mean reward 1510.700000
running time 4834.695024
Train_EnvstepsSoFar : 1060001
Train_AverageReturn : 1500.2
Train_BestReturn : 1510.7
TimeSinceStart : 4834.695024490356
Training Loss : 0.6292924880981445
Done logging...




********** Iteration 1061000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1062000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1063000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1064000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1065000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1066000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1067000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1068000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1069000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1070000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1070001
mean reward (100 episodes) 1481.000000
best mean reward 1510.700000
running time 4885.147060
Train_EnvstepsSoFar : 1070001
Train_AverageReturn : 1481.0
Train_BestReturn : 1510.7
TimeSinceStart : 4885.147059679031
Training Loss : 0.5666443109512329
Done logging...




********** Iteration 1071000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1072000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1073000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1074000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1075000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1076000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1077000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1078000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1079000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1080000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1080001
mean reward (100 episodes) 1412.400000
best mean reward 1510.700000
running time 4934.357010
Train_EnvstepsSoFar : 1080001
Train_AverageReturn : 1412.4
Train_BestReturn : 1510.7
TimeSinceStart : 4934.357009649277
Training Loss : 0.27059999108314514
Done logging...




********** Iteration 1081000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1082000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1083000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1084000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1085000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1086000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1087000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1088000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1089000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1090000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1090001
mean reward (100 episodes) 1428.500000
best mean reward 1510.700000
running time 4981.000407
Train_EnvstepsSoFar : 1090001
Train_AverageReturn : 1428.5
Train_BestReturn : 1510.7
TimeSinceStart : 4981.000407457352
Training Loss : 0.6439306735992432
Done logging...




********** Iteration 1091000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1092000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1093000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1094000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1095000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1096000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1097000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1098000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1099000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1100001
mean reward (100 episodes) 1432.800000
best mean reward 1510.700000
running time 5027.187540
Train_EnvstepsSoFar : 1100001
Train_AverageReturn : 1432.8
Train_BestReturn : 1510.7
TimeSinceStart : 5027.187539815903
Training Loss : 0.3046647310256958
Done logging...




********** Iteration 1101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1110001
mean reward (100 episodes) 1405.900000
best mean reward 1510.700000
running time 5071.745954
Train_EnvstepsSoFar : 1110001
Train_AverageReturn : 1405.9
Train_BestReturn : 1510.7
TimeSinceStart : 5071.7459535598755
Training Loss : 0.4211822748184204
Done logging...




********** Iteration 1111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1120001
mean reward (100 episodes) 1396.000000
best mean reward 1510.700000
running time 5116.342848
Train_EnvstepsSoFar : 1120001
Train_AverageReturn : 1396.0
Train_BestReturn : 1510.7
TimeSinceStart : 5116.342848300934
Training Loss : 0.4237029254436493
Done logging...




********** Iteration 1121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1130001
mean reward (100 episodes) 1414.100000
best mean reward 1510.700000
running time 5160.580052
Train_EnvstepsSoFar : 1130001
Train_AverageReturn : 1414.1
Train_BestReturn : 1510.7
TimeSinceStart : 5160.580052137375
Training Loss : 0.5208278894424438
Done logging...




********** Iteration 1131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1140001
mean reward (100 episodes) 1411.600000
best mean reward 1510.700000
running time 5204.739523
Train_EnvstepsSoFar : 1140001
Train_AverageReturn : 1411.6
Train_BestReturn : 1510.7
TimeSinceStart : 5204.739522695541
Training Loss : 0.6516637802124023
Done logging...




********** Iteration 1141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1150001
mean reward (100 episodes) 1439.000000
best mean reward 1510.700000
running time 5248.831572
Train_EnvstepsSoFar : 1150001
Train_AverageReturn : 1439.0
Train_BestReturn : 1510.7
TimeSinceStart : 5248.831572294235
Training Loss : 0.8125681281089783
Done logging...




********** Iteration 1151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1160001
mean reward (100 episodes) 1440.000000
best mean reward 1510.700000
running time 5292.938410
Train_EnvstepsSoFar : 1160001
Train_AverageReturn : 1440.0
Train_BestReturn : 1510.7
TimeSinceStart : 5292.938409566879
Training Loss : 0.48463118076324463
Done logging...




********** Iteration 1161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1170001
mean reward (100 episodes) 1384.300000
best mean reward 1510.700000
running time 5337.137786
Train_EnvstepsSoFar : 1170001
Train_AverageReturn : 1384.3
Train_BestReturn : 1510.7
TimeSinceStart : 5337.137786388397
Training Loss : 0.7925031185150146
Done logging...




********** Iteration 1171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1180001
mean reward (100 episodes) 1321.000000
best mean reward 1510.700000
running time 5381.392486
Train_EnvstepsSoFar : 1180001
Train_AverageReturn : 1321.0
Train_BestReturn : 1510.7
TimeSinceStart : 5381.39248585701
Training Loss : 0.4667297899723053
Done logging...




********** Iteration 1181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1190001
mean reward (100 episodes) 1361.100000
best mean reward 1510.700000
running time 5425.447500
Train_EnvstepsSoFar : 1190001
Train_AverageReturn : 1361.1
Train_BestReturn : 1510.7
TimeSinceStart : 5425.447499752045
Training Loss : 0.32820332050323486
Done logging...




********** Iteration 1191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1200001
mean reward (100 episodes) 1442.800000
best mean reward 1510.700000
running time 5469.704422
Train_EnvstepsSoFar : 1200001
Train_AverageReturn : 1442.8
Train_BestReturn : 1510.7
TimeSinceStart : 5469.7044224739075
Training Loss : 0.2915211617946625
Done logging...




********** Iteration 1201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1210001
mean reward (100 episodes) 1506.000000
best mean reward 1510.700000
running time 5513.546002
Train_EnvstepsSoFar : 1210001
Train_AverageReturn : 1506.0
Train_BestReturn : 1510.7
TimeSinceStart : 5513.546001672745
Training Loss : 1.0768091678619385
Done logging...




********** Iteration 1211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1220001
mean reward (100 episodes) 1508.700000
best mean reward 1510.700000
running time 5557.506100
Train_EnvstepsSoFar : 1220001
Train_AverageReturn : 1508.7
Train_BestReturn : 1510.7
TimeSinceStart : 5557.506099700928
Training Loss : 0.39748474955558777
Done logging...




********** Iteration 1221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1230001
mean reward (100 episodes) 1513.300000
best mean reward 1513.300000
running time 5601.279376
Train_EnvstepsSoFar : 1230001
Train_AverageReturn : 1513.3
Train_BestReturn : 1513.3
TimeSinceStart : 5601.27937579155
Training Loss : 0.5220373272895813
Done logging...




********** Iteration 1231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1240001
mean reward (100 episodes) 1452.000000
best mean reward 1513.300000
running time 5645.117057
Train_EnvstepsSoFar : 1240001
Train_AverageReturn : 1452.0
Train_BestReturn : 1513.3
TimeSinceStart : 5645.117057085037
Training Loss : 0.6090842485427856
Done logging...




********** Iteration 1241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1250001
mean reward (100 episodes) 1447.000000
best mean reward 1513.300000
running time 5689.033788
Train_EnvstepsSoFar : 1250001
Train_AverageReturn : 1447.0
Train_BestReturn : 1513.3
TimeSinceStart : 5689.033787727356
Training Loss : 0.6760680079460144
Done logging...




********** Iteration 1251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1260001
mean reward (100 episodes) 1476.500000
best mean reward 1513.300000
running time 5732.792115
Train_EnvstepsSoFar : 1260001
Train_AverageReturn : 1476.5
Train_BestReturn : 1513.3
TimeSinceStart : 5732.792114973068
Training Loss : 0.33839160203933716
Done logging...




********** Iteration 1261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1270001
mean reward (100 episodes) 1571.000000
best mean reward 1571.000000
running time 5776.614494
Train_EnvstepsSoFar : 1270001
Train_AverageReturn : 1571.0
Train_BestReturn : 1571.0
TimeSinceStart : 5776.6144943237305
Training Loss : 0.5915611982345581
Done logging...




********** Iteration 1271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1280001
mean reward (100 episodes) 1576.600000
best mean reward 1576.600000
running time 5820.567961
Train_EnvstepsSoFar : 1280001
Train_AverageReturn : 1576.6
Train_BestReturn : 1576.6
TimeSinceStart : 5820.567961215973
Training Loss : 0.690192461013794
Done logging...




********** Iteration 1281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1290001
mean reward (100 episodes) 1601.600000
best mean reward 1601.600000
running time 5864.150279
Train_EnvstepsSoFar : 1290001
Train_AverageReturn : 1601.6
Train_BestReturn : 1601.6
TimeSinceStart : 5864.150278568268
Training Loss : 0.21422412991523743
Done logging...




********** Iteration 1291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1300001
mean reward (100 episodes) 1626.000000
best mean reward 1626.000000
running time 5907.985279
Train_EnvstepsSoFar : 1300001
Train_AverageReturn : 1626.0
Train_BestReturn : 1626.0
TimeSinceStart : 5907.985278844833
Training Loss : 1.1381169557571411
Done logging...




********** Iteration 1301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1310001
mean reward (100 episodes) 1589.200000
best mean reward 1626.000000
running time 5951.833214
Train_EnvstepsSoFar : 1310001
Train_AverageReturn : 1589.2
Train_BestReturn : 1626.0
TimeSinceStart : 5951.833213567734
Training Loss : 0.8294859528541565
Done logging...




********** Iteration 1311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1320001
mean reward (100 episodes) 1522.500000
best mean reward 1626.000000
running time 5995.418279
Train_EnvstepsSoFar : 1320001
Train_AverageReturn : 1522.5
Train_BestReturn : 1626.0
TimeSinceStart : 5995.418278932571
Training Loss : 0.4040103256702423
Done logging...




********** Iteration 1321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1330001
mean reward (100 episodes) 1511.400000
best mean reward 1626.000000
running time 6039.026995
Train_EnvstepsSoFar : 1330001
Train_AverageReturn : 1511.4
Train_BestReturn : 1626.0
TimeSinceStart : 6039.026995182037
Training Loss : 0.27174344658851624
Done logging...




********** Iteration 1331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1340001
mean reward (100 episodes) 1471.800000
best mean reward 1626.000000
running time 6082.811597
Train_EnvstepsSoFar : 1340001
Train_AverageReturn : 1471.8
Train_BestReturn : 1626.0
TimeSinceStart : 6082.811597108841
Training Loss : 0.5064094066619873
Done logging...




********** Iteration 1341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1350001
mean reward (100 episodes) 1514.100000
best mean reward 1626.000000
running time 6126.418808
Train_EnvstepsSoFar : 1350001
Train_AverageReturn : 1514.1
Train_BestReturn : 1626.0
TimeSinceStart : 6126.418807983398
Training Loss : 0.49298471212387085
Done logging...




********** Iteration 1351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1360001
mean reward (100 episodes) 1536.500000
best mean reward 1626.000000
running time 6169.841872
Train_EnvstepsSoFar : 1360001
Train_AverageReturn : 1536.5
Train_BestReturn : 1626.0
TimeSinceStart : 6169.841871500015
Training Loss : 0.7778332233428955
Done logging...




********** Iteration 1361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1370001
mean reward (100 episodes) 1704.700000
best mean reward 1704.700000
running time 6213.495209
Train_EnvstepsSoFar : 1370001
Train_AverageReturn : 1704.7
Train_BestReturn : 1704.7
TimeSinceStart : 6213.495208740234
Training Loss : 0.2664130926132202
Done logging...




********** Iteration 1371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1380001
mean reward (100 episodes) 1653.200000
best mean reward 1704.700000
running time 6257.310749
Train_EnvstepsSoFar : 1380001
Train_AverageReturn : 1653.2
Train_BestReturn : 1704.7
TimeSinceStart : 6257.310749053955
Training Loss : 0.5697258710861206
Done logging...




********** Iteration 1381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1390001
mean reward (100 episodes) 1656.500000
best mean reward 1704.700000
running time 6300.947053
Train_EnvstepsSoFar : 1390001
Train_AverageReturn : 1656.5
Train_BestReturn : 1704.7
TimeSinceStart : 6300.947053432465
Training Loss : 0.5198266506195068
Done logging...




********** Iteration 1391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1400001
mean reward (100 episodes) 1554.300000
best mean reward 1704.700000
running time 6344.552239
Train_EnvstepsSoFar : 1400001
Train_AverageReturn : 1554.3
Train_BestReturn : 1704.7
TimeSinceStart : 6344.552239179611
Training Loss : 0.233184352517128
Done logging...




********** Iteration 1401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1410001
mean reward (100 episodes) 1592.400000
best mean reward 1704.700000
running time 6387.972011
Train_EnvstepsSoFar : 1410001
Train_AverageReturn : 1592.4
Train_BestReturn : 1704.7
TimeSinceStart : 6387.972010850906
Training Loss : 1.5021580457687378
Done logging...




********** Iteration 1411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1420001
mean reward (100 episodes) 1586.100000
best mean reward 1704.700000
running time 6432.149404
Train_EnvstepsSoFar : 1420001
Train_AverageReturn : 1586.1
Train_BestReturn : 1704.7
TimeSinceStart : 6432.149404287338
Training Loss : 0.8979713916778564
Done logging...




********** Iteration 1421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1430001
mean reward (100 episodes) 1566.400000
best mean reward 1704.700000
running time 6475.681359
Train_EnvstepsSoFar : 1430001
Train_AverageReturn : 1566.4
Train_BestReturn : 1704.7
TimeSinceStart : 6475.681359291077
Training Loss : 0.6337110996246338
Done logging...




********** Iteration 1431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1440001
mean reward (100 episodes) 1557.200000
best mean reward 1704.700000
running time 6519.145694
Train_EnvstepsSoFar : 1440001
Train_AverageReturn : 1557.2
Train_BestReturn : 1704.7
TimeSinceStart : 6519.145694255829
Training Loss : 1.3045294284820557
Done logging...




********** Iteration 1441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1450001
mean reward (100 episodes) 1570.400000
best mean reward 1704.700000
running time 6562.556572
Train_EnvstepsSoFar : 1450001
Train_AverageReturn : 1570.4
Train_BestReturn : 1704.7
TimeSinceStart : 6562.556572437286
Training Loss : 0.16791760921478271
Done logging...




********** Iteration 1451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1460001
mean reward (100 episodes) 1626.900000
best mean reward 1704.700000
running time 6605.863955
Train_EnvstepsSoFar : 1460001
Train_AverageReturn : 1626.9
Train_BestReturn : 1704.7
TimeSinceStart : 6605.8639550209045
Training Loss : 0.41270968317985535
Done logging...




********** Iteration 1461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1470001
mean reward (100 episodes) 1614.100000
best mean reward 1704.700000
running time 6649.096944
Train_EnvstepsSoFar : 1470001
Train_AverageReturn : 1614.1
Train_BestReturn : 1704.7
TimeSinceStart : 6649.096944093704
Training Loss : 0.6329219341278076
Done logging...




********** Iteration 1471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1480001
mean reward (100 episodes) 1580.800000
best mean reward 1704.700000
running time 6692.402524
Train_EnvstepsSoFar : 1480001
Train_AverageReturn : 1580.8
Train_BestReturn : 1704.7
TimeSinceStart : 6692.402524471283
Training Loss : 0.756460428237915
Done logging...




********** Iteration 1481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1490001
mean reward (100 episodes) 1607.400000
best mean reward 1704.700000
running time 6735.685308
Train_EnvstepsSoFar : 1490001
Train_AverageReturn : 1607.4
Train_BestReturn : 1704.7
TimeSinceStart : 6735.685307741165
Training Loss : 0.9880213737487793
Done logging...




********** Iteration 1491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1499000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1500000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1500001
mean reward (100 episodes) 1592.000000
best mean reward 1704.700000
running time 6782.640608
Train_EnvstepsSoFar : 1500001
Train_AverageReturn : 1592.0
Train_BestReturn : 1704.7
TimeSinceStart : 6782.640607595444
Training Loss : 0.2895444929599762
Done logging...




********** Iteration 1501000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1502000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1503000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1504000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1505000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1506000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1507000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1508000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1509000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1510000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1510001
mean reward (100 episodes) 1602.600000
best mean reward 1704.700000
running time 6825.605679
Train_EnvstepsSoFar : 1510001
Train_AverageReturn : 1602.6
Train_BestReturn : 1704.7
TimeSinceStart : 6825.60567855835
Training Loss : 1.7300970554351807
Done logging...




********** Iteration 1511000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1512000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1513000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1514000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1515000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1516000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1517000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1518000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1519000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1520000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1520001
mean reward (100 episodes) 1627.800000
best mean reward 1704.700000
running time 6868.822506
Train_EnvstepsSoFar : 1520001
Train_AverageReturn : 1627.8
Train_BestReturn : 1704.7
TimeSinceStart : 6868.822506427765
Training Loss : 0.5703487396240234
Done logging...




********** Iteration 1521000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1522000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1523000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1524000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1525000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1526000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1527000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1528000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1529000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1530000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1530001
mean reward (100 episodes) 1699.600000
best mean reward 1704.700000
running time 6912.361976
Train_EnvstepsSoFar : 1530001
Train_AverageReturn : 1699.6
Train_BestReturn : 1704.7
TimeSinceStart : 6912.361975908279
Training Loss : 0.4942929148674011
Done logging...




********** Iteration 1531000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1532000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1533000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1534000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1535000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1536000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1537000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1538000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1539000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1540000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1540001
mean reward (100 episodes) 1747.400000
best mean reward 1747.400000
running time 6955.637537
Train_EnvstepsSoFar : 1540001
Train_AverageReturn : 1747.4
Train_BestReturn : 1747.4
TimeSinceStart : 6955.6375370025635
Training Loss : 0.6333687901496887
Done logging...




********** Iteration 1541000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1542000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1543000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1544000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1545000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1546000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1547000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1548000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1549000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1550000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1550001
mean reward (100 episodes) 1641.000000
best mean reward 1747.400000
running time 6999.055695
Train_EnvstepsSoFar : 1550001
Train_AverageReturn : 1641.0
Train_BestReturn : 1747.4
TimeSinceStart : 6999.055694580078
Training Loss : 0.3340912461280823
Done logging...




********** Iteration 1551000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1552000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1553000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1554000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1555000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1556000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1557000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1558000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1559000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1560000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1560001
mean reward (100 episodes) 1561.100000
best mean reward 1747.400000
running time 7042.358527
Train_EnvstepsSoFar : 1560001
Train_AverageReturn : 1561.1
Train_BestReturn : 1747.4
TimeSinceStart : 7042.358527183533
Training Loss : 0.5887870192527771
Done logging...




********** Iteration 1561000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1562000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1563000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1564000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1565000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1566000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1567000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1568000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1569000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1570000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1570001
mean reward (100 episodes) 1488.100000
best mean reward 1747.400000
running time 7085.506631
Train_EnvstepsSoFar : 1570001
Train_AverageReturn : 1488.1
Train_BestReturn : 1747.4
TimeSinceStart : 7085.506631135941
Training Loss : 0.39292922616004944
Done logging...




********** Iteration 1571000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1572000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1573000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1574000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1575000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1576000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1577000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1578000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1579000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1580000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1580001
mean reward (100 episodes) 1520.100000
best mean reward 1747.400000
running time 7128.697417
Train_EnvstepsSoFar : 1580001
Train_AverageReturn : 1520.1
Train_BestReturn : 1747.4
TimeSinceStart : 7128.697416782379
Training Loss : 0.42887210845947266
Done logging...




********** Iteration 1581000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1582000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1583000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1584000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1585000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1586000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1587000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1588000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1589000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1590000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1590001
mean reward (100 episodes) 1580.600000
best mean reward 1747.400000
running time 7172.082870
Train_EnvstepsSoFar : 1590001
Train_AverageReturn : 1580.6
Train_BestReturn : 1747.4
TimeSinceStart : 7172.082869529724
Training Loss : 0.2425065040588379
Done logging...




********** Iteration 1591000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1592000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1593000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1594000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1595000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1596000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1597000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1598000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1599000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1600000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1600001
mean reward (100 episodes) 1662.200000
best mean reward 1747.400000
running time 7215.402154
Train_EnvstepsSoFar : 1600001
Train_AverageReturn : 1662.2
Train_BestReturn : 1747.4
TimeSinceStart : 7215.402154445648
Training Loss : 0.5352972745895386
Done logging...




********** Iteration 1601000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1602000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1603000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1604000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1605000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1606000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1607000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1608000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1609000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1610000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1610001
mean reward (100 episodes) 1710.800000
best mean reward 1747.400000
running time 7258.499875
Train_EnvstepsSoFar : 1610001
Train_AverageReturn : 1710.8
Train_BestReturn : 1747.4
TimeSinceStart : 7258.499874830246
Training Loss : 0.413183331489563
Done logging...




********** Iteration 1611000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1612000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1613000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1614000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1615000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1616000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1617000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1618000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1619000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1620000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1620001
mean reward (100 episodes) 1688.800000
best mean reward 1747.400000
running time 7301.739204
Train_EnvstepsSoFar : 1620001
Train_AverageReturn : 1688.8
Train_BestReturn : 1747.4
TimeSinceStart : 7301.739204406738
Training Loss : 0.4552704989910126
Done logging...




********** Iteration 1621000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1622000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1623000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1624000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1625000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1626000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1627000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1628000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1629000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1630000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1630001
mean reward (100 episodes) 1704.200000
best mean reward 1747.400000
running time 7345.137090
Train_EnvstepsSoFar : 1630001
Train_AverageReturn : 1704.2
Train_BestReturn : 1747.4
TimeSinceStart : 7345.137089729309
Training Loss : 0.3224354088306427
Done logging...




********** Iteration 1631000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1632000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1633000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1634000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1635000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1636000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1637000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1638000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1639000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1640000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1640001
mean reward (100 episodes) 1633.200000
best mean reward 1747.400000
running time 7388.621145
Train_EnvstepsSoFar : 1640001
Train_AverageReturn : 1633.2
Train_BestReturn : 1747.4
TimeSinceStart : 7388.621144533157
Training Loss : 0.6639567613601685
Done logging...




********** Iteration 1641000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1642000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1643000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1644000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1645000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1646000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1647000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1648000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1649000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1650000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1650001
mean reward (100 episodes) 1691.300000
best mean reward 1747.400000
running time 7432.127700
Train_EnvstepsSoFar : 1650001
Train_AverageReturn : 1691.3
Train_BestReturn : 1747.4
TimeSinceStart : 7432.127700328827
Training Loss : 0.7238013744354248
Done logging...




********** Iteration 1651000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1652000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1653000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1654000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1655000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1656000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1657000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1658000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1659000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1660000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1660001
mean reward (100 episodes) 1632.800000
best mean reward 1747.400000
running time 7475.729537
Train_EnvstepsSoFar : 1660001
Train_AverageReturn : 1632.8
Train_BestReturn : 1747.4
TimeSinceStart : 7475.729537248611
Training Loss : 0.38465431332588196
Done logging...




********** Iteration 1661000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1662000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1663000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1664000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1665000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1666000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1667000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1668000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1669000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1670000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1670001
mean reward (100 episodes) 1631.700000
best mean reward 1747.400000
running time 7519.413104
Train_EnvstepsSoFar : 1670001
Train_AverageReturn : 1631.7
Train_BestReturn : 1747.4
TimeSinceStart : 7519.413103580475
Training Loss : 0.5374888181686401
Done logging...




********** Iteration 1671000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1672000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1673000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1674000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1675000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1676000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1677000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1678000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1679000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1680000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1680001
mean reward (100 episodes) 1571.100000
best mean reward 1747.400000
running time 7562.810612
Train_EnvstepsSoFar : 1680001
Train_AverageReturn : 1571.1
Train_BestReturn : 1747.4
TimeSinceStart : 7562.810612201691
Training Loss : 0.2037803828716278
Done logging...




********** Iteration 1681000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1682000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1683000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1684000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1685000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1686000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1687000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1688000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1689000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1690000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1690001
mean reward (100 episodes) 1574.200000
best mean reward 1747.400000
running time 7606.233734
Train_EnvstepsSoFar : 1690001
Train_AverageReturn : 1574.2
Train_BestReturn : 1747.4
TimeSinceStart : 7606.233734130859
Training Loss : 1.2967851161956787
Done logging...




********** Iteration 1691000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1692000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1693000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1694000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1695000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1696000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1697000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1698000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1699000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1700000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1700001
mean reward (100 episodes) 1555.600000
best mean reward 1747.400000
running time 7649.710629
Train_EnvstepsSoFar : 1700001
Train_AverageReturn : 1555.6
Train_BestReturn : 1747.4
TimeSinceStart : 7649.710629463196
Training Loss : 0.3128077983856201
Done logging...




********** Iteration 1701000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1702000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1703000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1704000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1705000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1706000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1707000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1708000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1709000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1710000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1710001
mean reward (100 episodes) 1587.000000
best mean reward 1747.400000
running time 7693.172148
Train_EnvstepsSoFar : 1710001
Train_AverageReturn : 1587.0
Train_BestReturn : 1747.4
TimeSinceStart : 7693.172147989273
Training Loss : 0.3953562080860138
Done logging...




********** Iteration 1711000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1712000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1713000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1714000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1715000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1716000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1717000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1718000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1719000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1720000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1720001
mean reward (100 episodes) 1611.900000
best mean reward 1747.400000
running time 7736.856605
Train_EnvstepsSoFar : 1720001
Train_AverageReturn : 1611.9
Train_BestReturn : 1747.4
TimeSinceStart : 7736.856605291367
Training Loss : 0.6543768048286438
Done logging...




********** Iteration 1721000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1722000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1723000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1724000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1725000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1726000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1727000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1728000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1729000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1730000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1730001
mean reward (100 episodes) 1618.800000
best mean reward 1747.400000
running time 7780.347137
Train_EnvstepsSoFar : 1730001
Train_AverageReturn : 1618.8
Train_BestReturn : 1747.4
TimeSinceStart : 7780.347137451172
Training Loss : 0.546380877494812
Done logging...




********** Iteration 1731000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1732000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1733000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1734000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1735000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1736000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1737000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1738000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1739000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1740000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1740001
mean reward (100 episodes) 1594.000000
best mean reward 1747.400000
running time 7823.801255
Train_EnvstepsSoFar : 1740001
Train_AverageReturn : 1594.0
Train_BestReturn : 1747.4
TimeSinceStart : 7823.8012545108795
Training Loss : 0.7892807126045227
Done logging...




********** Iteration 1741000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1742000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1743000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1744000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1745000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1746000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1747000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1748000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1749000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1750000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1750001
mean reward (100 episodes) 1634.200000
best mean reward 1747.400000
running time 7867.244670
Train_EnvstepsSoFar : 1750001
Train_AverageReturn : 1634.2
Train_BestReturn : 1747.4
TimeSinceStart : 7867.244670152664
Training Loss : 0.32726794481277466
Done logging...




********** Iteration 1751000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1752000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1753000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1754000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1755000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1756000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1757000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1758000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1759000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1760000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1760001
mean reward (100 episodes) 1703.700000
best mean reward 1747.400000
running time 7910.674376
Train_EnvstepsSoFar : 1760001
Train_AverageReturn : 1703.7
Train_BestReturn : 1747.4
TimeSinceStart : 7910.674375772476
Training Loss : 0.5746841430664062
Done logging...




********** Iteration 1761000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1762000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1763000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1764000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1765000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1766000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1767000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1768000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1769000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1770000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1770001
mean reward (100 episodes) 1719.700000
best mean reward 1747.400000
running time 7954.164299
Train_EnvstepsSoFar : 1770001
Train_AverageReturn : 1719.7
Train_BestReturn : 1747.4
TimeSinceStart : 7954.164299249649
Training Loss : 1.025593876838684
Done logging...




********** Iteration 1771000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1772000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1773000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1774000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1775000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1776000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1777000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1778000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1779000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1780000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1780001
mean reward (100 episodes) 1744.800000
best mean reward 1747.400000
running time 7997.697562
Train_EnvstepsSoFar : 1780001
Train_AverageReturn : 1744.8
Train_BestReturn : 1747.4
TimeSinceStart : 7997.697562456131
Training Loss : 0.4421496093273163
Done logging...




********** Iteration 1781000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1782000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1783000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1784000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1785000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1786000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1787000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1788000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1789000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1790000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1790001
mean reward (100 episodes) 1683.000000
best mean reward 1747.400000
running time 8041.205663
Train_EnvstepsSoFar : 1790001
Train_AverageReturn : 1683.0
Train_BestReturn : 1747.4
TimeSinceStart : 8041.2056629657745
Training Loss : 0.2698444724082947
Done logging...




********** Iteration 1791000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1792000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1793000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1794000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1795000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1796000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1797000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1798000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1799000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1800000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1800001
mean reward (100 episodes) 1674.200000
best mean reward 1747.400000
running time 8084.474212
Train_EnvstepsSoFar : 1800001
Train_AverageReturn : 1674.2
Train_BestReturn : 1747.4
TimeSinceStart : 8084.474212169647
Training Loss : 0.21680143475532532
Done logging...




********** Iteration 1801000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1802000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1803000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1804000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1805000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1806000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1807000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1808000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1809000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1810000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1810001
mean reward (100 episodes) 1672.500000
best mean reward 1747.400000
running time 8127.765987
Train_EnvstepsSoFar : 1810001
Train_AverageReturn : 1672.5
Train_BestReturn : 1747.4
TimeSinceStart : 8127.7659866809845
Training Loss : 0.3042412996292114
Done logging...




********** Iteration 1811000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1812000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1813000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1814000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1815000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1816000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1817000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1818000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1819000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1820000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1820001
mean reward (100 episodes) 1664.200000
best mean reward 1747.400000
running time 8171.010907
Train_EnvstepsSoFar : 1820001
Train_AverageReturn : 1664.2
Train_BestReturn : 1747.4
TimeSinceStart : 8171.010906934738
Training Loss : 0.1774376928806305
Done logging...




********** Iteration 1821000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1822000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1823000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1824000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1825000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1826000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1827000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1828000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1829000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1830000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1830001
mean reward (100 episodes) 1626.400000
best mean reward 1747.400000
running time 8214.373670
Train_EnvstepsSoFar : 1830001
Train_AverageReturn : 1626.4
Train_BestReturn : 1747.4
TimeSinceStart : 8214.373669862747
Training Loss : 0.19655704498291016
Done logging...




********** Iteration 1831000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1832000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1833000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1834000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1835000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1836000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1837000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1838000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1839000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1840000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1840001
mean reward (100 episodes) 1593.000000
best mean reward 1747.400000
running time 8257.674493
Train_EnvstepsSoFar : 1840001
Train_AverageReturn : 1593.0
Train_BestReturn : 1747.4
TimeSinceStart : 8257.674492835999
Training Loss : 0.2077307552099228
Done logging...




********** Iteration 1841000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1842000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1843000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1844000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1845000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1846000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1847000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1848000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1849000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1850000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1850001
mean reward (100 episodes) 1637.500000
best mean reward 1747.400000
running time 8301.484276
Train_EnvstepsSoFar : 1850001
Train_AverageReturn : 1637.5
Train_BestReturn : 1747.4
TimeSinceStart : 8301.48427605629
Training Loss : 0.4454755187034607
Done logging...




********** Iteration 1851000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1852000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1853000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1854000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1855000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1856000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1857000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1858000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1859000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1860000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1860001
mean reward (100 episodes) 1733.600000
best mean reward 1747.400000
running time 8344.867046
Train_EnvstepsSoFar : 1860001
Train_AverageReturn : 1733.6
Train_BestReturn : 1747.4
TimeSinceStart : 8344.867045879364
Training Loss : 0.32505935430526733
Done logging...




********** Iteration 1861000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1862000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1863000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1864000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1865000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1866000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1867000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1868000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1869000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1870000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1870001
mean reward (100 episodes) 1776.600000
best mean reward 1776.600000
running time 8388.132222
Train_EnvstepsSoFar : 1870001
Train_AverageReturn : 1776.6
Train_BestReturn : 1776.6
TimeSinceStart : 8388.13222193718
Training Loss : 0.17428478598594666
Done logging...




********** Iteration 1871000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1872000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1873000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1874000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1875000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1876000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1877000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1878000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1879000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1880000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1880001
mean reward (100 episodes) 1856.100000
best mean reward 1856.100000
running time 8431.424834
Train_EnvstepsSoFar : 1880001
Train_AverageReturn : 1856.1
Train_BestReturn : 1856.1
TimeSinceStart : 8431.424834012985
Training Loss : 1.7239540815353394
Done logging...




********** Iteration 1881000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1882000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1883000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1884000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1885000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1886000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1887000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1888000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1889000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1890000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1890001
mean reward (100 episodes) 1789.300000
best mean reward 1856.100000
running time 8474.518232
Train_EnvstepsSoFar : 1890001
Train_AverageReturn : 1789.3
Train_BestReturn : 1856.1
TimeSinceStart : 8474.518232345581
Training Loss : 0.5033518671989441
Done logging...




********** Iteration 1891000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1892000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1893000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1894000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1895000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1896000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1897000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1898000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1899000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1900000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1900001
mean reward (100 episodes) 1798.200000
best mean reward 1856.100000
running time 8517.891964
Train_EnvstepsSoFar : 1900001
Train_AverageReturn : 1798.2
Train_BestReturn : 1856.1
TimeSinceStart : 8517.891963720322
Training Loss : 0.14293219149112701
Done logging...




********** Iteration 1901000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1902000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1903000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1904000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1905000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1906000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1907000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1908000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1909000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1910000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1910001
mean reward (100 episodes) 1688.100000
best mean reward 1856.100000
running time 8561.453556
Train_EnvstepsSoFar : 1910001
Train_AverageReturn : 1688.1
Train_BestReturn : 1856.1
TimeSinceStart : 8561.453555583954
Training Loss : 0.27809515595436096
Done logging...




********** Iteration 1911000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1912000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1913000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1914000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1915000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1916000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1917000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1918000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1919000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1920000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1920001
mean reward (100 episodes) 1680.600000
best mean reward 1856.100000
running time 8605.390229
Train_EnvstepsSoFar : 1920001
Train_AverageReturn : 1680.6
Train_BestReturn : 1856.1
TimeSinceStart : 8605.390228748322
Training Loss : 0.22004537284374237
Done logging...




********** Iteration 1921000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1922000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1923000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1924000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1925000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1926000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1927000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1928000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1929000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1930000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1930001
mean reward (100 episodes) 1652.700000
best mean reward 1856.100000
running time 8648.909508
Train_EnvstepsSoFar : 1930001
Train_AverageReturn : 1652.7
Train_BestReturn : 1856.1
TimeSinceStart : 8648.909507513046
Training Loss : 0.1270061731338501
Done logging...




********** Iteration 1931000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1932000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1933000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1934000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1935000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1936000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1937000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1938000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1939000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1940000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1940001
mean reward (100 episodes) 1754.600000
best mean reward 1856.100000
running time 8692.360753
Train_EnvstepsSoFar : 1940001
Train_AverageReturn : 1754.6
Train_BestReturn : 1856.1
TimeSinceStart : 8692.36075258255
Training Loss : 0.5831102728843689
Done logging...




********** Iteration 1941000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1942000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1943000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1944000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1945000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1946000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1947000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1948000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1949000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1950000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1950001
mean reward (100 episodes) 1761.000000
best mean reward 1856.100000
running time 8736.479281
Train_EnvstepsSoFar : 1950001
Train_AverageReturn : 1761.0
Train_BestReturn : 1856.1
TimeSinceStart : 8736.479280948639
Training Loss : 0.5210440158843994
Done logging...




********** Iteration 1951000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1952000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1953000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1954000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1955000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1956000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1957000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1958000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1959000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1960000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1960001
mean reward (100 episodes) 1815.900000
best mean reward 1856.100000
running time 8780.053618
Train_EnvstepsSoFar : 1960001
Train_AverageReturn : 1815.9
Train_BestReturn : 1856.1
TimeSinceStart : 8780.053618192673
Training Loss : 0.34357699751853943
Done logging...




********** Iteration 1961000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1962000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1963000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1964000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1965000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1966000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1967000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1968000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1969000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1970000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1970001
mean reward (100 episodes) 1828.400000
best mean reward 1856.100000
running time 8823.655986
Train_EnvstepsSoFar : 1970001
Train_AverageReturn : 1828.4
Train_BestReturn : 1856.1
TimeSinceStart : 8823.655985832214
Training Loss : 0.17317929863929749
Done logging...




********** Iteration 1971000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1972000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1973000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1974000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1975000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1976000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1977000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1978000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1979000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1980000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1980001
mean reward (100 episodes) 1839.700000
best mean reward 1856.100000
running time 8866.933312
Train_EnvstepsSoFar : 1980001
Train_AverageReturn : 1839.7
Train_BestReturn : 1856.1
TimeSinceStart : 8866.933312177658
Training Loss : 0.3013114333152771
Done logging...




********** Iteration 1981000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1982000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1983000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1984000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1985000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1986000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1987000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1988000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1989000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1990000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 1990001
mean reward (100 episodes) 1835.800000
best mean reward 1856.100000
running time 8910.293782
Train_EnvstepsSoFar : 1990001
Train_AverageReturn : 1835.8
Train_BestReturn : 1856.1
TimeSinceStart : 8910.293781995773
Training Loss : 0.4938274919986725
Done logging...




********** Iteration 1991000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1992000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1993000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1994000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1995000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1996000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1997000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1998000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 1999000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2000000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2000001
mean reward (100 episodes) 1714.000000
best mean reward 1856.100000
running time 8956.504225
Train_EnvstepsSoFar : 2000001
Train_AverageReturn : 1714.0
Train_BestReturn : 1856.1
TimeSinceStart : 8956.504225254059
Training Loss : 0.20676171779632568
Done logging...




********** Iteration 2001000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2002000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2003000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2004000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2005000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2006000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2007000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2008000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2009000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2010000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2010001
mean reward (100 episodes) 1733.000000
best mean reward 1856.100000
running time 9001.503710
Train_EnvstepsSoFar : 2010001
Train_AverageReturn : 1733.0
Train_BestReturn : 1856.1
TimeSinceStart : 9001.50370979309
Training Loss : 0.45037946105003357
Done logging...




********** Iteration 2011000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2012000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2013000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2014000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2015000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2016000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2017000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2018000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2019000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2020000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2020001
mean reward (100 episodes) 1741.600000
best mean reward 1856.100000
running time 9044.628274
Train_EnvstepsSoFar : 2020001
Train_AverageReturn : 1741.6
Train_BestReturn : 1856.1
TimeSinceStart : 9044.62827372551
Training Loss : 0.38373425602912903
Done logging...




********** Iteration 2021000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2022000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2023000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2024000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2025000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2026000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2027000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2028000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2029000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2030000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2030001
mean reward (100 episodes) 1722.500000
best mean reward 1856.100000
running time 9087.880459
Train_EnvstepsSoFar : 2030001
Train_AverageReturn : 1722.5
Train_BestReturn : 1856.1
TimeSinceStart : 9087.880459308624
Training Loss : 0.32486099004745483
Done logging...




********** Iteration 2031000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2032000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2033000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2034000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2035000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2036000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2037000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2038000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2039000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2040000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2040001
mean reward (100 episodes) 1687.200000
best mean reward 1856.100000
running time 9131.369357
Train_EnvstepsSoFar : 2040001
Train_AverageReturn : 1687.2
Train_BestReturn : 1856.1
TimeSinceStart : 9131.369356632233
Training Loss : 0.2767109274864197
Done logging...




********** Iteration 2041000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2042000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2043000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2044000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2045000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2046000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2047000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2048000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2049000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2050000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2050001
mean reward (100 episodes) 1666.300000
best mean reward 1856.100000
running time 9174.693979
Train_EnvstepsSoFar : 2050001
Train_AverageReturn : 1666.3
Train_BestReturn : 1856.1
TimeSinceStart : 9174.69397854805
Training Loss : 0.23527617752552032
Done logging...




********** Iteration 2051000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2052000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2053000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2054000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2055000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2056000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2057000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2058000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2059000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2060000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2060001
mean reward (100 episodes) 1758.400000
best mean reward 1856.100000
running time 9217.978105
Train_EnvstepsSoFar : 2060001
Train_AverageReturn : 1758.4
Train_BestReturn : 1856.1
TimeSinceStart : 9217.978105068207
Training Loss : 0.28096839785575867
Done logging...




********** Iteration 2061000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2062000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2063000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2064000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2065000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2066000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2067000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2068000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2069000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2070000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2070001
mean reward (100 episodes) 1789.000000
best mean reward 1856.100000
running time 9261.300621
Train_EnvstepsSoFar : 2070001
Train_AverageReturn : 1789.0
Train_BestReturn : 1856.1
TimeSinceStart : 9261.300620555878
Training Loss : 0.14981171488761902
Done logging...




********** Iteration 2071000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2072000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2073000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2074000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2075000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2076000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2077000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2078000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2079000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2080000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2080001
mean reward (100 episodes) 1813.700000
best mean reward 1856.100000
running time 9304.946370
Train_EnvstepsSoFar : 2080001
Train_AverageReturn : 1813.7
Train_BestReturn : 1856.1
TimeSinceStart : 9304.946370124817
Training Loss : 0.42786726355552673
Done logging...




********** Iteration 2081000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2082000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2083000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2084000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2085000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2086000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2087000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2088000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2089000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2090000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2090001
mean reward (100 episodes) 1761.800000
best mean reward 1856.100000
running time 9348.225776
Train_EnvstepsSoFar : 2090001
Train_AverageReturn : 1761.8
Train_BestReturn : 1856.1
TimeSinceStart : 9348.225775957108
Training Loss : 0.44271159172058105
Done logging...




********** Iteration 2091000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2092000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2093000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2094000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2095000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2096000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2097000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2098000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2099000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2100001
mean reward (100 episodes) 1700.100000
best mean reward 1856.100000
running time 9391.627943
Train_EnvstepsSoFar : 2100001
Train_AverageReturn : 1700.1
Train_BestReturn : 1856.1
TimeSinceStart : 9391.627942562103
Training Loss : 1.1516573429107666
Done logging...




********** Iteration 2101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2110001
mean reward (100 episodes) 1681.900000
best mean reward 1856.100000
running time 9434.911311
Train_EnvstepsSoFar : 2110001
Train_AverageReturn : 1681.9
Train_BestReturn : 1856.1
TimeSinceStart : 9434.91131067276
Training Loss : 0.4364001750946045
Done logging...




********** Iteration 2111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2120001
mean reward (100 episodes) 1754.300000
best mean reward 1856.100000
running time 9478.083669
Train_EnvstepsSoFar : 2120001
Train_AverageReturn : 1754.3
Train_BestReturn : 1856.1
TimeSinceStart : 9478.083668708801
Training Loss : 0.5270679593086243
Done logging...




********** Iteration 2121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2130001
mean reward (100 episodes) 1707.000000
best mean reward 1856.100000
running time 9521.369361
Train_EnvstepsSoFar : 2130001
Train_AverageReturn : 1707.0
Train_BestReturn : 1856.1
TimeSinceStart : 9521.369361162186
Training Loss : 0.3587836027145386
Done logging...




********** Iteration 2131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2140001
mean reward (100 episodes) 1634.500000
best mean reward 1856.100000
running time 9564.826020
Train_EnvstepsSoFar : 2140001
Train_AverageReturn : 1634.5
Train_BestReturn : 1856.1
TimeSinceStart : 9564.826019763947
Training Loss : 0.2672232389450073
Done logging...




********** Iteration 2141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2150001
mean reward (100 episodes) 1623.300000
best mean reward 1856.100000
running time 9608.343616
Train_EnvstepsSoFar : 2150001
Train_AverageReturn : 1623.3
Train_BestReturn : 1856.1
TimeSinceStart : 9608.34361577034
Training Loss : 0.5283398628234863
Done logging...




********** Iteration 2151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2160001
mean reward (100 episodes) 1631.200000
best mean reward 1856.100000
running time 9651.614888
Train_EnvstepsSoFar : 2160001
Train_AverageReturn : 1631.2
Train_BestReturn : 1856.1
TimeSinceStart : 9651.614887714386
Training Loss : 0.4898313879966736
Done logging...




********** Iteration 2161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2170001
mean reward (100 episodes) 1629.700000
best mean reward 1856.100000
running time 9694.898456
Train_EnvstepsSoFar : 2170001
Train_AverageReturn : 1629.7
Train_BestReturn : 1856.1
TimeSinceStart : 9694.89845609665
Training Loss : 0.8318549394607544
Done logging...




********** Iteration 2171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2180001
mean reward (100 episodes) 1619.900000
best mean reward 1856.100000
running time 9738.301703
Train_EnvstepsSoFar : 2180001
Train_AverageReturn : 1619.9
Train_BestReturn : 1856.1
TimeSinceStart : 9738.301703214645
Training Loss : 0.40838319063186646
Done logging...




********** Iteration 2181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2190001
mean reward (100 episodes) 1580.600000
best mean reward 1856.100000
running time 9781.894814
Train_EnvstepsSoFar : 2190001
Train_AverageReturn : 1580.6
Train_BestReturn : 1856.1
TimeSinceStart : 9781.894814014435
Training Loss : 0.41482317447662354
Done logging...




********** Iteration 2191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2200001
mean reward (100 episodes) 1645.600000
best mean reward 1856.100000
running time 9825.440475
Train_EnvstepsSoFar : 2200001
Train_AverageReturn : 1645.6
Train_BestReturn : 1856.1
TimeSinceStart : 9825.440475463867
Training Loss : 0.24860234558582306
Done logging...




********** Iteration 2201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2210001
mean reward (100 episodes) 1651.300000
best mean reward 1856.100000
running time 9869.061912
Train_EnvstepsSoFar : 2210001
Train_AverageReturn : 1651.3
Train_BestReturn : 1856.1
TimeSinceStart : 9869.061911821365
Training Loss : 0.2925560176372528
Done logging...




********** Iteration 2211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2220001
mean reward (100 episodes) 1709.900000
best mean reward 1856.100000
running time 9912.398258
Train_EnvstepsSoFar : 2220001
Train_AverageReturn : 1709.9
Train_BestReturn : 1856.1
TimeSinceStart : 9912.398257732391
Training Loss : 0.7059295773506165
Done logging...




********** Iteration 2221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2230001
mean reward (100 episodes) 1639.300000
best mean reward 1856.100000
running time 9955.572112
Train_EnvstepsSoFar : 2230001
Train_AverageReturn : 1639.3
Train_BestReturn : 1856.1
TimeSinceStart : 9955.572112321854
Training Loss : 0.7266451120376587
Done logging...




********** Iteration 2231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2240001
mean reward (100 episodes) 1676.700000
best mean reward 1856.100000
running time 9998.920134
Train_EnvstepsSoFar : 2240001
Train_AverageReturn : 1676.7
Train_BestReturn : 1856.1
TimeSinceStart : 9998.920133829117
Training Loss : 0.1566396951675415
Done logging...




********** Iteration 2241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2250001
mean reward (100 episodes) 1615.500000
best mean reward 1856.100000
running time 10041.959147
Train_EnvstepsSoFar : 2250001
Train_AverageReturn : 1615.5
Train_BestReturn : 1856.1
TimeSinceStart : 10041.959146738052
Training Loss : 0.3143777847290039
Done logging...




********** Iteration 2251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2260001
mean reward (100 episodes) 1628.400000
best mean reward 1856.100000
running time 10084.969031
Train_EnvstepsSoFar : 2260001
Train_AverageReturn : 1628.4
Train_BestReturn : 1856.1
TimeSinceStart : 10084.969030618668
Training Loss : 1.204282522201538
Done logging...




********** Iteration 2261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2270001
mean reward (100 episodes) 1603.600000
best mean reward 1856.100000
running time 10128.095513
Train_EnvstepsSoFar : 2270001
Train_AverageReturn : 1603.6
Train_BestReturn : 1856.1
TimeSinceStart : 10128.095513343811
Training Loss : 1.7313392162322998
Done logging...




********** Iteration 2271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2280001
mean reward (100 episodes) 1725.400000
best mean reward 1856.100000
running time 10171.164095
Train_EnvstepsSoFar : 2280001
Train_AverageReturn : 1725.4
Train_BestReturn : 1856.1
TimeSinceStart : 10171.164095163345
Training Loss : 1.0811041593551636
Done logging...




********** Iteration 2281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2290001
mean reward (100 episodes) 1727.800000
best mean reward 1856.100000
running time 10214.172099
Train_EnvstepsSoFar : 2290001
Train_AverageReturn : 1727.8
Train_BestReturn : 1856.1
TimeSinceStart : 10214.172098636627
Training Loss : 0.35171979665756226
Done logging...




********** Iteration 2291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2300001
mean reward (100 episodes) 1759.700000
best mean reward 1856.100000
running time 10257.036649
Train_EnvstepsSoFar : 2300001
Train_AverageReturn : 1759.7
Train_BestReturn : 1856.1
TimeSinceStart : 10257.036648750305
Training Loss : 0.17997851967811584
Done logging...




********** Iteration 2301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2310001
mean reward (100 episodes) 1556.100000
best mean reward 1856.100000
running time 10300.068397
Train_EnvstepsSoFar : 2310001
Train_AverageReturn : 1556.1
Train_BestReturn : 1856.1
TimeSinceStart : 10300.068397045135
Training Loss : 0.40143507719039917
Done logging...




********** Iteration 2311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2320001
mean reward (100 episodes) 1620.100000
best mean reward 1856.100000
running time 10342.997989
Train_EnvstepsSoFar : 2320001
Train_AverageReturn : 1620.1
Train_BestReturn : 1856.1
TimeSinceStart : 10342.997988939285
Training Loss : 0.15976457297801971
Done logging...




********** Iteration 2321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2330001
mean reward (100 episodes) 1636.700000
best mean reward 1856.100000
running time 10385.862967
Train_EnvstepsSoFar : 2330001
Train_AverageReturn : 1636.7
Train_BestReturn : 1856.1
TimeSinceStart : 10385.862966775894
Training Loss : 0.3065337836742401
Done logging...




********** Iteration 2331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2340001
mean reward (100 episodes) 1689.000000
best mean reward 1856.100000
running time 10428.968511
Train_EnvstepsSoFar : 2340001
Train_AverageReturn : 1689.0
Train_BestReturn : 1856.1
TimeSinceStart : 10428.968511343002
Training Loss : 1.4412426948547363
Done logging...




********** Iteration 2341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2350001
mean reward (100 episodes) 1760.700000
best mean reward 1856.100000
running time 10471.948557
Train_EnvstepsSoFar : 2350001
Train_AverageReturn : 1760.7
Train_BestReturn : 1856.1
TimeSinceStart : 10471.948556661606
Training Loss : 0.4221743047237396
Done logging...




********** Iteration 2351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2360001
mean reward (100 episodes) 1698.600000
best mean reward 1856.100000
running time 10514.973730
Train_EnvstepsSoFar : 2360001
Train_AverageReturn : 1698.6
Train_BestReturn : 1856.1
TimeSinceStart : 10514.973729610443
Training Loss : 1.2826682329177856
Done logging...




********** Iteration 2361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2370001
mean reward (100 episodes) 1750.400000
best mean reward 1856.100000
running time 10557.925051
Train_EnvstepsSoFar : 2370001
Train_AverageReturn : 1750.4
Train_BestReturn : 1856.1
TimeSinceStart : 10557.92505121231
Training Loss : 0.5980482697486877
Done logging...




********** Iteration 2371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2380001
mean reward (100 episodes) 1640.800000
best mean reward 1856.100000
running time 10600.800435
Train_EnvstepsSoFar : 2380001
Train_AverageReturn : 1640.8
Train_BestReturn : 1856.1
TimeSinceStart : 10600.800434827805
Training Loss : 0.3628150224685669
Done logging...




********** Iteration 2381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2390001
mean reward (100 episodes) 1641.900000
best mean reward 1856.100000
running time 10643.724503
Train_EnvstepsSoFar : 2390001
Train_AverageReturn : 1641.9
Train_BestReturn : 1856.1
TimeSinceStart : 10643.724503278732
Training Loss : 0.15452882647514343
Done logging...




********** Iteration 2391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2400001
mean reward (100 episodes) 1642.600000
best mean reward 1856.100000
running time 10686.695600
Train_EnvstepsSoFar : 2400001
Train_AverageReturn : 1642.6
Train_BestReturn : 1856.1
TimeSinceStart : 10686.69559955597
Training Loss : 0.7692232131958008
Done logging...




********** Iteration 2401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2410001
mean reward (100 episodes) 1699.700000
best mean reward 1856.100000
running time 10729.319447
Train_EnvstepsSoFar : 2410001
Train_AverageReturn : 1699.7
Train_BestReturn : 1856.1
TimeSinceStart : 10729.31944656372
Training Loss : 0.3811534345149994
Done logging...




********** Iteration 2411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2420001
mean reward (100 episodes) 1716.200000
best mean reward 1856.100000
running time 10772.067888
Train_EnvstepsSoFar : 2420001
Train_AverageReturn : 1716.2
Train_BestReturn : 1856.1
TimeSinceStart : 10772.067888498306
Training Loss : 0.5378629565238953
Done logging...




********** Iteration 2421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2430001
mean reward (100 episodes) 1702.500000
best mean reward 1856.100000
running time 10815.062534
Train_EnvstepsSoFar : 2430001
Train_AverageReturn : 1702.5
Train_BestReturn : 1856.1
TimeSinceStart : 10815.062533855438
Training Loss : 0.8737044930458069
Done logging...




********** Iteration 2431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2440001
mean reward (100 episodes) 1724.600000
best mean reward 1856.100000
running time 10857.958254
Train_EnvstepsSoFar : 2440001
Train_AverageReturn : 1724.6
Train_BestReturn : 1856.1
TimeSinceStart : 10857.958253860474
Training Loss : 0.5235768556594849
Done logging...




********** Iteration 2441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2450001
mean reward (100 episodes) 1747.300000
best mean reward 1856.100000
running time 10900.833062
Train_EnvstepsSoFar : 2450001
Train_AverageReturn : 1747.3
Train_BestReturn : 1856.1
TimeSinceStart : 10900.833061695099
Training Loss : 0.30851036310195923
Done logging...




********** Iteration 2451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2460001
mean reward (100 episodes) 1686.600000
best mean reward 1856.100000
running time 10943.827993
Train_EnvstepsSoFar : 2460001
Train_AverageReturn : 1686.6
Train_BestReturn : 1856.1
TimeSinceStart : 10943.827992677689
Training Loss : 0.5137205123901367
Done logging...




********** Iteration 2461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2470001
mean reward (100 episodes) 1671.600000
best mean reward 1856.100000
running time 10986.618099
Train_EnvstepsSoFar : 2470001
Train_AverageReturn : 1671.6
Train_BestReturn : 1856.1
TimeSinceStart : 10986.61809873581
Training Loss : 1.5579726696014404
Done logging...




********** Iteration 2471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2480001
mean reward (100 episodes) 1628.400000
best mean reward 1856.100000
running time 11029.327796
Train_EnvstepsSoFar : 2480001
Train_AverageReturn : 1628.4
Train_BestReturn : 1856.1
TimeSinceStart : 11029.327795743942
Training Loss : 0.27754098176956177
Done logging...




********** Iteration 2481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2490001
mean reward (100 episodes) 1689.200000
best mean reward 1856.100000
running time 11072.165602
Train_EnvstepsSoFar : 2490001
Train_AverageReturn : 1689.2
Train_BestReturn : 1856.1
TimeSinceStart : 11072.165602445602
Training Loss : 0.16930393874645233
Done logging...




********** Iteration 2491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2499000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2500000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2500001
mean reward (100 episodes) 1694.400000
best mean reward 1856.100000
running time 11119.106973
Train_EnvstepsSoFar : 2500001
Train_AverageReturn : 1694.4
Train_BestReturn : 1856.1
TimeSinceStart : 11119.106972694397
Training Loss : 0.20382079482078552
Done logging...




********** Iteration 2501000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2502000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2503000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2504000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2505000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2506000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2507000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2508000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2509000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2510000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2510001
mean reward (100 episodes) 1707.100000
best mean reward 1856.100000
running time 11161.462993
Train_EnvstepsSoFar : 2510001
Train_AverageReturn : 1707.1
Train_BestReturn : 1856.1
TimeSinceStart : 11161.462992668152
Training Loss : 0.4334644675254822
Done logging...




********** Iteration 2511000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2512000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2513000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2514000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2515000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2516000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2517000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2518000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2519000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2520000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2520001
mean reward (100 episodes) 1715.500000
best mean reward 1856.100000
running time 11204.543179
Train_EnvstepsSoFar : 2520001
Train_AverageReturn : 1715.5
Train_BestReturn : 1856.1
TimeSinceStart : 11204.54317855835
Training Loss : 1.314785361289978
Done logging...




********** Iteration 2521000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2522000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2523000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2524000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2525000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2526000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2527000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2528000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2529000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2530000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2530001
mean reward (100 episodes) 1661.200000
best mean reward 1856.100000
running time 11247.432049
Train_EnvstepsSoFar : 2530001
Train_AverageReturn : 1661.2
Train_BestReturn : 1856.1
TimeSinceStart : 11247.432048559189
Training Loss : 0.2252013385295868
Done logging...




********** Iteration 2531000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2532000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2533000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2534000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2535000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2536000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2537000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2538000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2539000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2540000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2540001
mean reward (100 episodes) 1638.600000
best mean reward 1856.100000
running time 11290.675898
Train_EnvstepsSoFar : 2540001
Train_AverageReturn : 1638.6
Train_BestReturn : 1856.1
TimeSinceStart : 11290.675897598267
Training Loss : 0.1481708288192749
Done logging...




********** Iteration 2541000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2542000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2543000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2544000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2545000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2546000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2547000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2548000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2549000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2550000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2550001
mean reward (100 episodes) 1628.100000
best mean reward 1856.100000
running time 11333.645200
Train_EnvstepsSoFar : 2550001
Train_AverageReturn : 1628.1
Train_BestReturn : 1856.1
TimeSinceStart : 11333.645200252533
Training Loss : 0.2511768639087677
Done logging...




********** Iteration 2551000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2552000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2553000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2554000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2555000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2556000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2557000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2558000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2559000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2560000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2560001
mean reward (100 episodes) 1681.300000
best mean reward 1856.100000
running time 11376.483007
Train_EnvstepsSoFar : 2560001
Train_AverageReturn : 1681.3
Train_BestReturn : 1856.1
TimeSinceStart : 11376.483007192612
Training Loss : 0.19969576597213745
Done logging...




********** Iteration 2561000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2562000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2563000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2564000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2565000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2566000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2567000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2568000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2569000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2570000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2570001
mean reward (100 episodes) 1698.000000
best mean reward 1856.100000
running time 11419.333325
Train_EnvstepsSoFar : 2570001
Train_AverageReturn : 1698.0
Train_BestReturn : 1856.1
TimeSinceStart : 11419.333325386047
Training Loss : 1.4334003925323486
Done logging...




********** Iteration 2571000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2572000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2573000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2574000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2575000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2576000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2577000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2578000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2579000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2580000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2580001
mean reward (100 episodes) 1802.800000
best mean reward 1856.100000
running time 11462.282820
Train_EnvstepsSoFar : 2580001
Train_AverageReturn : 1802.8
Train_BestReturn : 1856.1
TimeSinceStart : 11462.282819747925
Training Loss : 0.43016940355300903
Done logging...




********** Iteration 2581000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2582000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2583000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2584000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2585000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2586000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2587000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2588000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2589000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2590000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2590001
mean reward (100 episodes) 1708.800000
best mean reward 1856.100000
running time 11505.140437
Train_EnvstepsSoFar : 2590001
Train_AverageReturn : 1708.8
Train_BestReturn : 1856.1
TimeSinceStart : 11505.14043712616
Training Loss : 0.16603121161460876
Done logging...




********** Iteration 2591000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2592000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2593000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2594000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2595000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2596000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2597000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2598000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2599000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2600000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2600001
mean reward (100 episodes) 1686.400000
best mean reward 1856.100000
running time 11547.944386
Train_EnvstepsSoFar : 2600001
Train_AverageReturn : 1686.4
Train_BestReturn : 1856.1
TimeSinceStart : 11547.94438624382
Training Loss : 1.3486615419387817
Done logging...




********** Iteration 2601000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2602000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2603000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2604000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2605000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2606000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2607000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2608000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2609000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2610000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2610001
mean reward (100 episodes) 1668.100000
best mean reward 1856.100000
running time 11590.752771
Train_EnvstepsSoFar : 2610001
Train_AverageReturn : 1668.1
Train_BestReturn : 1856.1
TimeSinceStart : 11590.752770662308
Training Loss : 0.17637453973293304
Done logging...




********** Iteration 2611000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2612000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2613000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2614000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2615000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2616000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2617000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2618000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2619000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2620000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2620001
mean reward (100 episodes) 1757.100000
best mean reward 1856.100000
running time 11633.579599
Train_EnvstepsSoFar : 2620001
Train_AverageReturn : 1757.1
Train_BestReturn : 1856.1
TimeSinceStart : 11633.579598665237
Training Loss : 0.38084426522254944
Done logging...




********** Iteration 2621000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2622000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2623000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2624000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2625000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2626000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2627000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2628000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2629000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2630000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2630001
mean reward (100 episodes) 1776.000000
best mean reward 1856.100000
running time 11676.183489
Train_EnvstepsSoFar : 2630001
Train_AverageReturn : 1776.0
Train_BestReturn : 1856.1
TimeSinceStart : 11676.183489322662
Training Loss : 0.26458674669265747
Done logging...




********** Iteration 2631000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2632000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2633000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2634000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2635000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2636000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2637000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2638000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2639000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2640000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2640001
mean reward (100 episodes) 1774.600000
best mean reward 1856.100000
running time 11718.815186
Train_EnvstepsSoFar : 2640001
Train_AverageReturn : 1774.6
Train_BestReturn : 1856.1
TimeSinceStart : 11718.815185546875
Training Loss : 0.15014716982841492
Done logging...




********** Iteration 2641000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2642000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2643000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2644000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2645000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2646000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2647000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2648000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2649000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2650000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2650001
mean reward (100 episodes) 1726.500000
best mean reward 1856.100000
running time 11761.501728
Train_EnvstepsSoFar : 2650001
Train_AverageReturn : 1726.5
Train_BestReturn : 1856.1
TimeSinceStart : 11761.501727819443
Training Loss : 0.535052478313446
Done logging...




********** Iteration 2651000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2652000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2653000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2654000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2655000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2656000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2657000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2658000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2659000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2660000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2660001
mean reward (100 episodes) 1762.500000
best mean reward 1856.100000
running time 11804.237087
Train_EnvstepsSoFar : 2660001
Train_AverageReturn : 1762.5
Train_BestReturn : 1856.1
TimeSinceStart : 11804.237087488174
Training Loss : 0.3921404480934143
Done logging...




********** Iteration 2661000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2662000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2663000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2664000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2665000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2666000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2667000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2668000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2669000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2670000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2670001
mean reward (100 episodes) 1719.700000
best mean reward 1856.100000
running time 11847.084971
Train_EnvstepsSoFar : 2670001
Train_AverageReturn : 1719.7
Train_BestReturn : 1856.1
TimeSinceStart : 11847.08497095108
Training Loss : 0.35602766275405884
Done logging...




********** Iteration 2671000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2672000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2673000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2674000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2675000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2676000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2677000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2678000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2679000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2680000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2680001
mean reward (100 episodes) 1708.400000
best mean reward 1856.100000
running time 11889.878488
Train_EnvstepsSoFar : 2680001
Train_AverageReturn : 1708.4
Train_BestReturn : 1856.1
TimeSinceStart : 11889.878487586975
Training Loss : 0.47672855854034424
Done logging...




********** Iteration 2681000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2682000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2683000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2684000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2685000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2686000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2687000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2688000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2689000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2690000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2690001
mean reward (100 episodes) 1642.600000
best mean reward 1856.100000
running time 11932.425583
Train_EnvstepsSoFar : 2690001
Train_AverageReturn : 1642.6
Train_BestReturn : 1856.1
TimeSinceStart : 11932.42558312416
Training Loss : 0.35306406021118164
Done logging...




********** Iteration 2691000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2692000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2693000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2694000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2695000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2696000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2697000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2698000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2699000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2700000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2700001
mean reward (100 episodes) 1625.700000
best mean reward 1856.100000
running time 11975.144420
Train_EnvstepsSoFar : 2700001
Train_AverageReturn : 1625.7
Train_BestReturn : 1856.1
TimeSinceStart : 11975.144420146942
Training Loss : 1.6523371934890747
Done logging...




********** Iteration 2701000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2702000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2703000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2704000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2705000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2706000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2707000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2708000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2709000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2710000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2710001
mean reward (100 episodes) 1608.800000
best mean reward 1856.100000
running time 12018.126984
Train_EnvstepsSoFar : 2710001
Train_AverageReturn : 1608.8
Train_BestReturn : 1856.1
TimeSinceStart : 12018.126983880997
Training Loss : 0.29656416177749634
Done logging...




********** Iteration 2711000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2712000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2713000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2714000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2715000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2716000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2717000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2718000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2719000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2720000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2720001
mean reward (100 episodes) 1618.100000
best mean reward 1856.100000
running time 12060.824693
Train_EnvstepsSoFar : 2720001
Train_AverageReturn : 1618.1
Train_BestReturn : 1856.1
TimeSinceStart : 12060.824693441391
Training Loss : 1.263818383216858
Done logging...




********** Iteration 2721000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2722000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2723000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2724000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2725000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2726000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2727000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2728000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2729000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2730000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2730001
mean reward (100 episodes) 1604.500000
best mean reward 1856.100000
running time 12103.661598
Train_EnvstepsSoFar : 2730001
Train_AverageReturn : 1604.5
Train_BestReturn : 1856.1
TimeSinceStart : 12103.661597967148
Training Loss : 0.29905569553375244
Done logging...




********** Iteration 2731000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2732000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2733000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2734000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2735000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2736000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2737000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2738000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2739000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2740000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2740001
mean reward (100 episodes) 1600.100000
best mean reward 1856.100000
running time 12146.444691
Train_EnvstepsSoFar : 2740001
Train_AverageReturn : 1600.1
Train_BestReturn : 1856.1
TimeSinceStart : 12146.444690942764
Training Loss : 0.20077699422836304
Done logging...




********** Iteration 2741000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2742000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2743000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2744000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2745000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2746000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2747000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2748000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2749000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2750000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2750001
mean reward (100 episodes) 1611.600000
best mean reward 1856.100000
running time 12189.206897
Train_EnvstepsSoFar : 2750001
Train_AverageReturn : 1611.6
Train_BestReturn : 1856.1
TimeSinceStart : 12189.206897258759
Training Loss : 0.43951326608657837
Done logging...




********** Iteration 2751000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2752000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2753000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2754000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2755000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2756000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2757000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2758000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2759000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2760000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2760001
mean reward (100 episodes) 1641.700000
best mean reward 1856.100000
running time 12232.133667
Train_EnvstepsSoFar : 2760001
Train_AverageReturn : 1641.7
Train_BestReturn : 1856.1
TimeSinceStart : 12232.133666753769
Training Loss : 0.4979723393917084
Done logging...




********** Iteration 2761000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2762000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2763000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2764000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2765000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2766000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2767000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2768000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2769000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2770000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2770001
mean reward (100 episodes) 1772.100000
best mean reward 1856.100000
running time 12274.902282
Train_EnvstepsSoFar : 2770001
Train_AverageReturn : 1772.1
Train_BestReturn : 1856.1
TimeSinceStart : 12274.90228176117
Training Loss : 0.08324921131134033
Done logging...




********** Iteration 2771000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2772000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2773000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2774000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2775000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2776000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2777000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2778000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2779000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2780000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2780001
mean reward (100 episodes) 1736.500000
best mean reward 1856.100000
running time 12317.597851
Train_EnvstepsSoFar : 2780001
Train_AverageReturn : 1736.5
Train_BestReturn : 1856.1
TimeSinceStart : 12317.597851276398
Training Loss : 2.3792128562927246
Done logging...




********** Iteration 2781000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2782000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2783000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2784000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2785000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2786000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2787000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2788000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2789000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2790000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2790001
mean reward (100 episodes) 1731.400000
best mean reward 1856.100000
running time 12360.334189
Train_EnvstepsSoFar : 2790001
Train_AverageReturn : 1731.4
Train_BestReturn : 1856.1
TimeSinceStart : 12360.33418917656
Training Loss : 0.635066032409668
Done logging...




********** Iteration 2791000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2792000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2793000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2794000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2795000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2796000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2797000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2798000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2799000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2800000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2800001
mean reward (100 episodes) 1626.000000
best mean reward 1856.100000
running time 12402.938980
Train_EnvstepsSoFar : 2800001
Train_AverageReturn : 1626.0
Train_BestReturn : 1856.1
TimeSinceStart : 12402.938980102539
Training Loss : 0.1713465452194214
Done logging...




********** Iteration 2801000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2802000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2803000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2804000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2805000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2806000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2807000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2808000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2809000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2810000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2810001
mean reward (100 episodes) 1667.000000
best mean reward 1856.100000
running time 12445.699061
Train_EnvstepsSoFar : 2810001
Train_AverageReturn : 1667.0
Train_BestReturn : 1856.1
TimeSinceStart : 12445.6990609169
Training Loss : 0.8311487436294556
Done logging...




********** Iteration 2811000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2812000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2813000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2814000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2815000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2816000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2817000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2818000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2819000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2820000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2820001
mean reward (100 episodes) 1790.800000
best mean reward 1856.100000
running time 12488.637179
Train_EnvstepsSoFar : 2820001
Train_AverageReturn : 1790.8
Train_BestReturn : 1856.1
TimeSinceStart : 12488.637178897858
Training Loss : 1.2596988677978516
Done logging...




********** Iteration 2821000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2822000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2823000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2824000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2825000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2826000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2827000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2828000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2829000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2830000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2830001
mean reward (100 episodes) 1814.500000
best mean reward 1856.100000
running time 12531.445295
Train_EnvstepsSoFar : 2830001
Train_AverageReturn : 1814.5
Train_BestReturn : 1856.1
TimeSinceStart : 12531.445294618607
Training Loss : 0.47524842619895935
Done logging...




********** Iteration 2831000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2832000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2833000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2834000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2835000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2836000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2837000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2838000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2839000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2840000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2840001
mean reward (100 episodes) 1723.800000
best mean reward 1856.100000
running time 12574.287979
Train_EnvstepsSoFar : 2840001
Train_AverageReturn : 1723.8
Train_BestReturn : 1856.1
TimeSinceStart : 12574.287979125977
Training Loss : 0.3774730861186981
Done logging...




********** Iteration 2841000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2842000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2843000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2844000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2845000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2846000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2847000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2848000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2849000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2850000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2850001
mean reward (100 episodes) 1668.200000
best mean reward 1856.100000
running time 12616.921739
Train_EnvstepsSoFar : 2850001
Train_AverageReturn : 1668.2
Train_BestReturn : 1856.1
TimeSinceStart : 12616.921739339828
Training Loss : 1.4384474754333496
Done logging...




********** Iteration 2851000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2852000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2853000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2854000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2855000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2856000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2857000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2858000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2859000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2860000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2860001
mean reward (100 episodes) 1684.600000
best mean reward 1856.100000
running time 12659.577716
Train_EnvstepsSoFar : 2860001
Train_AverageReturn : 1684.6
Train_BestReturn : 1856.1
TimeSinceStart : 12659.577716350555
Training Loss : 0.2620622217655182
Done logging...




********** Iteration 2861000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2862000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2863000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2864000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2865000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2866000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2867000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2868000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2869000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2870000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2870001
mean reward (100 episodes) 1649.300000
best mean reward 1856.100000
running time 12702.257371
Train_EnvstepsSoFar : 2870001
Train_AverageReturn : 1649.3
Train_BestReturn : 1856.1
TimeSinceStart : 12702.257371425629
Training Loss : 0.23027023673057556
Done logging...




********** Iteration 2871000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2872000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2873000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2874000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2875000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2876000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2877000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2878000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2879000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2880000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2880001
mean reward (100 episodes) 1616.200000
best mean reward 1856.100000
running time 12744.940683
Train_EnvstepsSoFar : 2880001
Train_AverageReturn : 1616.2
Train_BestReturn : 1856.1
TimeSinceStart : 12744.94068312645
Training Loss : 0.09860048443078995
Done logging...




********** Iteration 2881000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2882000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2883000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2884000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2885000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2886000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2887000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2888000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2889000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2890000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2890001
mean reward (100 episodes) 1612.700000
best mean reward 1856.100000
running time 12787.466158
Train_EnvstepsSoFar : 2890001
Train_AverageReturn : 1612.7
Train_BestReturn : 1856.1
TimeSinceStart : 12787.46615767479
Training Loss : 0.3425108790397644
Done logging...




********** Iteration 2891000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2892000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2893000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2894000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2895000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2896000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2897000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2898000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2899000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2900000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2900001
mean reward (100 episodes) 1618.900000
best mean reward 1856.100000
running time 12830.159550
Train_EnvstepsSoFar : 2900001
Train_AverageReturn : 1618.9
Train_BestReturn : 1856.1
TimeSinceStart : 12830.15955042839
Training Loss : 0.2533133625984192
Done logging...




********** Iteration 2901000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2902000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2903000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2904000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2905000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2906000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2907000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2908000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2909000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2910000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2910001
mean reward (100 episodes) 1618.900000
best mean reward 1856.100000
running time 12872.997319
Train_EnvstepsSoFar : 2910001
Train_AverageReturn : 1618.9
Train_BestReturn : 1856.1
TimeSinceStart : 12872.997318983078
Training Loss : 0.3535239100456238
Done logging...




********** Iteration 2911000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2912000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2913000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2914000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2915000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2916000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2917000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2918000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2919000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2920000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2920001
mean reward (100 episodes) 1637.400000
best mean reward 1856.100000
running time 12915.798271
Train_EnvstepsSoFar : 2920001
Train_AverageReturn : 1637.4
Train_BestReturn : 1856.1
TimeSinceStart : 12915.79827094078
Training Loss : 1.3557605743408203
Done logging...




********** Iteration 2921000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2922000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2923000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2924000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2925000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2926000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2927000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2928000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2929000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2930000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2930001
mean reward (100 episodes) 1688.500000
best mean reward 1856.100000
running time 12958.449140
Train_EnvstepsSoFar : 2930001
Train_AverageReturn : 1688.5
Train_BestReturn : 1856.1
TimeSinceStart : 12958.44913983345
Training Loss : 1.3678817749023438
Done logging...




********** Iteration 2931000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2932000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2933000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2934000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2935000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2936000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2937000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2938000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2939000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2940000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2940001
mean reward (100 episodes) 1682.000000
best mean reward 1856.100000
running time 13001.150679
Train_EnvstepsSoFar : 2940001
Train_AverageReturn : 1682.0
Train_BestReturn : 1856.1
TimeSinceStart : 13001.150678873062
Training Loss : 0.2805873453617096
Done logging...




********** Iteration 2941000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2942000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2943000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2944000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2945000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2946000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2947000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2948000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2949000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2950000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2950001
mean reward (100 episodes) 1704.400000
best mean reward 1856.100000
running time 13043.777002
Train_EnvstepsSoFar : 2950001
Train_AverageReturn : 1704.4
Train_BestReturn : 1856.1
TimeSinceStart : 13043.777002334595
Training Loss : 0.40191739797592163
Done logging...




********** Iteration 2951000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2952000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2953000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2954000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2955000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2956000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2957000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2958000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2959000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2960000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2960001
mean reward (100 episodes) 1630.600000
best mean reward 1856.100000
running time 13086.494382
Train_EnvstepsSoFar : 2960001
Train_AverageReturn : 1630.6
Train_BestReturn : 1856.1
TimeSinceStart : 13086.494381904602
Training Loss : 0.18012917041778564
Done logging...




********** Iteration 2961000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2962000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2963000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2964000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2965000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2966000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2967000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2968000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2969000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2970000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2970001
mean reward (100 episodes) 1685.500000
best mean reward 1856.100000
running time 13129.059446
Train_EnvstepsSoFar : 2970001
Train_AverageReturn : 1685.5
Train_BestReturn : 1856.1
TimeSinceStart : 13129.05944609642
Training Loss : 0.37368249893188477
Done logging...




********** Iteration 2971000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2972000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2973000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2974000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2975000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2976000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2977000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2978000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2979000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2980000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2980001
mean reward (100 episodes) 1689.500000
best mean reward 1856.100000
running time 13171.876985
Train_EnvstepsSoFar : 2980001
Train_AverageReturn : 1689.5
Train_BestReturn : 1856.1
TimeSinceStart : 13171.876985311508
Training Loss : 0.6644439697265625
Done logging...




********** Iteration 2981000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2982000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2983000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2984000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2985000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2986000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2987000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2988000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2989000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2990000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 2990001
mean reward (100 episodes) 1684.300000
best mean reward 1856.100000
running time 13214.410625
Train_EnvstepsSoFar : 2990001
Train_AverageReturn : 1684.3
Train_BestReturn : 1856.1
TimeSinceStart : 13214.410624742508
Training Loss : 0.3085387349128723
Done logging...




********** Iteration 2991000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2992000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2993000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2994000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2995000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2996000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2997000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2998000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 2999000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3000000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3000001
mean reward (100 episodes) 1597.800000
best mean reward 1856.100000
running time 13260.709870
Train_EnvstepsSoFar : 3000001
Train_AverageReturn : 1597.8
Train_BestReturn : 1856.1
TimeSinceStart : 13260.709870100021
Training Loss : 0.6141192317008972
Done logging...




********** Iteration 3001000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3002000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3003000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3004000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3005000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3006000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3007000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3008000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3009000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3010000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3010001
mean reward (100 episodes) 1557.700000
best mean reward 1856.100000
running time 13303.179336
Train_EnvstepsSoFar : 3010001
Train_AverageReturn : 1557.7
Train_BestReturn : 1856.1
TimeSinceStart : 13303.179336309433
Training Loss : 0.5369626879692078
Done logging...




********** Iteration 3011000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3012000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3013000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3014000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3015000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3016000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3017000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3018000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3019000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3020000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3020001
mean reward (100 episodes) 1556.600000
best mean reward 1856.100000
running time 13345.724830
Train_EnvstepsSoFar : 3020001
Train_AverageReturn : 1556.6
Train_BestReturn : 1856.1
TimeSinceStart : 13345.724830389023
Training Loss : 0.26161307096481323
Done logging...




********** Iteration 3021000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3022000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3023000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3024000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3025000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3026000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3027000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3028000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3029000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3030000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3030001
mean reward (100 episodes) 1591.800000
best mean reward 1856.100000
running time 13388.230098
Train_EnvstepsSoFar : 3030001
Train_AverageReturn : 1591.8
Train_BestReturn : 1856.1
TimeSinceStart : 13388.230097532272
Training Loss : 0.14726975560188293
Done logging...




********** Iteration 3031000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3032000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3033000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3034000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3035000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3036000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3037000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3038000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3039000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3040000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3040001
mean reward (100 episodes) 1593.100000
best mean reward 1856.100000
running time 13431.141807
Train_EnvstepsSoFar : 3040001
Train_AverageReturn : 1593.1
Train_BestReturn : 1856.1
TimeSinceStart : 13431.141806602478
Training Loss : 0.18693102896213531
Done logging...




********** Iteration 3041000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3042000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3043000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3044000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3045000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3046000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3047000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3048000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3049000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3050000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3050001
mean reward (100 episodes) 1619.200000
best mean reward 1856.100000
running time 13473.944259
Train_EnvstepsSoFar : 3050001
Train_AverageReturn : 1619.2
Train_BestReturn : 1856.1
TimeSinceStart : 13473.94425868988
Training Loss : 0.17075087130069733
Done logging...




********** Iteration 3051000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3052000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3053000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3054000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3055000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3056000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3057000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3058000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3059000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3060000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3060001
mean reward (100 episodes) 1637.400000
best mean reward 1856.100000
running time 13516.624338
Train_EnvstepsSoFar : 3060001
Train_AverageReturn : 1637.4
Train_BestReturn : 1856.1
TimeSinceStart : 13516.624337673187
Training Loss : 0.2528780698776245
Done logging...




********** Iteration 3061000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3062000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3063000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3064000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3065000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3066000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3067000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3068000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3069000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3070000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3070001
mean reward (100 episodes) 1676.000000
best mean reward 1856.100000
running time 13559.341472
Train_EnvstepsSoFar : 3070001
Train_AverageReturn : 1676.0
Train_BestReturn : 1856.1
TimeSinceStart : 13559.341471672058
Training Loss : 1.3941898345947266
Done logging...




********** Iteration 3071000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3072000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3073000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3074000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3075000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3076000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3077000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3078000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3079000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3080000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3080001
mean reward (100 episodes) 1703.300000
best mean reward 1856.100000
running time 13602.142404
Train_EnvstepsSoFar : 3080001
Train_AverageReturn : 1703.3
Train_BestReturn : 1856.1
TimeSinceStart : 13602.142404317856
Training Loss : 0.12170170247554779
Done logging...




********** Iteration 3081000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3082000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3083000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3084000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3085000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3086000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3087000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3088000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3089000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3090000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3090001
mean reward (100 episodes) 1775.700000
best mean reward 1856.100000
running time 13644.826700
Train_EnvstepsSoFar : 3090001
Train_AverageReturn : 1775.7
Train_BestReturn : 1856.1
TimeSinceStart : 13644.826699733734
Training Loss : 0.39545515179634094
Done logging...




********** Iteration 3091000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3092000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3093000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3094000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3095000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3096000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3097000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3098000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3099000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3100001
mean reward (100 episodes) 1763.000000
best mean reward 1856.100000
running time 13687.706007
Train_EnvstepsSoFar : 3100001
Train_AverageReturn : 1763.0
Train_BestReturn : 1856.1
TimeSinceStart : 13687.706007242203
Training Loss : 0.33643943071365356
Done logging...




********** Iteration 3101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3110001
mean reward (100 episodes) 1668.000000
best mean reward 1856.100000
running time 13730.547540
Train_EnvstepsSoFar : 3110001
Train_AverageReturn : 1668.0
Train_BestReturn : 1856.1
TimeSinceStart : 13730.547540187836
Training Loss : 0.9668980240821838
Done logging...




********** Iteration 3111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3120001
mean reward (100 episodes) 1593.100000
best mean reward 1856.100000
running time 13773.138111
Train_EnvstepsSoFar : 3120001
Train_AverageReturn : 1593.1
Train_BestReturn : 1856.1
TimeSinceStart : 13773.13811135292
Training Loss : 0.2447066605091095
Done logging...




********** Iteration 3121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3130001
mean reward (100 episodes) 1643.900000
best mean reward 1856.100000
running time 13815.797152
Train_EnvstepsSoFar : 3130001
Train_AverageReturn : 1643.9
Train_BestReturn : 1856.1
TimeSinceStart : 13815.797152280807
Training Loss : 0.154495969414711
Done logging...




********** Iteration 3131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3140001
mean reward (100 episodes) 1656.600000
best mean reward 1856.100000
running time 13858.612697
Train_EnvstepsSoFar : 3140001
Train_AverageReturn : 1656.6
Train_BestReturn : 1856.1
TimeSinceStart : 13858.612697124481
Training Loss : 0.5590842962265015
Done logging...




********** Iteration 3141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3150001
mean reward (100 episodes) 1672.000000
best mean reward 1856.100000
running time 13901.360444
Train_EnvstepsSoFar : 3150001
Train_AverageReturn : 1672.0
Train_BestReturn : 1856.1
TimeSinceStart : 13901.360444068909
Training Loss : 0.12680655717849731
Done logging...




********** Iteration 3151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3160001
mean reward (100 episodes) 1659.700000
best mean reward 1856.100000
running time 13944.278662
Train_EnvstepsSoFar : 3160001
Train_AverageReturn : 1659.7
Train_BestReturn : 1856.1
TimeSinceStart : 13944.278661966324
Training Loss : 0.3142528533935547
Done logging...




********** Iteration 3161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3170001
mean reward (100 episodes) 1654.400000
best mean reward 1856.100000
running time 13987.131927
Train_EnvstepsSoFar : 3170001
Train_AverageReturn : 1654.4
Train_BestReturn : 1856.1
TimeSinceStart : 13987.131927251816
Training Loss : 0.33864691853523254
Done logging...




********** Iteration 3171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3180001
mean reward (100 episodes) 1603.500000
best mean reward 1856.100000
running time 14030.232051
Train_EnvstepsSoFar : 3180001
Train_AverageReturn : 1603.5
Train_BestReturn : 1856.1
TimeSinceStart : 14030.232050657272
Training Loss : 1.7671387195587158
Done logging...




********** Iteration 3181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3190001
mean reward (100 episodes) 1615.200000
best mean reward 1856.100000
running time 14073.211172
Train_EnvstepsSoFar : 3190001
Train_AverageReturn : 1615.2
Train_BestReturn : 1856.1
TimeSinceStart : 14073.2111723423
Training Loss : 0.2956748604774475
Done logging...




********** Iteration 3191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3200001
mean reward (100 episodes) 1614.500000
best mean reward 1856.100000
running time 14116.326456
Train_EnvstepsSoFar : 3200001
Train_AverageReturn : 1614.5
Train_BestReturn : 1856.1
TimeSinceStart : 14116.32645559311
Training Loss : 0.23286771774291992
Done logging...




********** Iteration 3201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3210001
mean reward (100 episodes) 1699.700000
best mean reward 1856.100000
running time 14159.505758
Train_EnvstepsSoFar : 3210001
Train_AverageReturn : 1699.7
Train_BestReturn : 1856.1
TimeSinceStart : 14159.505757570267
Training Loss : 0.2538895308971405
Done logging...




********** Iteration 3211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3220001
mean reward (100 episodes) 1656.400000
best mean reward 1856.100000
running time 14202.466632
Train_EnvstepsSoFar : 3220001
Train_AverageReturn : 1656.4
Train_BestReturn : 1856.1
TimeSinceStart : 14202.466631889343
Training Loss : 1.929832935333252
Done logging...




********** Iteration 3221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3230001
mean reward (100 episodes) 1725.300000
best mean reward 1856.100000
running time 14245.377087
Train_EnvstepsSoFar : 3230001
Train_AverageReturn : 1725.3
Train_BestReturn : 1856.1
TimeSinceStart : 14245.377087116241
Training Loss : 0.21708250045776367
Done logging...




********** Iteration 3231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3240001
mean reward (100 episodes) 1678.700000
best mean reward 1856.100000
running time 14288.538950
Train_EnvstepsSoFar : 3240001
Train_AverageReturn : 1678.7
Train_BestReturn : 1856.1
TimeSinceStart : 14288.53895020485
Training Loss : 0.25081560015678406
Done logging...




********** Iteration 3241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3250001
mean reward (100 episodes) 1727.200000
best mean reward 1856.100000
running time 14331.763560
Train_EnvstepsSoFar : 3250001
Train_AverageReturn : 1727.2
Train_BestReturn : 1856.1
TimeSinceStart : 14331.763560056686
Training Loss : 0.4891946315765381
Done logging...




********** Iteration 3251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3260001
mean reward (100 episodes) 1689.700000
best mean reward 1856.100000
running time 14374.760589
Train_EnvstepsSoFar : 3260001
Train_AverageReturn : 1689.7
Train_BestReturn : 1856.1
TimeSinceStart : 14374.760588884354
Training Loss : 0.22106900811195374
Done logging...




********** Iteration 3261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3270001
mean reward (100 episodes) 1612.500000
best mean reward 1856.100000
running time 14417.824336
Train_EnvstepsSoFar : 3270001
Train_AverageReturn : 1612.5
Train_BestReturn : 1856.1
TimeSinceStart : 14417.824335813522
Training Loss : 1.6219749450683594
Done logging...




********** Iteration 3271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3280001
mean reward (100 episodes) 1624.700000
best mean reward 1856.100000
running time 14460.941631
Train_EnvstepsSoFar : 3280001
Train_AverageReturn : 1624.7
Train_BestReturn : 1856.1
TimeSinceStart : 14460.941631317139
Training Loss : 0.5015897750854492
Done logging...




********** Iteration 3281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3290001
mean reward (100 episodes) 1577.600000
best mean reward 1856.100000
running time 14504.141587
Train_EnvstepsSoFar : 3290001
Train_AverageReturn : 1577.6
Train_BestReturn : 1856.1
TimeSinceStart : 14504.141586780548
Training Loss : 0.9753096699714661
Done logging...




********** Iteration 3291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3300001
mean reward (100 episodes) 1608.900000
best mean reward 1856.100000
running time 14547.355471
Train_EnvstepsSoFar : 3300001
Train_AverageReturn : 1608.9
Train_BestReturn : 1856.1
TimeSinceStart : 14547.355471372604
Training Loss : 0.18549588322639465
Done logging...




********** Iteration 3301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3310001
mean reward (100 episodes) 1616.000000
best mean reward 1856.100000
running time 14590.635043
Train_EnvstepsSoFar : 3310001
Train_AverageReturn : 1616.0
Train_BestReturn : 1856.1
TimeSinceStart : 14590.635043382645
Training Loss : 0.2866289019584656
Done logging...




********** Iteration 3311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3320001
mean reward (100 episodes) 1624.600000
best mean reward 1856.100000
running time 14633.810836
Train_EnvstepsSoFar : 3320001
Train_AverageReturn : 1624.6
Train_BestReturn : 1856.1
TimeSinceStart : 14633.810835838318
Training Loss : 1.2573556900024414
Done logging...




********** Iteration 3321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3330001
mean reward (100 episodes) 1675.000000
best mean reward 1856.100000
running time 14677.230893
Train_EnvstepsSoFar : 3330001
Train_AverageReturn : 1675.0
Train_BestReturn : 1856.1
TimeSinceStart : 14677.230892658234
Training Loss : 0.16874179244041443
Done logging...




********** Iteration 3331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3340001
mean reward (100 episodes) 1647.800000
best mean reward 1856.100000
running time 14720.439990
Train_EnvstepsSoFar : 3340001
Train_AverageReturn : 1647.8
Train_BestReturn : 1856.1
TimeSinceStart : 14720.439989566803
Training Loss : 0.15846309065818787
Done logging...




********** Iteration 3341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3350001
mean reward (100 episodes) 1670.000000
best mean reward 1856.100000
running time 14763.645835
Train_EnvstepsSoFar : 3350001
Train_AverageReturn : 1670.0
Train_BestReturn : 1856.1
TimeSinceStart : 14763.645834684372
Training Loss : 0.17564795911312103
Done logging...




********** Iteration 3351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3360001
mean reward (100 episodes) 1676.500000
best mean reward 1856.100000
running time 14806.963195
Train_EnvstepsSoFar : 3360001
Train_AverageReturn : 1676.5
Train_BestReturn : 1856.1
TimeSinceStart : 14806.963195085526
Training Loss : 0.2273114025592804
Done logging...




********** Iteration 3361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3370001
mean reward (100 episodes) 1729.100000
best mean reward 1856.100000
running time 14850.185750
Train_EnvstepsSoFar : 3370001
Train_AverageReturn : 1729.1
Train_BestReturn : 1856.1
TimeSinceStart : 14850.185749530792
Training Loss : 0.15835703909397125
Done logging...




********** Iteration 3371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3380001
mean reward (100 episodes) 1772.400000
best mean reward 1856.100000
running time 14893.321739
Train_EnvstepsSoFar : 3380001
Train_AverageReturn : 1772.4
Train_BestReturn : 1856.1
TimeSinceStart : 14893.32173871994
Training Loss : 2.149031639099121
Done logging...




********** Iteration 3381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3390001
mean reward (100 episodes) 1788.400000
best mean reward 1856.100000
running time 14936.589396
Train_EnvstepsSoFar : 3390001
Train_AverageReturn : 1788.4
Train_BestReturn : 1856.1
TimeSinceStart : 14936.58939576149
Training Loss : 0.41732126474380493
Done logging...




********** Iteration 3391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3400001
mean reward (100 episodes) 1751.400000
best mean reward 1856.100000
running time 14979.925855
Train_EnvstepsSoFar : 3400001
Train_AverageReturn : 1751.4
Train_BestReturn : 1856.1
TimeSinceStart : 14979.925854682922
Training Loss : 0.6216341853141785
Done logging...




********** Iteration 3401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3410001
mean reward (100 episodes) 1786.900000
best mean reward 1856.100000
running time 15023.221489
Train_EnvstepsSoFar : 3410001
Train_AverageReturn : 1786.9
Train_BestReturn : 1856.1
TimeSinceStart : 15023.221488952637
Training Loss : 0.14373832941055298
Done logging...




********** Iteration 3411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3420001
mean reward (100 episodes) 1752.800000
best mean reward 1856.100000
running time 15066.512311
Train_EnvstepsSoFar : 3420001
Train_AverageReturn : 1752.8
Train_BestReturn : 1856.1
TimeSinceStart : 15066.512311220169
Training Loss : 0.9426637887954712
Done logging...




********** Iteration 3421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3430001
mean reward (100 episodes) 1801.000000
best mean reward 1856.100000
running time 15109.554344
Train_EnvstepsSoFar : 3430001
Train_AverageReturn : 1801.0
Train_BestReturn : 1856.1
TimeSinceStart : 15109.554343700409
Training Loss : 0.23744729161262512
Done logging...




********** Iteration 3431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3440001
mean reward (100 episodes) 1736.600000
best mean reward 1856.100000
running time 15152.694644
Train_EnvstepsSoFar : 3440001
Train_AverageReturn : 1736.6
Train_BestReturn : 1856.1
TimeSinceStart : 15152.694644212723
Training Loss : 1.3245079517364502
Done logging...




********** Iteration 3441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3450001
mean reward (100 episodes) 1750.200000
best mean reward 1856.100000
running time 15196.100252
Train_EnvstepsSoFar : 3450001
Train_AverageReturn : 1750.2
Train_BestReturn : 1856.1
TimeSinceStart : 15196.10025191307
Training Loss : 0.14299584925174713
Done logging...




********** Iteration 3451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3460001
mean reward (100 episodes) 1668.200000
best mean reward 1856.100000
running time 15239.571733
Train_EnvstepsSoFar : 3460001
Train_AverageReturn : 1668.2
Train_BestReturn : 1856.1
TimeSinceStart : 15239.571732759476
Training Loss : 0.40674224495887756
Done logging...




********** Iteration 3461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3470001
mean reward (100 episodes) 1660.600000
best mean reward 1856.100000
running time 15282.980764
Train_EnvstepsSoFar : 3470001
Train_AverageReturn : 1660.6
Train_BestReturn : 1856.1
TimeSinceStart : 15282.980763912201
Training Loss : 0.5873473882675171
Done logging...




********** Iteration 3471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3480001
mean reward (100 episodes) 1640.900000
best mean reward 1856.100000
running time 15326.494455
Train_EnvstepsSoFar : 3480001
Train_AverageReturn : 1640.9
Train_BestReturn : 1856.1
TimeSinceStart : 15326.494454622269
Training Loss : 1.9352436065673828
Done logging...




********** Iteration 3481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3490001
mean reward (100 episodes) 1747.000000
best mean reward 1856.100000
running time 15369.979653
Train_EnvstepsSoFar : 3490001
Train_AverageReturn : 1747.0
Train_BestReturn : 1856.1
TimeSinceStart : 15369.97965335846
Training Loss : 0.2581045925617218
Done logging...




********** Iteration 3491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3499000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3500000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3500001
mean reward (100 episodes) 1774.400000
best mean reward 1856.100000
running time 15415.641247
Train_EnvstepsSoFar : 3500001
Train_AverageReturn : 1774.4
Train_BestReturn : 1856.1
TimeSinceStart : 15415.641247034073
Training Loss : 0.3576448857784271
Done logging...




********** Iteration 3501000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3502000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3503000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3504000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3505000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3506000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3507000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3508000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3509000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3510000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3510001
mean reward (100 episodes) 1714.200000
best mean reward 1856.100000
running time 15458.752783
Train_EnvstepsSoFar : 3510001
Train_AverageReturn : 1714.2
Train_BestReturn : 1856.1
TimeSinceStart : 15458.752783298492
Training Loss : 0.6363521218299866
Done logging...




********** Iteration 3511000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3512000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3513000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3514000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3515000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3516000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3517000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3518000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3519000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3520000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3520001
mean reward (100 episodes) 1648.900000
best mean reward 1856.100000
running time 15502.281451
Train_EnvstepsSoFar : 3520001
Train_AverageReturn : 1648.9
Train_BestReturn : 1856.1
TimeSinceStart : 15502.281450510025
Training Loss : 0.14318326115608215
Done logging...




********** Iteration 3521000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3522000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3523000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3524000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3525000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3526000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3527000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3528000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3529000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3530000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3530001
mean reward (100 episodes) 1671.500000
best mean reward 1856.100000
running time 15545.583999
Train_EnvstepsSoFar : 3530001
Train_AverageReturn : 1671.5
Train_BestReturn : 1856.1
TimeSinceStart : 15545.583999156952
Training Loss : 2.1359567642211914
Done logging...




********** Iteration 3531000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3532000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3533000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3534000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3535000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3536000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3537000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3538000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3539000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3540000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3540001
mean reward (100 episodes) 1645.300000
best mean reward 1856.100000
running time 15589.008747
Train_EnvstepsSoFar : 3540001
Train_AverageReturn : 1645.3
Train_BestReturn : 1856.1
TimeSinceStart : 15589.008746862411
Training Loss : 1.3367549180984497
Done logging...




********** Iteration 3541000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3542000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3543000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3544000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3545000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3546000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3547000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3548000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3549000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3550000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3550001
mean reward (100 episodes) 1705.700000
best mean reward 1856.100000
running time 15632.461128
Train_EnvstepsSoFar : 3550001
Train_AverageReturn : 1705.7
Train_BestReturn : 1856.1
TimeSinceStart : 15632.461127758026
Training Loss : 0.6861755847930908
Done logging...




********** Iteration 3551000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3552000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3553000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3554000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3555000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3556000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3557000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3558000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3559000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3560000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3560001
mean reward (100 episodes) 1635.900000
best mean reward 1856.100000
running time 15675.819593
Train_EnvstepsSoFar : 3560001
Train_AverageReturn : 1635.9
Train_BestReturn : 1856.1
TimeSinceStart : 15675.819592952728
Training Loss : 0.3059207797050476
Done logging...




********** Iteration 3561000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3562000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3563000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3564000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3565000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3566000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3567000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3568000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3569000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3570000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3570001
mean reward (100 episodes) 1657.900000
best mean reward 1856.100000
running time 15719.535691
Train_EnvstepsSoFar : 3570001
Train_AverageReturn : 1657.9
Train_BestReturn : 1856.1
TimeSinceStart : 15719.535690546036
Training Loss : 0.6235334873199463
Done logging...




********** Iteration 3571000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3572000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3573000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3574000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3575000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3576000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3577000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3578000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3579000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3580000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3580001
mean reward (100 episodes) 1639.000000
best mean reward 1856.100000
running time 15763.369980
Train_EnvstepsSoFar : 3580001
Train_AverageReturn : 1639.0
Train_BestReturn : 1856.1
TimeSinceStart : 15763.369980096817
Training Loss : 0.22200505435466766
Done logging...




********** Iteration 3581000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3582000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3583000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3584000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3585000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3586000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3587000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3588000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3589000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3590000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3590001
mean reward (100 episodes) 1737.700000
best mean reward 1856.100000
running time 15806.825051
Train_EnvstepsSoFar : 3590001
Train_AverageReturn : 1737.7
Train_BestReturn : 1856.1
TimeSinceStart : 15806.82505106926
Training Loss : 0.21003256738185883
Done logging...




********** Iteration 3591000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3592000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3593000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3594000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3595000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3596000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3597000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3598000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3599000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3600000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3600001
mean reward (100 episodes) 1700.200000
best mean reward 1856.100000
running time 15850.299579
Train_EnvstepsSoFar : 3600001
Train_AverageReturn : 1700.2
Train_BestReturn : 1856.1
TimeSinceStart : 15850.299578666687
Training Loss : 0.24998082220554352
Done logging...




********** Iteration 3601000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3602000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3603000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3604000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3605000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3606000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3607000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3608000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3609000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3610000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3610001
mean reward (100 episodes) 1678.300000
best mean reward 1856.100000
running time 15893.746183
Train_EnvstepsSoFar : 3610001
Train_AverageReturn : 1678.3
Train_BestReturn : 1856.1
TimeSinceStart : 15893.746183395386
Training Loss : 0.23468317091464996
Done logging...




********** Iteration 3611000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3612000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3613000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3614000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3615000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3616000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3617000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3618000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3619000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3620000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3620001
mean reward (100 episodes) 1651.900000
best mean reward 1856.100000
running time 15937.132879
Train_EnvstepsSoFar : 3620001
Train_AverageReturn : 1651.9
Train_BestReturn : 1856.1
TimeSinceStart : 15937.132879018784
Training Loss : 0.22478526830673218
Done logging...




********** Iteration 3621000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3622000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3623000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3624000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3625000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3626000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3627000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3628000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3629000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3630000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3630001
mean reward (100 episodes) 1730.600000
best mean reward 1856.100000
running time 15980.647287
Train_EnvstepsSoFar : 3630001
Train_AverageReturn : 1730.6
Train_BestReturn : 1856.1
TimeSinceStart : 15980.647287368774
Training Loss : 0.1689644157886505
Done logging...




********** Iteration 3631000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3632000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3633000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3634000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3635000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3636000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3637000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3638000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3639000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3640000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3640001
mean reward (100 episodes) 1674.200000
best mean reward 1856.100000
running time 16024.147091
Train_EnvstepsSoFar : 3640001
Train_AverageReturn : 1674.2
Train_BestReturn : 1856.1
TimeSinceStart : 16024.147090911865
Training Loss : 1.0400711297988892
Done logging...




********** Iteration 3641000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3642000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3643000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3644000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3645000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3646000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3647000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3648000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3649000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3650000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3650001
mean reward (100 episodes) 1659.100000
best mean reward 1856.100000
running time 16067.486360
Train_EnvstepsSoFar : 3650001
Train_AverageReturn : 1659.1
Train_BestReturn : 1856.1
TimeSinceStart : 16067.486360311508
Training Loss : 0.24237574636936188
Done logging...




********** Iteration 3651000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3652000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3653000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3654000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3655000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3656000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3657000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3658000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3659000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3660000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3660001
mean reward (100 episodes) 1673.000000
best mean reward 1856.100000
running time 16111.055765
Train_EnvstepsSoFar : 3660001
Train_AverageReturn : 1673.0
Train_BestReturn : 1856.1
TimeSinceStart : 16111.055764913559
Training Loss : 0.17459142208099365
Done logging...




********** Iteration 3661000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3662000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3663000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3664000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3665000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3666000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3667000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3668000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3669000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3670000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3670001
mean reward (100 episodes) 1723.300000
best mean reward 1856.100000
running time 16154.649921
Train_EnvstepsSoFar : 3670001
Train_AverageReturn : 1723.3
Train_BestReturn : 1856.1
TimeSinceStart : 16154.649921417236
Training Loss : 0.6846481561660767
Done logging...




********** Iteration 3671000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3672000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3673000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3674000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3675000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3676000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3677000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3678000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3679000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3680000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3680001
mean reward (100 episodes) 1729.400000
best mean reward 1856.100000
running time 16198.182319
Train_EnvstepsSoFar : 3680001
Train_AverageReturn : 1729.4
Train_BestReturn : 1856.1
TimeSinceStart : 16198.182319402695
Training Loss : 0.5112271308898926
Done logging...




********** Iteration 3681000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3682000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3683000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3684000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3685000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3686000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3687000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3688000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3689000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3690000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3690001
mean reward (100 episodes) 1688.300000
best mean reward 1856.100000
running time 16241.796264
Train_EnvstepsSoFar : 3690001
Train_AverageReturn : 1688.3
Train_BestReturn : 1856.1
TimeSinceStart : 16241.796263694763
Training Loss : 0.9830593466758728
Done logging...




********** Iteration 3691000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3692000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3693000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3694000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3695000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3696000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3697000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3698000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3699000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3700000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3700001
mean reward (100 episodes) 1654.500000
best mean reward 1856.100000
running time 16285.258058
Train_EnvstepsSoFar : 3700001
Train_AverageReturn : 1654.5
Train_BestReturn : 1856.1
TimeSinceStart : 16285.258057832718
Training Loss : 2.3944897651672363
Done logging...




********** Iteration 3701000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3702000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3703000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3704000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3705000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3706000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3707000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3708000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3709000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3710000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3710001
mean reward (100 episodes) 1629.600000
best mean reward 1856.100000
running time 16328.909898
Train_EnvstepsSoFar : 3710001
Train_AverageReturn : 1629.6
Train_BestReturn : 1856.1
TimeSinceStart : 16328.90989780426
Training Loss : 0.20825806260108948
Done logging...




********** Iteration 3711000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3712000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3713000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3714000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3715000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3716000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3717000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3718000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3719000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3720000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3720001
mean reward (100 episodes) 1667.400000
best mean reward 1856.100000
running time 16372.519465
Train_EnvstepsSoFar : 3720001
Train_AverageReturn : 1667.4
Train_BestReturn : 1856.1
TimeSinceStart : 16372.519465446472
Training Loss : 1.3512898683547974
Done logging...




********** Iteration 3721000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3722000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3723000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3724000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3725000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3726000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3727000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3728000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3729000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3730000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3730001
mean reward (100 episodes) 1697.400000
best mean reward 1856.100000
running time 16415.963221
Train_EnvstepsSoFar : 3730001
Train_AverageReturn : 1697.4
Train_BestReturn : 1856.1
TimeSinceStart : 16415.96322107315
Training Loss : 0.23302827775478363
Done logging...




********** Iteration 3731000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3732000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3733000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3734000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3735000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3736000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3737000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3738000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3739000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3740000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3740001
mean reward (100 episodes) 1685.900000
best mean reward 1856.100000
running time 16459.427176
Train_EnvstepsSoFar : 3740001
Train_AverageReturn : 1685.9
Train_BestReturn : 1856.1
TimeSinceStart : 16459.42717552185
Training Loss : 0.1976931393146515
Done logging...




********** Iteration 3741000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3742000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3743000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3744000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3745000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3746000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3747000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3748000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3749000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3750000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3750001
mean reward (100 episodes) 1728.900000
best mean reward 1856.100000
running time 16502.746475
Train_EnvstepsSoFar : 3750001
Train_AverageReturn : 1728.9
Train_BestReturn : 1856.1
TimeSinceStart : 16502.746475219727
Training Loss : 1.2935527563095093
Done logging...




********** Iteration 3751000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3752000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3753000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3754000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3755000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3756000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3757000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3758000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3759000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3760000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3760001
mean reward (100 episodes) 1698.200000
best mean reward 1856.100000
running time 16546.277575
Train_EnvstepsSoFar : 3760001
Train_AverageReturn : 1698.2
Train_BestReturn : 1856.1
TimeSinceStart : 16546.27757525444
Training Loss : 0.10494275391101837
Done logging...




********** Iteration 3761000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3762000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3763000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3764000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3765000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3766000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3767000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3768000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3769000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3770000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3770001
mean reward (100 episodes) 1721.600000
best mean reward 1856.100000
running time 16589.770734
Train_EnvstepsSoFar : 3770001
Train_AverageReturn : 1721.6
Train_BestReturn : 1856.1
TimeSinceStart : 16589.770733833313
Training Loss : 0.3238040804862976
Done logging...




********** Iteration 3771000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3772000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3773000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3774000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3775000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3776000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3777000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3778000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3779000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3780000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3780001
mean reward (100 episodes) 1681.100000
best mean reward 1856.100000
running time 16633.211084
Train_EnvstepsSoFar : 3780001
Train_AverageReturn : 1681.1
Train_BestReturn : 1856.1
TimeSinceStart : 16633.211084127426
Training Loss : 1.3230564594268799
Done logging...




********** Iteration 3781000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3782000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3783000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3784000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3785000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3786000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3787000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3788000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3789000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3790000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3790001
mean reward (100 episodes) 1683.300000
best mean reward 1856.100000
running time 16676.566034
Train_EnvstepsSoFar : 3790001
Train_AverageReturn : 1683.3
Train_BestReturn : 1856.1
TimeSinceStart : 16676.566034317017
Training Loss : 0.5848428010940552
Done logging...




********** Iteration 3791000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3792000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3793000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3794000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3795000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3796000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3797000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3798000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3799000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3800000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3800001
mean reward (100 episodes) 1707.200000
best mean reward 1856.100000
running time 16720.001942
Train_EnvstepsSoFar : 3800001
Train_AverageReturn : 1707.2
Train_BestReturn : 1856.1
TimeSinceStart : 16720.001942396164
Training Loss : 0.28915756940841675
Done logging...




********** Iteration 3801000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3802000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3803000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3804000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3805000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3806000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3807000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3808000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3809000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3810000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3810001
mean reward (100 episodes) 1683.000000
best mean reward 1856.100000
running time 16763.385381
Train_EnvstepsSoFar : 3810001
Train_AverageReturn : 1683.0
Train_BestReturn : 1856.1
TimeSinceStart : 16763.38538122177
Training Loss : 1.8842356204986572
Done logging...




********** Iteration 3811000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3812000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3813000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3814000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3815000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3816000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3817000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3818000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3819000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3820000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3820001
mean reward (100 episodes) 1630.900000
best mean reward 1856.100000
running time 16806.944398
Train_EnvstepsSoFar : 3820001
Train_AverageReturn : 1630.9
Train_BestReturn : 1856.1
TimeSinceStart : 16806.94439816475
Training Loss : 0.18460099399089813
Done logging...




********** Iteration 3821000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3822000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3823000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3824000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3825000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3826000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3827000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3828000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3829000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3830000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3830001
mean reward (100 episodes) 1564.200000
best mean reward 1856.100000
running time 16850.446573
Train_EnvstepsSoFar : 3830001
Train_AverageReturn : 1564.2
Train_BestReturn : 1856.1
TimeSinceStart : 16850.446573495865
Training Loss : 1.2671527862548828
Done logging...




********** Iteration 3831000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3832000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3833000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3834000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3835000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3836000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3837000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3838000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3839000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3840000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3840001
mean reward (100 episodes) 1588.900000
best mean reward 1856.100000
running time 16893.992534
Train_EnvstepsSoFar : 3840001
Train_AverageReturn : 1588.9
Train_BestReturn : 1856.1
TimeSinceStart : 16893.992533922195
Training Loss : 1.5440927743911743
Done logging...




********** Iteration 3841000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3842000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3843000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3844000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3845000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3846000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3847000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3848000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3849000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3850000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3850001
mean reward (100 episodes) 1661.400000
best mean reward 1856.100000
running time 16937.419523
Train_EnvstepsSoFar : 3850001
Train_AverageReturn : 1661.4
Train_BestReturn : 1856.1
TimeSinceStart : 16937.4195227623
Training Loss : 3.5567831993103027
Done logging...




********** Iteration 3851000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3852000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3853000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3854000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3855000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3856000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3857000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3858000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3859000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3860000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3860001
mean reward (100 episodes) 1780.300000
best mean reward 1856.100000
running time 16980.845915
Train_EnvstepsSoFar : 3860001
Train_AverageReturn : 1780.3
Train_BestReturn : 1856.1
TimeSinceStart : 16980.84591460228
Training Loss : 0.20340324938297272
Done logging...




********** Iteration 3861000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3862000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3863000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3864000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3865000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3866000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3867000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3868000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3869000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3870000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3870001
mean reward (100 episodes) 1699.000000
best mean reward 1856.100000
running time 17024.159608
Train_EnvstepsSoFar : 3870001
Train_AverageReturn : 1699.0
Train_BestReturn : 1856.1
TimeSinceStart : 17024.15960764885
Training Loss : 0.2737249732017517
Done logging...




********** Iteration 3871000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3872000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3873000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3874000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3875000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3876000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3877000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3878000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3879000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3880000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3880001
mean reward (100 episodes) 1672.000000
best mean reward 1856.100000
running time 17067.324391
Train_EnvstepsSoFar : 3880001
Train_AverageReturn : 1672.0
Train_BestReturn : 1856.1
TimeSinceStart : 17067.324390888214
Training Loss : 0.588884711265564
Done logging...




********** Iteration 3881000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3882000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3883000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3884000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3885000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3886000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3887000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3888000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3889000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3890000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3890001
mean reward (100 episodes) 1565.300000
best mean reward 1856.100000
running time 17110.605815
Train_EnvstepsSoFar : 3890001
Train_AverageReturn : 1565.3
Train_BestReturn : 1856.1
TimeSinceStart : 17110.605815410614
Training Loss : 0.4273347854614258
Done logging...




********** Iteration 3891000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3892000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3893000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3894000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3895000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3896000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3897000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3898000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3899000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3900000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3900001
mean reward (100 episodes) 1601.200000
best mean reward 1856.100000
running time 17153.740528
Train_EnvstepsSoFar : 3900001
Train_AverageReturn : 1601.2
Train_BestReturn : 1856.1
TimeSinceStart : 17153.740527629852
Training Loss : 0.2998707890510559
Done logging...




********** Iteration 3901000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3902000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3903000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3904000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3905000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3906000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3907000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3908000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3909000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3910000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3910001
mean reward (100 episodes) 1691.500000
best mean reward 1856.100000
running time 17196.902240
Train_EnvstepsSoFar : 3910001
Train_AverageReturn : 1691.5
Train_BestReturn : 1856.1
TimeSinceStart : 17196.90223956108
Training Loss : 0.11872778832912445
Done logging...




********** Iteration 3911000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3912000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3913000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3914000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3915000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3916000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3917000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3918000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3919000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3920000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3920001
mean reward (100 episodes) 1807.100000
best mean reward 1856.100000
running time 17240.075640
Train_EnvstepsSoFar : 3920001
Train_AverageReturn : 1807.1
Train_BestReturn : 1856.1
TimeSinceStart : 17240.07563972473
Training Loss : 0.1887325644493103
Done logging...




********** Iteration 3921000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3922000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3923000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3924000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3925000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3926000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3927000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3928000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3929000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3930000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3930001
mean reward (100 episodes) 1828.000000
best mean reward 1856.100000
running time 17283.266349
Train_EnvstepsSoFar : 3930001
Train_AverageReturn : 1828.0
Train_BestReturn : 1856.1
TimeSinceStart : 17283.266348600388
Training Loss : 0.1876002550125122
Done logging...




********** Iteration 3931000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3932000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3933000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3934000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3935000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3936000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3937000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3938000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3939000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3940000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3940001
mean reward (100 episodes) 1807.100000
best mean reward 1856.100000
running time 17326.384088
Train_EnvstepsSoFar : 3940001
Train_AverageReturn : 1807.1
Train_BestReturn : 1856.1
TimeSinceStart : 17326.38408780098
Training Loss : 1.4609068632125854
Done logging...




********** Iteration 3941000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3942000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3943000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3944000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3945000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3946000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3947000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3948000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3949000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3950000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3950001
mean reward (100 episodes) 1709.600000
best mean reward 1856.100000
running time 17369.434280
Train_EnvstepsSoFar : 3950001
Train_AverageReturn : 1709.6
Train_BestReturn : 1856.1
TimeSinceStart : 17369.43428015709
Training Loss : 0.4573712944984436
Done logging...




********** Iteration 3951000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3952000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3953000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3954000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3955000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3956000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3957000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3958000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3959000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3960000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3960001
mean reward (100 episodes) 1701.100000
best mean reward 1856.100000
running time 17412.589492
Train_EnvstepsSoFar : 3960001
Train_AverageReturn : 1701.1
Train_BestReturn : 1856.1
TimeSinceStart : 17412.589492321014
Training Loss : 0.34097641706466675
Done logging...




********** Iteration 3961000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3962000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3963000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3964000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3965000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3966000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3967000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3968000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3969000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3970000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3970001
mean reward (100 episodes) 1748.800000
best mean reward 1856.100000
running time 17455.627982
Train_EnvstepsSoFar : 3970001
Train_AverageReturn : 1748.8
Train_BestReturn : 1856.1
TimeSinceStart : 17455.62798190117
Training Loss : 0.7556959390640259
Done logging...




********** Iteration 3971000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3972000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3973000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3974000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3975000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3976000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3977000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3978000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3979000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3980000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3980001
mean reward (100 episodes) 1786.600000
best mean reward 1856.100000
running time 17498.704764
Train_EnvstepsSoFar : 3980001
Train_AverageReturn : 1786.6
Train_BestReturn : 1856.1
TimeSinceStart : 17498.70476436615
Training Loss : 0.312721848487854
Done logging...




********** Iteration 3981000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3982000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3983000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3984000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3985000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3986000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3987000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3988000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3989000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3990000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 3990001
mean reward (100 episodes) 1772.500000
best mean reward 1856.100000
running time 17541.871406
Train_EnvstepsSoFar : 3990001
Train_AverageReturn : 1772.5
Train_BestReturn : 1856.1
TimeSinceStart : 17541.8714056015
Training Loss : 0.18038031458854675
Done logging...




********** Iteration 3991000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3992000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3993000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3994000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3995000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3996000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3997000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3998000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 3999000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4000000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4000001
mean reward (100 episodes) 1732.900000
best mean reward 1856.100000
running time 17587.604544
Train_EnvstepsSoFar : 4000001
Train_AverageReturn : 1732.9
Train_BestReturn : 1856.1
TimeSinceStart : 17587.604543685913
Training Loss : 0.11349395662546158
Done logging...




********** Iteration 4001000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4002000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4003000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4004000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4005000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4006000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4007000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4008000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4009000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4010000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4010001
mean reward (100 episodes) 1708.200000
best mean reward 1856.100000
running time 17630.335607
Train_EnvstepsSoFar : 4010001
Train_AverageReturn : 1708.2
Train_BestReturn : 1856.1
TimeSinceStart : 17630.335607290268
Training Loss : 0.09222709387540817
Done logging...




********** Iteration 4011000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4012000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4013000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4014000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4015000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4016000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4017000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4018000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4019000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4020000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4020001
mean reward (100 episodes) 1692.200000
best mean reward 1856.100000
running time 17673.477622
Train_EnvstepsSoFar : 4020001
Train_AverageReturn : 1692.2
Train_BestReturn : 1856.1
TimeSinceStart : 17673.477621793747
Training Loss : 1.4684484004974365
Done logging...




********** Iteration 4021000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4022000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4023000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4024000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4025000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4026000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4027000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4028000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4029000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4030000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4030001
mean reward (100 episodes) 1698.300000
best mean reward 1856.100000
running time 17716.609974
Train_EnvstepsSoFar : 4030001
Train_AverageReturn : 1698.3
Train_BestReturn : 1856.1
TimeSinceStart : 17716.609974384308
Training Loss : 1.464407205581665
Done logging...




********** Iteration 4031000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4032000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4033000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4034000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4035000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4036000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4037000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4038000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4039000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4040000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4040001
mean reward (100 episodes) 1677.300000
best mean reward 1856.100000
running time 17759.610674
Train_EnvstepsSoFar : 4040001
Train_AverageReturn : 1677.3
Train_BestReturn : 1856.1
TimeSinceStart : 17759.610673666
Training Loss : 0.178113654255867
Done logging...




********** Iteration 4041000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4042000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4043000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4044000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4045000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4046000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4047000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4048000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4049000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4050000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4050001
mean reward (100 episodes) 1682.300000
best mean reward 1856.100000
running time 17802.353788
Train_EnvstepsSoFar : 4050001
Train_AverageReturn : 1682.3
Train_BestReturn : 1856.1
TimeSinceStart : 17802.353788375854
Training Loss : 0.21893873810768127
Done logging...




********** Iteration 4051000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4052000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4053000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4054000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4055000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4056000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4057000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4058000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4059000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4060000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4060001
mean reward (100 episodes) 1724.000000
best mean reward 1856.100000
running time 17845.385437
Train_EnvstepsSoFar : 4060001
Train_AverageReturn : 1724.0
Train_BestReturn : 1856.1
TimeSinceStart : 17845.3854367733
Training Loss : 1.964823603630066
Done logging...




********** Iteration 4061000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4062000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4063000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4064000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4065000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4066000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4067000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4068000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4069000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4070000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4070001
mean reward (100 episodes) 1699.700000
best mean reward 1856.100000
running time 17888.434263
Train_EnvstepsSoFar : 4070001
Train_AverageReturn : 1699.7
Train_BestReturn : 1856.1
TimeSinceStart : 17888.43426299095
Training Loss : 0.488256573677063
Done logging...




********** Iteration 4071000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4072000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4073000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4074000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4075000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4076000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4077000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4078000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4079000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4080000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4080001
mean reward (100 episodes) 1662.500000
best mean reward 1856.100000
running time 17931.418400
Train_EnvstepsSoFar : 4080001
Train_AverageReturn : 1662.5
Train_BestReturn : 1856.1
TimeSinceStart : 17931.418399572372
Training Loss : 0.4870591461658478
Done logging...




********** Iteration 4081000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4082000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4083000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4084000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4085000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4086000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4087000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4088000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4089000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4090000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4090001
mean reward (100 episodes) 1649.300000
best mean reward 1856.100000
running time 17974.388866
Train_EnvstepsSoFar : 4090001
Train_AverageReturn : 1649.3
Train_BestReturn : 1856.1
TimeSinceStart : 17974.388865709305
Training Loss : 0.14302147924900055
Done logging...




********** Iteration 4091000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4092000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4093000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4094000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4095000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4096000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4097000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4098000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4099000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4100001
mean reward (100 episodes) 1654.300000
best mean reward 1856.100000
running time 18017.514878
Train_EnvstepsSoFar : 4100001
Train_AverageReturn : 1654.3
Train_BestReturn : 1856.1
TimeSinceStart : 18017.51487803459
Training Loss : 0.5260552167892456
Done logging...




********** Iteration 4101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4110001
mean reward (100 episodes) 1656.600000
best mean reward 1856.100000
running time 18060.568039
Train_EnvstepsSoFar : 4110001
Train_AverageReturn : 1656.6
Train_BestReturn : 1856.1
TimeSinceStart : 18060.56803917885
Training Loss : 1.068744421005249
Done logging...




********** Iteration 4111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4120001
mean reward (100 episodes) 1682.900000
best mean reward 1856.100000
running time 18103.670454
Train_EnvstepsSoFar : 4120001
Train_AverageReturn : 1682.9
Train_BestReturn : 1856.1
TimeSinceStart : 18103.67045354843
Training Loss : 0.5950933694839478
Done logging...




********** Iteration 4121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4130001
mean reward (100 episodes) 1621.500000
best mean reward 1856.100000
running time 18146.770452
Train_EnvstepsSoFar : 4130001
Train_AverageReturn : 1621.5
Train_BestReturn : 1856.1
TimeSinceStart : 18146.770451545715
Training Loss : 0.3090430200099945
Done logging...




********** Iteration 4131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4140001
mean reward (100 episodes) 1570.800000
best mean reward 1856.100000
running time 18189.727409
Train_EnvstepsSoFar : 4140001
Train_AverageReturn : 1570.8
Train_BestReturn : 1856.1
TimeSinceStart : 18189.727408647537
Training Loss : 0.20020692050457
Done logging...




********** Iteration 4141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4150001
mean reward (100 episodes) 1602.800000
best mean reward 1856.100000
running time 18232.640323
Train_EnvstepsSoFar : 4150001
Train_AverageReturn : 1602.8
Train_BestReturn : 1856.1
TimeSinceStart : 18232.640323400497
Training Loss : 0.09996746480464935
Done logging...




********** Iteration 4151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4160001
mean reward (100 episodes) 1628.200000
best mean reward 1856.100000
running time 18275.676390
Train_EnvstepsSoFar : 4160001
Train_AverageReturn : 1628.2
Train_BestReturn : 1856.1
TimeSinceStart : 18275.67639017105
Training Loss : 0.6211535930633545
Done logging...




********** Iteration 4161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4170001
mean reward (100 episodes) 1681.700000
best mean reward 1856.100000
running time 18318.693022
Train_EnvstepsSoFar : 4170001
Train_AverageReturn : 1681.7
Train_BestReturn : 1856.1
TimeSinceStart : 18318.693021774292
Training Loss : 1.4576765298843384
Done logging...




********** Iteration 4171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4180001
mean reward (100 episodes) 1660.400000
best mean reward 1856.100000
running time 18361.855167
Train_EnvstepsSoFar : 4180001
Train_AverageReturn : 1660.4
Train_BestReturn : 1856.1
TimeSinceStart : 18361.85516691208
Training Loss : 0.20591014623641968
Done logging...




********** Iteration 4181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4190001
mean reward (100 episodes) 1665.200000
best mean reward 1856.100000
running time 18404.690729
Train_EnvstepsSoFar : 4190001
Train_AverageReturn : 1665.2
Train_BestReturn : 1856.1
TimeSinceStart : 18404.690728902817
Training Loss : 0.14708426594734192
Done logging...




********** Iteration 4191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4200001
mean reward (100 episodes) 1741.300000
best mean reward 1856.100000
running time 18447.884383
Train_EnvstepsSoFar : 4200001
Train_AverageReturn : 1741.3
Train_BestReturn : 1856.1
TimeSinceStart : 18447.884383440018
Training Loss : 0.4686473608016968
Done logging...




********** Iteration 4201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4210001
mean reward (100 episodes) 1711.000000
best mean reward 1856.100000
running time 18490.832452
Train_EnvstepsSoFar : 4210001
Train_AverageReturn : 1711.0
Train_BestReturn : 1856.1
TimeSinceStart : 18490.832451820374
Training Loss : 0.22334155440330505
Done logging...




********** Iteration 4211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4220001
mean reward (100 episodes) 1746.200000
best mean reward 1856.100000
running time 18533.836832
Train_EnvstepsSoFar : 4220001
Train_AverageReturn : 1746.2
Train_BestReturn : 1856.1
TimeSinceStart : 18533.83683156967
Training Loss : 0.2511172890663147
Done logging...




********** Iteration 4221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4230001
mean reward (100 episodes) 1666.900000
best mean reward 1856.100000
running time 18576.921972
Train_EnvstepsSoFar : 4230001
Train_AverageReturn : 1666.9
Train_BestReturn : 1856.1
TimeSinceStart : 18576.92197227478
Training Loss : 0.19818073511123657
Done logging...




********** Iteration 4231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4240001
mean reward (100 episodes) 1712.600000
best mean reward 1856.100000
running time 18620.200802
Train_EnvstepsSoFar : 4240001
Train_AverageReturn : 1712.6
Train_BestReturn : 1856.1
TimeSinceStart : 18620.200802087784
Training Loss : 1.8519201278686523
Done logging...




********** Iteration 4241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4250001
mean reward (100 episodes) 1710.300000
best mean reward 1856.100000
running time 18663.425090
Train_EnvstepsSoFar : 4250001
Train_AverageReturn : 1710.3
Train_BestReturn : 1856.1
TimeSinceStart : 18663.425089597702
Training Loss : 0.26174822449684143
Done logging...




********** Iteration 4251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4260001
mean reward (100 episodes) 1675.700000
best mean reward 1856.100000
running time 18706.358836
Train_EnvstepsSoFar : 4260001
Train_AverageReturn : 1675.7
Train_BestReturn : 1856.1
TimeSinceStart : 18706.358835935593
Training Loss : 0.0976174846291542
Done logging...




********** Iteration 4261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4270001
mean reward (100 episodes) 1650.200000
best mean reward 1856.100000
running time 18749.446708
Train_EnvstepsSoFar : 4270001
Train_AverageReturn : 1650.2
Train_BestReturn : 1856.1
TimeSinceStart : 18749.446707725525
Training Loss : 0.9075188040733337
Done logging...




********** Iteration 4271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4280001
mean reward (100 episodes) 1693.600000
best mean reward 1856.100000
running time 18792.463493
Train_EnvstepsSoFar : 4280001
Train_AverageReturn : 1693.6
Train_BestReturn : 1856.1
TimeSinceStart : 18792.46349287033
Training Loss : 0.21744094789028168
Done logging...




********** Iteration 4281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4290001
mean reward (100 episodes) 1689.800000
best mean reward 1856.100000
running time 18835.519907
Train_EnvstepsSoFar : 4290001
Train_AverageReturn : 1689.8
Train_BestReturn : 1856.1
TimeSinceStart : 18835.5199072361
Training Loss : 0.3357059359550476
Done logging...




********** Iteration 4291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4300001
mean reward (100 episodes) 1732.800000
best mean reward 1856.100000
running time 18878.431066
Train_EnvstepsSoFar : 4300001
Train_AverageReturn : 1732.8
Train_BestReturn : 1856.1
TimeSinceStart : 18878.431065559387
Training Loss : 0.740293562412262
Done logging...




********** Iteration 4301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4310001
mean reward (100 episodes) 1768.100000
best mean reward 1856.100000
running time 18921.401754
Train_EnvstepsSoFar : 4310001
Train_AverageReturn : 1768.1
Train_BestReturn : 1856.1
TimeSinceStart : 18921.401754140854
Training Loss : 0.11994250118732452
Done logging...




********** Iteration 4311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4320001
mean reward (100 episodes) 1802.500000
best mean reward 1856.100000
running time 18964.267092
Train_EnvstepsSoFar : 4320001
Train_AverageReturn : 1802.5
Train_BestReturn : 1856.1
TimeSinceStart : 18964.26709151268
Training Loss : 0.573788583278656
Done logging...




********** Iteration 4321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4330001
mean reward (100 episodes) 1847.900000
best mean reward 1856.100000
running time 19007.138318
Train_EnvstepsSoFar : 4330001
Train_AverageReturn : 1847.9
Train_BestReturn : 1856.1
TimeSinceStart : 19007.138318300247
Training Loss : 1.4978746175765991
Done logging...




********** Iteration 4331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4340001
mean reward (100 episodes) 1808.500000
best mean reward 1856.100000
running time 19050.436219
Train_EnvstepsSoFar : 4340001
Train_AverageReturn : 1808.5
Train_BestReturn : 1856.1
TimeSinceStart : 19050.436218738556
Training Loss : 0.6570884585380554
Done logging...




********** Iteration 4341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4350001
mean reward (100 episodes) 1824.700000
best mean reward 1856.100000
running time 19093.016187
Train_EnvstepsSoFar : 4350001
Train_AverageReturn : 1824.7
Train_BestReturn : 1856.1
TimeSinceStart : 19093.016187429428
Training Loss : 0.13894091546535492
Done logging...




********** Iteration 4351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4360001
mean reward (100 episodes) 1743.300000
best mean reward 1856.100000
running time 19135.785850
Train_EnvstepsSoFar : 4360001
Train_AverageReturn : 1743.3
Train_BestReturn : 1856.1
TimeSinceStart : 19135.785849809647
Training Loss : 0.20734263956546783
Done logging...




********** Iteration 4361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4370001
mean reward (100 episodes) 1704.100000
best mean reward 1856.100000
running time 19178.762049
Train_EnvstepsSoFar : 4370001
Train_AverageReturn : 1704.1
Train_BestReturn : 1856.1
TimeSinceStart : 19178.762048721313
Training Loss : 0.18369269371032715
Done logging...




********** Iteration 4371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4380001
mean reward (100 episodes) 1667.300000
best mean reward 1856.100000
running time 19221.704828
Train_EnvstepsSoFar : 4380001
Train_AverageReturn : 1667.3
Train_BestReturn : 1856.1
TimeSinceStart : 19221.704827785492
Training Loss : 0.450541615486145
Done logging...




********** Iteration 4381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4390001
mean reward (100 episodes) 1719.400000
best mean reward 1856.100000
running time 19264.564023
Train_EnvstepsSoFar : 4390001
Train_AverageReturn : 1719.4
Train_BestReturn : 1856.1
TimeSinceStart : 19264.564023017883
Training Loss : 0.7999811172485352
Done logging...




********** Iteration 4391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4400001
mean reward (100 episodes) 1780.700000
best mean reward 1856.100000
running time 19307.417949
Train_EnvstepsSoFar : 4400001
Train_AverageReturn : 1780.7
Train_BestReturn : 1856.1
TimeSinceStart : 19307.417949438095
Training Loss : 0.7388538122177124
Done logging...




********** Iteration 4401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4410001
mean reward (100 episodes) 1835.800000
best mean reward 1856.100000
running time 19350.273059
Train_EnvstepsSoFar : 4410001
Train_AverageReturn : 1835.8
Train_BestReturn : 1856.1
TimeSinceStart : 19350.273058891296
Training Loss : 0.12724736332893372
Done logging...




********** Iteration 4411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4420001
mean reward (100 episodes) 1804.300000
best mean reward 1856.100000
running time 19393.082111
Train_EnvstepsSoFar : 4420001
Train_AverageReturn : 1804.3
Train_BestReturn : 1856.1
TimeSinceStart : 19393.082110643387
Training Loss : 0.15456929802894592
Done logging...




********** Iteration 4421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4430001
mean reward (100 episodes) 1782.000000
best mean reward 1856.100000
running time 19435.866953
Train_EnvstepsSoFar : 4430001
Train_AverageReturn : 1782.0
Train_BestReturn : 1856.1
TimeSinceStart : 19435.8669526577
Training Loss : 0.20335936546325684
Done logging...




********** Iteration 4431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4440001
mean reward (100 episodes) 1768.100000
best mean reward 1856.100000
running time 19478.844633
Train_EnvstepsSoFar : 4440001
Train_AverageReturn : 1768.1
Train_BestReturn : 1856.1
TimeSinceStart : 19478.844633340836
Training Loss : 0.14880259335041046
Done logging...




********** Iteration 4441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4450001
mean reward (100 episodes) 1780.500000
best mean reward 1856.100000
running time 19521.770566
Train_EnvstepsSoFar : 4450001
Train_AverageReturn : 1780.5
Train_BestReturn : 1856.1
TimeSinceStart : 19521.770566225052
Training Loss : 0.7870238423347473
Done logging...




********** Iteration 4451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4460001
mean reward (100 episodes) 1802.200000
best mean reward 1856.100000
running time 19564.767431
Train_EnvstepsSoFar : 4460001
Train_AverageReturn : 1802.2
Train_BestReturn : 1856.1
TimeSinceStart : 19564.767431020737
Training Loss : 1.511038064956665
Done logging...




********** Iteration 4461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4470001
mean reward (100 episodes) 1761.500000
best mean reward 1856.100000
running time 19607.517958
Train_EnvstepsSoFar : 4470001
Train_AverageReturn : 1761.5
Train_BestReturn : 1856.1
TimeSinceStart : 19607.517957687378
Training Loss : 1.8089675903320312
Done logging...




********** Iteration 4471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4480001
mean reward (100 episodes) 1698.800000
best mean reward 1856.100000
running time 19650.323942
Train_EnvstepsSoFar : 4480001
Train_AverageReturn : 1698.8
Train_BestReturn : 1856.1
TimeSinceStart : 19650.32394218445
Training Loss : 0.2761572599411011
Done logging...




********** Iteration 4481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4490001
mean reward (100 episodes) 1618.700000
best mean reward 1856.100000
running time 19693.475624
Train_EnvstepsSoFar : 4490001
Train_AverageReturn : 1618.7
Train_BestReturn : 1856.1
TimeSinceStart : 19693.47562432289
Training Loss : 0.5656610131263733
Done logging...




********** Iteration 4491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4499000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4500000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4500001
mean reward (100 episodes) 1677.200000
best mean reward 1856.100000
running time 19738.908611
Train_EnvstepsSoFar : 4500001
Train_AverageReturn : 1677.2
Train_BestReturn : 1856.1
TimeSinceStart : 19738.90861058235
Training Loss : 0.5571019053459167
Done logging...




********** Iteration 4501000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4502000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4503000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4504000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4505000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4506000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4507000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4508000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4509000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4510000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4510001
mean reward (100 episodes) 1739.300000
best mean reward 1856.100000
running time 19781.480360
Train_EnvstepsSoFar : 4510001
Train_AverageReturn : 1739.3
Train_BestReturn : 1856.1
TimeSinceStart : 19781.480360031128
Training Loss : 1.0328294038772583
Done logging...




********** Iteration 4511000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4512000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4513000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4514000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4515000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4516000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4517000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4518000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4519000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4520000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4520001
mean reward (100 episodes) 1682.500000
best mean reward 1856.100000
running time 19824.661494
Train_EnvstepsSoFar : 4520001
Train_AverageReturn : 1682.5
Train_BestReturn : 1856.1
TimeSinceStart : 19824.661494493484
Training Loss : 0.197207972407341
Done logging...




********** Iteration 4521000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4522000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4523000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4524000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4525000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4526000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4527000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4528000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4529000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4530000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4530001
mean reward (100 episodes) 1628.400000
best mean reward 1856.100000
running time 19868.130280
Train_EnvstepsSoFar : 4530001
Train_AverageReturn : 1628.4
Train_BestReturn : 1856.1
TimeSinceStart : 19868.130280017853
Training Loss : 0.18151816725730896
Done logging...




********** Iteration 4531000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4532000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4533000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4534000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4535000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4536000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4537000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4538000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4539000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4540000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4540001
mean reward (100 episodes) 1636.600000
best mean reward 1856.100000
running time 19911.036353
Train_EnvstepsSoFar : 4540001
Train_AverageReturn : 1636.6
Train_BestReturn : 1856.1
TimeSinceStart : 19911.036353349686
Training Loss : 0.2526169717311859
Done logging...




********** Iteration 4541000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4542000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4543000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4544000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4545000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4546000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4547000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4548000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4549000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4550000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4550001
mean reward (100 episodes) 1680.200000
best mean reward 1856.100000
running time 19953.815874
Train_EnvstepsSoFar : 4550001
Train_AverageReturn : 1680.2
Train_BestReturn : 1856.1
TimeSinceStart : 19953.81587409973
Training Loss : 0.12291308492422104
Done logging...




********** Iteration 4551000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4552000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4553000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4554000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4555000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4556000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4557000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4558000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4559000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4560000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4560001
mean reward (100 episodes) 1700.000000
best mean reward 1856.100000
running time 19996.490256
Train_EnvstepsSoFar : 4560001
Train_AverageReturn : 1700.0
Train_BestReturn : 1856.1
TimeSinceStart : 19996.49025630951
Training Loss : 1.4662981033325195
Done logging...




********** Iteration 4561000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4562000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4563000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4564000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4565000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4566000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4567000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4568000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4569000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4570000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4570001
mean reward (100 episodes) 1662.200000
best mean reward 1856.100000
running time 20039.259802
Train_EnvstepsSoFar : 4570001
Train_AverageReturn : 1662.2
Train_BestReturn : 1856.1
TimeSinceStart : 20039.259802103043
Training Loss : 0.1283586025238037
Done logging...




********** Iteration 4571000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4572000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4573000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4574000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4575000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4576000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4577000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4578000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4579000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4580000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4580001
mean reward (100 episodes) 1705.700000
best mean reward 1856.100000
running time 20082.188745
Train_EnvstepsSoFar : 4580001
Train_AverageReturn : 1705.7
Train_BestReturn : 1856.1
TimeSinceStart : 20082.18874502182
Training Loss : 0.9851324558258057
Done logging...




********** Iteration 4581000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4582000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4583000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4584000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4585000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4586000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4587000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4588000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4589000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4590000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4590001
mean reward (100 episodes) 1705.300000
best mean reward 1856.100000
running time 20125.025182
Train_EnvstepsSoFar : 4590001
Train_AverageReturn : 1705.3
Train_BestReturn : 1856.1
TimeSinceStart : 20125.02518248558
Training Loss : 0.1320037543773651
Done logging...




********** Iteration 4591000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4592000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4593000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4594000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4595000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4596000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4597000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4598000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4599000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4600000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4600001
mean reward (100 episodes) 1708.900000
best mean reward 1856.100000
running time 20167.797501
Train_EnvstepsSoFar : 4600001
Train_AverageReturn : 1708.9
Train_BestReturn : 1856.1
TimeSinceStart : 20167.79750061035
Training Loss : 0.323048859834671
Done logging...




********** Iteration 4601000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4602000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4603000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4604000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4605000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4606000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4607000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4608000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4609000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4610000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4610001
mean reward (100 episodes) 1752.600000
best mean reward 1856.100000
running time 20210.340021
Train_EnvstepsSoFar : 4610001
Train_AverageReturn : 1752.6
Train_BestReturn : 1856.1
TimeSinceStart : 20210.340021133423
Training Loss : 0.17748133838176727
Done logging...




********** Iteration 4611000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4612000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4613000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4614000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4615000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4616000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4617000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4618000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4619000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4620000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4620001
mean reward (100 episodes) 1755.100000
best mean reward 1856.100000
running time 20253.105470
Train_EnvstepsSoFar : 4620001
Train_AverageReturn : 1755.1
Train_BestReturn : 1856.1
TimeSinceStart : 20253.105469703674
Training Loss : 1.4354455471038818
Done logging...




********** Iteration 4621000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4622000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4623000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4624000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4625000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4626000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4627000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4628000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4629000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4630000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4630001
mean reward (100 episodes) 1765.100000
best mean reward 1856.100000
running time 20296.123097
Train_EnvstepsSoFar : 4630001
Train_AverageReturn : 1765.1
Train_BestReturn : 1856.1
TimeSinceStart : 20296.12309741974
Training Loss : 0.3145608603954315
Done logging...




********** Iteration 4631000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4632000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4633000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4634000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4635000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4636000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4637000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4638000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4639000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4640000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4640001
mean reward (100 episodes) 1746.600000
best mean reward 1856.100000
running time 20338.692609
Train_EnvstepsSoFar : 4640001
Train_AverageReturn : 1746.6
Train_BestReturn : 1856.1
TimeSinceStart : 20338.692608594894
Training Loss : 0.7123015522956848
Done logging...




********** Iteration 4641000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4642000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4643000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4644000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4645000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4646000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4647000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4648000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4649000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4650000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4650001
mean reward (100 episodes) 1731.700000
best mean reward 1856.100000
running time 20381.444181
Train_EnvstepsSoFar : 4650001
Train_AverageReturn : 1731.7
Train_BestReturn : 1856.1
TimeSinceStart : 20381.444181203842
Training Loss : 0.2274574637413025
Done logging...




********** Iteration 4651000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4652000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4653000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4654000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4655000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4656000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4657000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4658000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4659000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4660000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4660001
mean reward (100 episodes) 1728.700000
best mean reward 1856.100000
running time 20424.297742
Train_EnvstepsSoFar : 4660001
Train_AverageReturn : 1728.7
Train_BestReturn : 1856.1
TimeSinceStart : 20424.297741889954
Training Loss : 0.3542722463607788
Done logging...




********** Iteration 4661000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4662000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4663000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4664000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4665000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4666000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4667000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4668000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4669000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4670000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4670001
mean reward (100 episodes) 1676.300000
best mean reward 1856.100000
running time 20467.264097
Train_EnvstepsSoFar : 4670001
Train_AverageReturn : 1676.3
Train_BestReturn : 1856.1
TimeSinceStart : 20467.264097452164
Training Loss : 0.25706666707992554
Done logging...




********** Iteration 4671000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4672000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4673000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4674000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4675000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4676000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4677000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4678000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4679000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4680000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4680001
mean reward (100 episodes) 1704.700000
best mean reward 1856.100000
running time 20510.178771
Train_EnvstepsSoFar : 4680001
Train_AverageReturn : 1704.7
Train_BestReturn : 1856.1
TimeSinceStart : 20510.1787712574
Training Loss : 0.40396636724472046
Done logging...




********** Iteration 4681000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4682000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4683000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4684000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4685000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4686000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4687000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4688000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4689000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4690000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4690001
mean reward (100 episodes) 1694.300000
best mean reward 1856.100000
running time 20552.806071
Train_EnvstepsSoFar : 4690001
Train_AverageReturn : 1694.3
Train_BestReturn : 1856.1
TimeSinceStart : 20552.806071043015
Training Loss : 0.2547188997268677
Done logging...




********** Iteration 4691000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4692000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4693000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4694000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4695000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4696000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4697000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4698000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4699000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4700000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4700001
mean reward (100 episodes) 1753.100000
best mean reward 1856.100000
running time 20595.613690
Train_EnvstepsSoFar : 4700001
Train_AverageReturn : 1753.1
Train_BestReturn : 1856.1
TimeSinceStart : 20595.613690137863
Training Loss : 0.32852494716644287
Done logging...




********** Iteration 4701000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4702000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4703000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4704000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4705000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4706000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4707000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4708000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4709000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4710000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4710001
mean reward (100 episodes) 1725.600000
best mean reward 1856.100000
running time 20638.561145
Train_EnvstepsSoFar : 4710001
Train_AverageReturn : 1725.6
Train_BestReturn : 1856.1
TimeSinceStart : 20638.561144828796
Training Loss : 1.8803479671478271
Done logging...




********** Iteration 4711000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4712000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4713000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4714000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4715000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4716000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4717000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4718000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4719000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4720000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4720001
mean reward (100 episodes) 1674.400000
best mean reward 1856.100000
running time 20681.467408
Train_EnvstepsSoFar : 4720001
Train_AverageReturn : 1674.4
Train_BestReturn : 1856.1
TimeSinceStart : 20681.467408180237
Training Loss : 2.0305514335632324
Done logging...




********** Iteration 4721000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4722000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4723000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4724000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4725000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4726000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4727000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4728000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4729000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4730000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4730001
mean reward (100 episodes) 1656.400000
best mean reward 1856.100000
running time 20724.433439
Train_EnvstepsSoFar : 4730001
Train_AverageReturn : 1656.4
Train_BestReturn : 1856.1
TimeSinceStart : 20724.433438777924
Training Loss : 0.13082033395767212
Done logging...




********** Iteration 4731000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4732000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4733000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4734000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4735000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4736000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4737000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4738000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4739000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4740000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4740001
mean reward (100 episodes) 1679.100000
best mean reward 1856.100000
running time 20767.300041
Train_EnvstepsSoFar : 4740001
Train_AverageReturn : 1679.1
Train_BestReturn : 1856.1
TimeSinceStart : 20767.300040721893
Training Loss : 0.2510671019554138
Done logging...




********** Iteration 4741000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4742000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4743000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4744000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4745000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4746000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4747000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4748000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4749000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4750000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4750001
mean reward (100 episodes) 1750.900000
best mean reward 1856.100000
running time 20810.578057
Train_EnvstepsSoFar : 4750001
Train_AverageReturn : 1750.9
Train_BestReturn : 1856.1
TimeSinceStart : 20810.578056573868
Training Loss : 0.19103604555130005
Done logging...




********** Iteration 4751000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4752000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4753000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4754000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4755000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4756000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4757000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4758000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4759000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4760000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4760001
mean reward (100 episodes) 1782.500000
best mean reward 1856.100000
running time 20853.882869
Train_EnvstepsSoFar : 4760001
Train_AverageReturn : 1782.5
Train_BestReturn : 1856.1
TimeSinceStart : 20853.882868766785
Training Loss : 0.1590120494365692
Done logging...




********** Iteration 4761000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4762000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4763000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4764000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4765000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4766000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4767000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4768000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4769000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4770000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4770001
mean reward (100 episodes) 1792.300000
best mean reward 1856.100000
running time 20897.030225
Train_EnvstepsSoFar : 4770001
Train_AverageReturn : 1792.3
Train_BestReturn : 1856.1
TimeSinceStart : 20897.03022480011
Training Loss : 1.0156993865966797
Done logging...




********** Iteration 4771000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4772000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4773000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4774000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4775000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4776000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4777000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4778000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4779000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4780000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4780001
mean reward (100 episodes) 1797.600000
best mean reward 1856.100000
running time 20940.110606
Train_EnvstepsSoFar : 4780001
Train_AverageReturn : 1797.6
Train_BestReturn : 1856.1
TimeSinceStart : 20940.110605955124
Training Loss : 0.3213285207748413
Done logging...




********** Iteration 4781000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4782000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4783000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4784000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4785000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4786000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4787000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4788000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4789000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4790000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4790001
mean reward (100 episodes) 1749.500000
best mean reward 1856.100000
running time 20983.354651
Train_EnvstepsSoFar : 4790001
Train_AverageReturn : 1749.5
Train_BestReturn : 1856.1
TimeSinceStart : 20983.35465145111
Training Loss : 2.2290940284729004
Done logging...




********** Iteration 4791000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4792000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4793000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4794000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4795000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4796000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4797000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4798000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4799000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4800000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4800001
mean reward (100 episodes) 1773.500000
best mean reward 1856.100000
running time 21026.815258
Train_EnvstepsSoFar : 4800001
Train_AverageReturn : 1773.5
Train_BestReturn : 1856.1
TimeSinceStart : 21026.815257787704
Training Loss : 0.2982705235481262
Done logging...




********** Iteration 4801000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4802000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4803000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4804000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4805000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4806000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4807000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4808000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4809000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4810000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4810001
mean reward (100 episodes) 1748.300000
best mean reward 1856.100000
running time 21069.876122
Train_EnvstepsSoFar : 4810001
Train_AverageReturn : 1748.3
Train_BestReturn : 1856.1
TimeSinceStart : 21069.87612247467
Training Loss : 1.0492398738861084
Done logging...




********** Iteration 4811000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4812000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4813000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4814000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4815000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4816000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4817000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4818000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4819000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4820000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4820001
mean reward (100 episodes) 1715.500000
best mean reward 1856.100000
running time 21112.979220
Train_EnvstepsSoFar : 4820001
Train_AverageReturn : 1715.5
Train_BestReturn : 1856.1
TimeSinceStart : 21112.979219913483
Training Loss : 0.24742534756660461
Done logging...




********** Iteration 4821000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4822000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4823000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4824000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4825000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4826000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4827000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4828000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4829000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4830000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4830001
mean reward (100 episodes) 1640.800000
best mean reward 1856.100000
running time 21155.983850
Train_EnvstepsSoFar : 4830001
Train_AverageReturn : 1640.8
Train_BestReturn : 1856.1
TimeSinceStart : 21155.98384952545
Training Loss : 2.360170841217041
Done logging...




********** Iteration 4831000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4832000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4833000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4834000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4835000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4836000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4837000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4838000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4839000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4840000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4840001
mean reward (100 episodes) 1604.000000
best mean reward 1856.100000
running time 21199.230907
Train_EnvstepsSoFar : 4840001
Train_AverageReturn : 1604.0
Train_BestReturn : 1856.1
TimeSinceStart : 21199.230907440186
Training Loss : 0.22122018039226532
Done logging...




********** Iteration 4841000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4842000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4843000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4844000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4845000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4846000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4847000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4848000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4849000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4850000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4850001
mean reward (100 episodes) 1623.100000
best mean reward 1856.100000
running time 21242.422427
Train_EnvstepsSoFar : 4850001
Train_AverageReturn : 1623.1
Train_BestReturn : 1856.1
TimeSinceStart : 21242.42242717743
Training Loss : 0.31563717126846313
Done logging...




********** Iteration 4851000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4852000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4853000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4854000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4855000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4856000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4857000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4858000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4859000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4860000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4860001
mean reward (100 episodes) 1660.600000
best mean reward 1856.100000
running time 21285.718447
Train_EnvstepsSoFar : 4860001
Train_AverageReturn : 1660.6
Train_BestReturn : 1856.1
TimeSinceStart : 21285.718446969986
Training Loss : 0.2179242968559265
Done logging...




********** Iteration 4861000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4862000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4863000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4864000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4865000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4866000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4867000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4868000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4869000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4870000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4870001
mean reward (100 episodes) 1725.400000
best mean reward 1856.100000
running time 21328.996426
Train_EnvstepsSoFar : 4870001
Train_AverageReturn : 1725.4
Train_BestReturn : 1856.1
TimeSinceStart : 21328.99642586708
Training Loss : 0.18170660734176636
Done logging...




********** Iteration 4871000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4872000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4873000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4874000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4875000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4876000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4877000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4878000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4879000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4880000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4880001
mean reward (100 episodes) 1793.600000
best mean reward 1856.100000
running time 21372.153749
Train_EnvstepsSoFar : 4880001
Train_AverageReturn : 1793.6
Train_BestReturn : 1856.1
TimeSinceStart : 21372.153749227524
Training Loss : 2.075773239135742
Done logging...




********** Iteration 4881000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4882000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4883000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4884000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4885000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4886000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4887000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4888000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4889000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4890000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4890001
mean reward (100 episodes) 1742.000000
best mean reward 1856.100000
running time 21415.297218
Train_EnvstepsSoFar : 4890001
Train_AverageReturn : 1742.0
Train_BestReturn : 1856.1
TimeSinceStart : 21415.297218322754
Training Loss : 2.210280656814575
Done logging...




********** Iteration 4891000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4892000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4893000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4894000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4895000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4896000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4897000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4898000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4899000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4900000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4900001
mean reward (100 episodes) 1668.200000
best mean reward 1856.100000
running time 21458.657490
Train_EnvstepsSoFar : 4900001
Train_AverageReturn : 1668.2
Train_BestReturn : 1856.1
TimeSinceStart : 21458.65748977661
Training Loss : 0.47430184483528137
Done logging...




********** Iteration 4901000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4902000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4903000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4904000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4905000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4906000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4907000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4908000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4909000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4910000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4910001
mean reward (100 episodes) 1699.300000
best mean reward 1856.100000
running time 21501.657335
Train_EnvstepsSoFar : 4910001
Train_AverageReturn : 1699.3
Train_BestReturn : 1856.1
TimeSinceStart : 21501.657334804535
Training Loss : 0.16879799962043762
Done logging...




********** Iteration 4911000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4912000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4913000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4914000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4915000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4916000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4917000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4918000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4919000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4920000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4920001
mean reward (100 episodes) 1726.200000
best mean reward 1856.100000
running time 21544.765099
Train_EnvstepsSoFar : 4920001
Train_AverageReturn : 1726.2
Train_BestReturn : 1856.1
TimeSinceStart : 21544.765098810196
Training Loss : 0.37149253487586975
Done logging...




********** Iteration 4921000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4922000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4923000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4924000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4925000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4926000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4927000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4928000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4929000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4930000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4930001
mean reward (100 episodes) 1781.000000
best mean reward 1856.100000
running time 21588.004887
Train_EnvstepsSoFar : 4930001
Train_AverageReturn : 1781.0
Train_BestReturn : 1856.1
TimeSinceStart : 21588.004886865616
Training Loss : 0.23885691165924072
Done logging...




********** Iteration 4931000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4932000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4933000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4934000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4935000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4936000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4937000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4938000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4939000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4940000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4940001
mean reward (100 episodes) 1757.500000
best mean reward 1856.100000
running time 21631.182168
Train_EnvstepsSoFar : 4940001
Train_AverageReturn : 1757.5
Train_BestReturn : 1856.1
TimeSinceStart : 21631.18216776848
Training Loss : 0.5095210671424866
Done logging...




********** Iteration 4941000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4942000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4943000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4944000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4945000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4946000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4947000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4948000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4949000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4950000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4950001
mean reward (100 episodes) 1767.000000
best mean reward 1856.100000
running time 21674.478052
Train_EnvstepsSoFar : 4950001
Train_AverageReturn : 1767.0
Train_BestReturn : 1856.1
TimeSinceStart : 21674.478051900864
Training Loss : 1.5182113647460938
Done logging...




********** Iteration 4951000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4952000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4953000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4954000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4955000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4956000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4957000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4958000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4959000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4960000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4960001
mean reward (100 episodes) 1801.100000
best mean reward 1856.100000
running time 21717.653983
Train_EnvstepsSoFar : 4960001
Train_AverageReturn : 1801.1
Train_BestReturn : 1856.1
TimeSinceStart : 21717.653982639313
Training Loss : 0.12844304740428925
Done logging...




********** Iteration 4961000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4962000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4963000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4964000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4965000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4966000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4967000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4968000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4969000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4970000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4970001
mean reward (100 episodes) 1852.500000
best mean reward 1856.100000
running time 21761.054986
Train_EnvstepsSoFar : 4970001
Train_AverageReturn : 1852.5
Train_BestReturn : 1856.1
TimeSinceStart : 21761.0549864769
Training Loss : 0.1899111270904541
Done logging...




********** Iteration 4971000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4972000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4973000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4974000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4975000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4976000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4977000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4978000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4979000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4980000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4980001
mean reward (100 episodes) 1881.200000
best mean reward 1881.200000
running time 21804.369558
Train_EnvstepsSoFar : 4980001
Train_AverageReturn : 1881.2
Train_BestReturn : 1881.2
TimeSinceStart : 21804.36955833435
Training Loss : 0.2328873574733734
Done logging...




********** Iteration 4981000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4982000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4983000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4984000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4985000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4986000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4987000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4988000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4989000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4990000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 4990001
mean reward (100 episodes) 1804.200000
best mean reward 1881.200000
running time 21847.815177
Train_EnvstepsSoFar : 4990001
Train_AverageReturn : 1804.2
Train_BestReturn : 1881.2
TimeSinceStart : 21847.815176725388
Training Loss : 0.3890228867530823
Done logging...




********** Iteration 4991000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4992000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4993000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4994000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4995000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4996000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4997000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4998000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 4999000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5000000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5000001
mean reward (100 episodes) 1763.000000
best mean reward 1881.200000
running time 21893.771166
Train_EnvstepsSoFar : 5000001
Train_AverageReturn : 1763.0
Train_BestReturn : 1881.2
TimeSinceStart : 21893.771166086197
Training Loss : 0.24959796667099
Done logging...




********** Iteration 5001000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5002000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5003000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5004000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5005000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5006000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5007000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5008000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5009000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5010000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5010001
mean reward (100 episodes) 1741.700000
best mean reward 1881.200000
running time 21936.703062
Train_EnvstepsSoFar : 5010001
Train_AverageReturn : 1741.7
Train_BestReturn : 1881.2
TimeSinceStart : 21936.703062295914
Training Loss : 0.15778927505016327
Done logging...




********** Iteration 5011000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5012000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5013000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5014000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5015000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5016000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5017000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5018000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5019000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5020000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5020001
mean reward (100 episodes) 1767.400000
best mean reward 1881.200000
running time 21979.988548
Train_EnvstepsSoFar : 5020001
Train_AverageReturn : 1767.4
Train_BestReturn : 1881.2
TimeSinceStart : 21979.98854804039
Training Loss : 0.24655914306640625
Done logging...




********** Iteration 5021000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5022000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5023000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5024000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5025000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5026000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5027000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5028000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5029000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5030000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5030001
mean reward (100 episodes) 1759.500000
best mean reward 1881.200000
running time 22023.409509
Train_EnvstepsSoFar : 5030001
Train_AverageReturn : 1759.5
Train_BestReturn : 1881.2
TimeSinceStart : 22023.40950870514
Training Loss : 0.17749743163585663
Done logging...




********** Iteration 5031000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5032000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5033000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5034000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5035000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5036000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5037000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5038000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5039000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5040000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5040001
mean reward (100 episodes) 1721.100000
best mean reward 1881.200000
running time 22066.748067
Train_EnvstepsSoFar : 5040001
Train_AverageReturn : 1721.1
Train_BestReturn : 1881.2
TimeSinceStart : 22066.74806690216
Training Loss : 0.8159828782081604
Done logging...




********** Iteration 5041000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5042000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5043000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5044000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5045000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5046000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5047000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5048000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5049000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5050000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5050001
mean reward (100 episodes) 1745.300000
best mean reward 1881.200000
running time 22110.016313
Train_EnvstepsSoFar : 5050001
Train_AverageReturn : 1745.3
Train_BestReturn : 1881.2
TimeSinceStart : 22110.01631307602
Training Loss : 0.17863807082176208
Done logging...




********** Iteration 5051000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5052000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5053000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5054000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5055000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5056000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5057000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5058000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5059000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5060000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5060001
mean reward (100 episodes) 1723.500000
best mean reward 1881.200000
running time 22153.235788
Train_EnvstepsSoFar : 5060001
Train_AverageReturn : 1723.5
Train_BestReturn : 1881.2
TimeSinceStart : 22153.23578810692
Training Loss : 0.27501171827316284
Done logging...




********** Iteration 5061000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5062000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5063000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5064000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5065000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5066000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5067000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5068000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5069000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5070000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5070001
mean reward (100 episodes) 1733.100000
best mean reward 1881.200000
running time 22196.761157
Train_EnvstepsSoFar : 5070001
Train_AverageReturn : 1733.1
Train_BestReturn : 1881.2
TimeSinceStart : 22196.76115655899
Training Loss : 0.25362980365753174
Done logging...




********** Iteration 5071000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5072000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5073000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5074000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5075000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5076000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5077000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5078000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5079000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5080000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5080001
mean reward (100 episodes) 1775.100000
best mean reward 1881.200000
running time 22239.858734
Train_EnvstepsSoFar : 5080001
Train_AverageReturn : 1775.1
Train_BestReturn : 1881.2
TimeSinceStart : 22239.85873413086
Training Loss : 1.0011930465698242
Done logging...




********** Iteration 5081000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5082000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5083000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5084000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5085000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5086000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5087000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5088000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5089000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5090000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5090001
mean reward (100 episodes) 1742.700000
best mean reward 1881.200000
running time 22283.349924
Train_EnvstepsSoFar : 5090001
Train_AverageReturn : 1742.7
Train_BestReturn : 1881.2
TimeSinceStart : 22283.349924087524
Training Loss : 0.17441198229789734
Done logging...




********** Iteration 5091000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5092000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5093000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5094000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5095000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5096000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5097000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5098000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5099000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5100001
mean reward (100 episodes) 1720.400000
best mean reward 1881.200000
running time 22326.627732
Train_EnvstepsSoFar : 5100001
Train_AverageReturn : 1720.4
Train_BestReturn : 1881.2
TimeSinceStart : 22326.62773156166
Training Loss : 0.1785348355770111
Done logging...




********** Iteration 5101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5110001
mean reward (100 episodes) 1691.900000
best mean reward 1881.200000
running time 22370.225457
Train_EnvstepsSoFar : 5110001
Train_AverageReturn : 1691.9
Train_BestReturn : 1881.2
TimeSinceStart : 22370.225457191467
Training Loss : 0.17673945426940918
Done logging...




********** Iteration 5111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5120001
mean reward (100 episodes) 1789.000000
best mean reward 1881.200000
running time 22413.595812
Train_EnvstepsSoFar : 5120001
Train_AverageReturn : 1789.0
Train_BestReturn : 1881.2
TimeSinceStart : 22413.595811605453
Training Loss : 0.3175918757915497
Done logging...




********** Iteration 5121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5130001
mean reward (100 episodes) 1736.000000
best mean reward 1881.200000
running time 22457.096071
Train_EnvstepsSoFar : 5130001
Train_AverageReturn : 1736.0
Train_BestReturn : 1881.2
TimeSinceStart : 22457.096071004868
Training Loss : 1.119873285293579
Done logging...




********** Iteration 5131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5140001
mean reward (100 episodes) 1733.600000
best mean reward 1881.200000
running time 22500.539000
Train_EnvstepsSoFar : 5140001
Train_AverageReturn : 1733.6
Train_BestReturn : 1881.2
TimeSinceStart : 22500.53900027275
Training Loss : 1.5176246166229248
Done logging...




********** Iteration 5141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5150001
mean reward (100 episodes) 1702.300000
best mean reward 1881.200000
running time 22543.888458
Train_EnvstepsSoFar : 5150001
Train_AverageReturn : 1702.3
Train_BestReturn : 1881.2
TimeSinceStart : 22543.88845849037
Training Loss : 0.1788424253463745
Done logging...




********** Iteration 5151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5160001
mean reward (100 episodes) 1707.100000
best mean reward 1881.200000
running time 22587.358726
Train_EnvstepsSoFar : 5160001
Train_AverageReturn : 1707.1
Train_BestReturn : 1881.2
TimeSinceStart : 22587.358726263046
Training Loss : 0.7244626879692078
Done logging...




********** Iteration 5161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5170001
mean reward (100 episodes) 1739.500000
best mean reward 1881.200000
running time 22630.856968
Train_EnvstepsSoFar : 5170001
Train_AverageReturn : 1739.5
Train_BestReturn : 1881.2
TimeSinceStart : 22630.856968402863
Training Loss : 0.7064796090126038
Done logging...




********** Iteration 5171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5180001
mean reward (100 episodes) 1731.200000
best mean reward 1881.200000
running time 22674.164147
Train_EnvstepsSoFar : 5180001
Train_AverageReturn : 1731.2
Train_BestReturn : 1881.2
TimeSinceStart : 22674.164146900177
Training Loss : 0.17857632040977478
Done logging...




********** Iteration 5181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5190001
mean reward (100 episodes) 1695.400000
best mean reward 1881.200000
running time 22717.543264
Train_EnvstepsSoFar : 5190001
Train_AverageReturn : 1695.4
Train_BestReturn : 1881.2
TimeSinceStart : 22717.5432639122
Training Loss : 0.7415754795074463
Done logging...




********** Iteration 5191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5200001
mean reward (100 episodes) 1737.100000
best mean reward 1881.200000
running time 22760.776539
Train_EnvstepsSoFar : 5200001
Train_AverageReturn : 1737.1
Train_BestReturn : 1881.2
TimeSinceStart : 22760.776538848877
Training Loss : 0.21542491018772125
Done logging...




********** Iteration 5201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5210001
mean reward (100 episodes) 1754.500000
best mean reward 1881.200000
running time 22804.048960
Train_EnvstepsSoFar : 5210001
Train_AverageReturn : 1754.5
Train_BestReturn : 1881.2
TimeSinceStart : 22804.048960208893
Training Loss : 0.14062820374965668
Done logging...




********** Iteration 5211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5220001
mean reward (100 episodes) 1741.800000
best mean reward 1881.200000
running time 22847.268199
Train_EnvstepsSoFar : 5220001
Train_AverageReturn : 1741.8
Train_BestReturn : 1881.2
TimeSinceStart : 22847.26819872856
Training Loss : 1.5753024816513062
Done logging...




********** Iteration 5221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5230001
mean reward (100 episodes) 1737.800000
best mean reward 1881.200000
running time 22890.633988
Train_EnvstepsSoFar : 5230001
Train_AverageReturn : 1737.8
Train_BestReturn : 1881.2
TimeSinceStart : 22890.633987665176
Training Loss : 0.1759028434753418
Done logging...




********** Iteration 5231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5240001
mean reward (100 episodes) 1723.900000
best mean reward 1881.200000
running time 22933.883710
Train_EnvstepsSoFar : 5240001
Train_AverageReturn : 1723.9
Train_BestReturn : 1881.2
TimeSinceStart : 22933.88370990753
Training Loss : 0.29006004333496094
Done logging...




********** Iteration 5241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5250001
mean reward (100 episodes) 1672.100000
best mean reward 1881.200000
running time 22977.491345
Train_EnvstepsSoFar : 5250001
Train_AverageReturn : 1672.1
Train_BestReturn : 1881.2
TimeSinceStart : 22977.491344690323
Training Loss : 1.0833766460418701
Done logging...




********** Iteration 5251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5260001
mean reward (100 episodes) 1707.300000
best mean reward 1881.200000
running time 23020.984375
Train_EnvstepsSoFar : 5260001
Train_AverageReturn : 1707.3
Train_BestReturn : 1881.2
TimeSinceStart : 23020.984374523163
Training Loss : 0.9455379843711853
Done logging...




********** Iteration 5261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5270001
mean reward (100 episodes) 1709.000000
best mean reward 1881.200000
running time 23064.409688
Train_EnvstepsSoFar : 5270001
Train_AverageReturn : 1709.0
Train_BestReturn : 1881.2
TimeSinceStart : 23064.409687757492
Training Loss : 1.9398561716079712
Done logging...




********** Iteration 5271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5280001
mean reward (100 episodes) 1835.600000
best mean reward 1881.200000
running time 23107.592316
Train_EnvstepsSoFar : 5280001
Train_AverageReturn : 1835.6
Train_BestReturn : 1881.2
TimeSinceStart : 23107.592315673828
Training Loss : 0.1374594271183014
Done logging...




********** Iteration 5281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5290001
mean reward (100 episodes) 1857.300000
best mean reward 1881.200000
running time 23150.801731
Train_EnvstepsSoFar : 5290001
Train_AverageReturn : 1857.3
Train_BestReturn : 1881.2
TimeSinceStart : 23150.80173110962
Training Loss : 0.2262885719537735
Done logging...




********** Iteration 5291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5300001
mean reward (100 episodes) 1866.000000
best mean reward 1881.200000
running time 23194.029792
Train_EnvstepsSoFar : 5300001
Train_AverageReturn : 1866.0
Train_BestReturn : 1881.2
TimeSinceStart : 23194.02979207039
Training Loss : 0.35253873467445374
Done logging...




********** Iteration 5301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5310001
mean reward (100 episodes) 1784.600000
best mean reward 1881.200000
running time 23237.484141
Train_EnvstepsSoFar : 5310001
Train_AverageReturn : 1784.6
Train_BestReturn : 1881.2
TimeSinceStart : 23237.484141349792
Training Loss : 0.9555538296699524
Done logging...




********** Iteration 5311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5320001
mean reward (100 episodes) 1756.900000
best mean reward 1881.200000
running time 23280.762635
Train_EnvstepsSoFar : 5320001
Train_AverageReturn : 1756.9
Train_BestReturn : 1881.2
TimeSinceStart : 23280.762634515762
Training Loss : 0.1295313835144043
Done logging...




********** Iteration 5321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5330001
mean reward (100 episodes) 1707.100000
best mean reward 1881.200000
running time 23323.885574
Train_EnvstepsSoFar : 5330001
Train_AverageReturn : 1707.1
Train_BestReturn : 1881.2
TimeSinceStart : 23323.8855741024
Training Loss : 0.7290887832641602
Done logging...




********** Iteration 5331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5340001
mean reward (100 episodes) 1718.000000
best mean reward 1881.200000
running time 23367.333263
Train_EnvstepsSoFar : 5340001
Train_AverageReturn : 1718.0
Train_BestReturn : 1881.2
TimeSinceStart : 23367.33326292038
Training Loss : 0.10428769886493683
Done logging...




********** Iteration 5341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5350001
mean reward (100 episodes) 1701.800000
best mean reward 1881.200000
running time 23410.653405
Train_EnvstepsSoFar : 5350001
Train_AverageReturn : 1701.8
Train_BestReturn : 1881.2
TimeSinceStart : 23410.653404951096
Training Loss : 1.4622644186019897
Done logging...




********** Iteration 5351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5360001
mean reward (100 episodes) 1653.100000
best mean reward 1881.200000
running time 23453.969331
Train_EnvstepsSoFar : 5360001
Train_AverageReturn : 1653.1
Train_BestReturn : 1881.2
TimeSinceStart : 23453.969331026077
Training Loss : 1.2450166940689087
Done logging...




********** Iteration 5361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5370001
mean reward (100 episodes) 1632.600000
best mean reward 1881.200000
running time 23497.356329
Train_EnvstepsSoFar : 5370001
Train_AverageReturn : 1632.6
Train_BestReturn : 1881.2
TimeSinceStart : 23497.356329202652
Training Loss : 0.21951138973236084
Done logging...




********** Iteration 5371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5380001
mean reward (100 episodes) 1673.600000
best mean reward 1881.200000
running time 23540.542126
Train_EnvstepsSoFar : 5380001
Train_AverageReturn : 1673.6
Train_BestReturn : 1881.2
TimeSinceStart : 23540.54212641716
Training Loss : 2.463125228881836
Done logging...




********** Iteration 5381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5390001
mean reward (100 episodes) 1633.400000
best mean reward 1881.200000
running time 23583.855893
Train_EnvstepsSoFar : 5390001
Train_AverageReturn : 1633.4
Train_BestReturn : 1881.2
TimeSinceStart : 23583.855892896652
Training Loss : 0.8817658424377441
Done logging...




********** Iteration 5391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5400001
mean reward (100 episodes) 1619.600000
best mean reward 1881.200000
running time 23627.241731
Train_EnvstepsSoFar : 5400001
Train_AverageReturn : 1619.6
Train_BestReturn : 1881.2
TimeSinceStart : 23627.24173116684
Training Loss : 0.9070000648498535
Done logging...




********** Iteration 5401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5410001
mean reward (100 episodes) 1605.400000
best mean reward 1881.200000
running time 23670.582697
Train_EnvstepsSoFar : 5410001
Train_AverageReturn : 1605.4
Train_BestReturn : 1881.2
TimeSinceStart : 23670.582696914673
Training Loss : 1.4361943006515503
Done logging...




********** Iteration 5411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5420001
mean reward (100 episodes) 1701.500000
best mean reward 1881.200000
running time 23713.892072
Train_EnvstepsSoFar : 5420001
Train_AverageReturn : 1701.5
Train_BestReturn : 1881.2
TimeSinceStart : 23713.892072200775
Training Loss : 1.1504302024841309
Done logging...




********** Iteration 5421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5430001
mean reward (100 episodes) 1687.800000
best mean reward 1881.200000
running time 23757.158016
Train_EnvstepsSoFar : 5430001
Train_AverageReturn : 1687.8
Train_BestReturn : 1881.2
TimeSinceStart : 23757.158015727997
Training Loss : 0.7174471616744995
Done logging...




********** Iteration 5431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5440001
mean reward (100 episodes) 1669.800000
best mean reward 1881.200000
running time 23800.557743
Train_EnvstepsSoFar : 5440001
Train_AverageReturn : 1669.8
Train_BestReturn : 1881.2
TimeSinceStart : 23800.55774331093
Training Loss : 0.49804067611694336
Done logging...




********** Iteration 5441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5450001
mean reward (100 episodes) 1675.900000
best mean reward 1881.200000
running time 23843.739443
Train_EnvstepsSoFar : 5450001
Train_AverageReturn : 1675.9
Train_BestReturn : 1881.2
TimeSinceStart : 23843.739443302155
Training Loss : 1.648378610610962
Done logging...




********** Iteration 5451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5460001
mean reward (100 episodes) 1736.800000
best mean reward 1881.200000
running time 23887.035731
Train_EnvstepsSoFar : 5460001
Train_AverageReturn : 1736.8
Train_BestReturn : 1881.2
TimeSinceStart : 23887.035731315613
Training Loss : 2.050267219543457
Done logging...




********** Iteration 5461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5470001
mean reward (100 episodes) 1717.100000
best mean reward 1881.200000
running time 23930.181564
Train_EnvstepsSoFar : 5470001
Train_AverageReturn : 1717.1
Train_BestReturn : 1881.2
TimeSinceStart : 23930.181564331055
Training Loss : 1.6547120809555054
Done logging...




********** Iteration 5471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5480001
mean reward (100 episodes) 1712.300000
best mean reward 1881.200000
running time 23973.376890
Train_EnvstepsSoFar : 5480001
Train_AverageReturn : 1712.3
Train_BestReturn : 1881.2
TimeSinceStart : 23973.376890182495
Training Loss : 0.40578487515449524
Done logging...




********** Iteration 5481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5490001
mean reward (100 episodes) 1638.200000
best mean reward 1881.200000
running time 24016.569728
Train_EnvstepsSoFar : 5490001
Train_AverageReturn : 1638.2
Train_BestReturn : 1881.2
TimeSinceStart : 24016.56972837448
Training Loss : 0.35283759236335754
Done logging...




********** Iteration 5491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5499000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5500000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5500001
mean reward (100 episodes) 1707.600000
best mean reward 1881.200000
running time 24061.911010
Train_EnvstepsSoFar : 5500001
Train_AverageReturn : 1707.6
Train_BestReturn : 1881.2
TimeSinceStart : 24061.91101026535
Training Loss : 0.2258957326412201
Done logging...




********** Iteration 5501000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5502000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5503000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5504000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5505000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5506000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5507000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5508000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5509000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5510000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5510001
mean reward (100 episodes) 1778.600000
best mean reward 1881.200000
running time 24104.570557
Train_EnvstepsSoFar : 5510001
Train_AverageReturn : 1778.6
Train_BestReturn : 1881.2
TimeSinceStart : 24104.570557117462
Training Loss : 0.21327091753482819
Done logging...




********** Iteration 5511000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5512000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5513000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5514000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5515000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5516000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5517000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5518000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5519000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5520000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5520001
mean reward (100 episodes) 1838.100000
best mean reward 1881.200000
running time 24147.728987
Train_EnvstepsSoFar : 5520001
Train_AverageReturn : 1838.1
Train_BestReturn : 1881.2
TimeSinceStart : 24147.728986740112
Training Loss : 0.13135913014411926
Done logging...




********** Iteration 5521000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5522000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5523000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5524000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5525000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5526000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5527000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5528000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5529000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5530000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5530001
mean reward (100 episodes) 1839.000000
best mean reward 1881.200000
running time 24191.000214
Train_EnvstepsSoFar : 5530001
Train_AverageReturn : 1839.0
Train_BestReturn : 1881.2
TimeSinceStart : 24191.000214099884
Training Loss : 0.12785576283931732
Done logging...




********** Iteration 5531000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5532000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5533000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5534000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5535000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5536000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5537000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5538000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5539000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5540000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5540001
mean reward (100 episodes) 1819.000000
best mean reward 1881.200000
running time 24234.135022
Train_EnvstepsSoFar : 5540001
Train_AverageReturn : 1819.0
Train_BestReturn : 1881.2
TimeSinceStart : 24234.135021924973
Training Loss : 0.10310608148574829
Done logging...




********** Iteration 5541000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5542000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5543000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5544000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5545000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5546000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5547000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5548000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5549000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5550000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5550001
mean reward (100 episodes) 1711.200000
best mean reward 1881.200000
running time 24277.118470
Train_EnvstepsSoFar : 5550001
Train_AverageReturn : 1711.2
Train_BestReturn : 1881.2
TimeSinceStart : 24277.11846971512
Training Loss : 1.0328768491744995
Done logging...




********** Iteration 5551000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5552000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5553000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5554000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5555000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5556000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5557000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5558000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5559000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5560000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5560001
mean reward (100 episodes) 1703.300000
best mean reward 1881.200000
running time 24320.092901
Train_EnvstepsSoFar : 5560001
Train_AverageReturn : 1703.3
Train_BestReturn : 1881.2
TimeSinceStart : 24320.09290099144
Training Loss : 0.9598892331123352
Done logging...




********** Iteration 5561000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5562000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5563000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5564000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5565000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5566000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5567000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5568000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5569000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5570000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5570001
mean reward (100 episodes) 1645.500000
best mean reward 1881.200000
running time 24363.183917
Train_EnvstepsSoFar : 5570001
Train_AverageReturn : 1645.5
Train_BestReturn : 1881.2
TimeSinceStart : 24363.183916807175
Training Loss : 0.2082318365573883
Done logging...




********** Iteration 5571000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5572000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5573000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5574000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5575000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5576000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5577000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5578000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5579000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5580000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5580001
mean reward (100 episodes) 1673.300000
best mean reward 1881.200000
running time 24406.417962
Train_EnvstepsSoFar : 5580001
Train_AverageReturn : 1673.3
Train_BestReturn : 1881.2
TimeSinceStart : 24406.417961597443
Training Loss : 0.266238808631897
Done logging...




********** Iteration 5581000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5582000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5583000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5584000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5585000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5586000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5587000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5588000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5589000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5590000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5590001
mean reward (100 episodes) 1795.300000
best mean reward 1881.200000
running time 24449.555972
Train_EnvstepsSoFar : 5590001
Train_AverageReturn : 1795.3
Train_BestReturn : 1881.2
TimeSinceStart : 24449.555971860886
Training Loss : 1.845127820968628
Done logging...




********** Iteration 5591000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5592000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5593000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5594000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5595000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5596000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5597000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5598000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5599000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5600000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5600001
mean reward (100 episodes) 1806.300000
best mean reward 1881.200000
running time 24492.675885
Train_EnvstepsSoFar : 5600001
Train_AverageReturn : 1806.3
Train_BestReturn : 1881.2
TimeSinceStart : 24492.675884723663
Training Loss : 0.13720238208770752
Done logging...




********** Iteration 5601000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5602000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5603000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5604000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5605000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5606000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5607000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5608000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5609000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5610000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5610001
mean reward (100 episodes) 1830.700000
best mean reward 1881.200000
running time 24535.686614
Train_EnvstepsSoFar : 5610001
Train_AverageReturn : 1830.7
Train_BestReturn : 1881.2
TimeSinceStart : 24535.686613559723
Training Loss : 1.5531867742538452
Done logging...




********** Iteration 5611000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5612000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5613000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5614000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5615000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5616000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5617000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5618000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5619000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5620000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5620001
mean reward (100 episodes) 1712.300000
best mean reward 1881.200000
running time 24578.879895
Train_EnvstepsSoFar : 5620001
Train_AverageReturn : 1712.3
Train_BestReturn : 1881.2
TimeSinceStart : 24578.879894971848
Training Loss : 0.849535346031189
Done logging...




********** Iteration 5621000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5622000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5623000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5624000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5625000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5626000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5627000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5628000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5629000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5630000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5630001
mean reward (100 episodes) 1686.200000
best mean reward 1881.200000
running time 24621.985319
Train_EnvstepsSoFar : 5630001
Train_AverageReturn : 1686.2
Train_BestReturn : 1881.2
TimeSinceStart : 24621.985318899155
Training Loss : 1.3203554153442383
Done logging...




********** Iteration 5631000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5632000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5633000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5634000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5635000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5636000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5637000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5638000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5639000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5640000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5640001
mean reward (100 episodes) 1657.100000
best mean reward 1881.200000
running time 24665.026238
Train_EnvstepsSoFar : 5640001
Train_AverageReturn : 1657.1
Train_BestReturn : 1881.2
TimeSinceStart : 24665.026238441467
Training Loss : 0.15358217060565948
Done logging...




********** Iteration 5641000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5642000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5643000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5644000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5645000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5646000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5647000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5648000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5649000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5650000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5650001
mean reward (100 episodes) 1683.700000
best mean reward 1881.200000
running time 24707.967283
Train_EnvstepsSoFar : 5650001
Train_AverageReturn : 1683.7
Train_BestReturn : 1881.2
TimeSinceStart : 24707.967283010483
Training Loss : 0.8785417079925537
Done logging...




********** Iteration 5651000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5652000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5653000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5654000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5655000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5656000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5657000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5658000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5659000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5660000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5660001
mean reward (100 episodes) 1771.900000
best mean reward 1881.200000
running time 24750.983989
Train_EnvstepsSoFar : 5660001
Train_AverageReturn : 1771.9
Train_BestReturn : 1881.2
TimeSinceStart : 24750.98398923874
Training Loss : 0.12023895233869553
Done logging...




********** Iteration 5661000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5662000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5663000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5664000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5665000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5666000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5667000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5668000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5669000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5670000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5670001
mean reward (100 episodes) 1812.800000
best mean reward 1881.200000
running time 24793.969175
Train_EnvstepsSoFar : 5670001
Train_AverageReturn : 1812.8
Train_BestReturn : 1881.2
TimeSinceStart : 24793.96917462349
Training Loss : 0.7892040014266968
Done logging...




********** Iteration 5671000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5672000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5673000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5674000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5675000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5676000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5677000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5678000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5679000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5680000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5680001
mean reward (100 episodes) 1800.900000
best mean reward 1881.200000
running time 24836.792145
Train_EnvstepsSoFar : 5680001
Train_AverageReturn : 1800.9
Train_BestReturn : 1881.2
TimeSinceStart : 24836.792145490646
Training Loss : 0.12709882855415344
Done logging...




********** Iteration 5681000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5682000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5683000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5684000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5685000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5686000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5687000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5688000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5689000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5690000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5690001
mean reward (100 episodes) 1752.500000
best mean reward 1881.200000
running time 24880.022561
Train_EnvstepsSoFar : 5690001
Train_AverageReturn : 1752.5
Train_BestReturn : 1881.2
TimeSinceStart : 24880.022561073303
Training Loss : 0.1522337645292282
Done logging...




********** Iteration 5691000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5692000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5693000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5694000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5695000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5696000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5697000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5698000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5699000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5700000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5700001
mean reward (100 episodes) 1777.000000
best mean reward 1881.200000
running time 24923.035847
Train_EnvstepsSoFar : 5700001
Train_AverageReturn : 1777.0
Train_BestReturn : 1881.2
TimeSinceStart : 24923.03584742546
Training Loss : 1.3939334154129028
Done logging...




********** Iteration 5701000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5702000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5703000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5704000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5705000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5706000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5707000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5708000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5709000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5710000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5710001
mean reward (100 episodes) 1837.400000
best mean reward 1881.200000
running time 24965.901684
Train_EnvstepsSoFar : 5710001
Train_AverageReturn : 1837.4
Train_BestReturn : 1881.2
TimeSinceStart : 24965.90168404579
Training Loss : 2.027827501296997
Done logging...




********** Iteration 5711000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5712000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5713000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5714000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5715000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5716000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5717000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5718000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5719000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5720000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5720001
mean reward (100 episodes) 1794.100000
best mean reward 1881.200000
running time 25008.497237
Train_EnvstepsSoFar : 5720001
Train_AverageReturn : 1794.1
Train_BestReturn : 1881.2
TimeSinceStart : 25008.497237443924
Training Loss : 0.3608679175376892
Done logging...




********** Iteration 5721000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5722000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5723000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5724000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5725000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5726000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5727000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5728000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5729000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5730000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5730001
mean reward (100 episodes) 1699.100000
best mean reward 1881.200000
running time 25051.296784
Train_EnvstepsSoFar : 5730001
Train_AverageReturn : 1699.1
Train_BestReturn : 1881.2
TimeSinceStart : 25051.296783685684
Training Loss : 1.4874399900436401
Done logging...




********** Iteration 5731000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5732000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5733000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5734000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5735000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5736000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5737000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5738000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5739000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5740000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5740001
mean reward (100 episodes) 1656.000000
best mean reward 1881.200000
running time 25094.203743
Train_EnvstepsSoFar : 5740001
Train_AverageReturn : 1656.0
Train_BestReturn : 1881.2
TimeSinceStart : 25094.203742980957
Training Loss : 1.0477781295776367
Done logging...




********** Iteration 5741000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5742000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5743000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5744000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5745000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5746000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5747000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5748000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5749000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5750000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5750001
mean reward (100 episodes) 1708.900000
best mean reward 1881.200000
running time 25137.061201
Train_EnvstepsSoFar : 5750001
Train_AverageReturn : 1708.9
Train_BestReturn : 1881.2
TimeSinceStart : 25137.061200857162
Training Loss : 0.4773848056793213
Done logging...




********** Iteration 5751000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5752000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5753000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5754000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5755000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5756000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5757000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5758000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5759000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5760000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5760001
mean reward (100 episodes) 1747.000000
best mean reward 1881.200000
running time 25179.887283
Train_EnvstepsSoFar : 5760001
Train_AverageReturn : 1747.0
Train_BestReturn : 1881.2
TimeSinceStart : 25179.887282848358
Training Loss : 1.8904168605804443
Done logging...




********** Iteration 5761000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5762000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5763000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5764000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5765000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5766000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5767000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5768000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5769000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5770000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5770001
mean reward (100 episodes) 1797.300000
best mean reward 1881.200000
running time 25222.558957
Train_EnvstepsSoFar : 5770001
Train_AverageReturn : 1797.3
Train_BestReturn : 1881.2
TimeSinceStart : 25222.558957099915
Training Loss : 1.8847359418869019
Done logging...




********** Iteration 5771000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5772000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5773000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5774000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5775000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5776000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5777000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5778000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5779000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5780000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5780001
mean reward (100 episodes) 1758.500000
best mean reward 1881.200000
running time 25265.395420
Train_EnvstepsSoFar : 5780001
Train_AverageReturn : 1758.5
Train_BestReturn : 1881.2
TimeSinceStart : 25265.395420074463
Training Loss : 0.9863632321357727
Done logging...




********** Iteration 5781000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5782000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5783000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5784000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5785000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5786000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5787000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5788000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5789000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5790000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5790001
mean reward (100 episodes) 1725.800000
best mean reward 1881.200000
running time 25308.068379
Train_EnvstepsSoFar : 5790001
Train_AverageReturn : 1725.8
Train_BestReturn : 1881.2
TimeSinceStart : 25308.068378686905
Training Loss : 0.2886357307434082
Done logging...




********** Iteration 5791000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5792000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5793000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5794000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5795000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5796000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5797000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5798000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5799000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5800000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5800001
mean reward (100 episodes) 1717.900000
best mean reward 1881.200000
running time 25351.127293
Train_EnvstepsSoFar : 5800001
Train_AverageReturn : 1717.9
Train_BestReturn : 1881.2
TimeSinceStart : 25351.127292633057
Training Loss : 0.4607071876525879
Done logging...




********** Iteration 5801000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5802000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5803000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5804000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5805000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5806000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5807000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5808000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5809000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5810000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5810001
mean reward (100 episodes) 1743.400000
best mean reward 1881.200000
running time 25394.032794
Train_EnvstepsSoFar : 5810001
Train_AverageReturn : 1743.4
Train_BestReturn : 1881.2
TimeSinceStart : 25394.0327937603
Training Loss : 0.2850118577480316
Done logging...




********** Iteration 5811000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5812000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5813000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5814000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5815000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5816000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5817000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5818000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5819000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5820000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5820001
mean reward (100 episodes) 1773.900000
best mean reward 1881.200000
running time 25436.959130
Train_EnvstepsSoFar : 5820001
Train_AverageReturn : 1773.9
Train_BestReturn : 1881.2
TimeSinceStart : 25436.95913028717
Training Loss : 0.7198361158370972
Done logging...




********** Iteration 5821000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5822000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5823000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5824000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5825000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5826000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5827000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5828000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5829000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5830000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5830001
mean reward (100 episodes) 1761.200000
best mean reward 1881.200000
running time 25479.790882
Train_EnvstepsSoFar : 5830001
Train_AverageReturn : 1761.2
Train_BestReturn : 1881.2
TimeSinceStart : 25479.790881872177
Training Loss : 0.26788225769996643
Done logging...




********** Iteration 5831000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5832000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5833000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5834000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5835000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5836000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5837000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5838000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5839000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5840000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5840001
mean reward (100 episodes) 1779.000000
best mean reward 1881.200000
running time 25522.699696
Train_EnvstepsSoFar : 5840001
Train_AverageReturn : 1779.0
Train_BestReturn : 1881.2
TimeSinceStart : 25522.699696063995
Training Loss : 0.6616783738136292
Done logging...




********** Iteration 5841000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5842000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5843000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5844000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5845000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5846000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5847000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5848000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5849000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5850000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5850001
mean reward (100 episodes) 1768.700000
best mean reward 1881.200000
running time 25565.371647
Train_EnvstepsSoFar : 5850001
Train_AverageReturn : 1768.7
Train_BestReturn : 1881.2
TimeSinceStart : 25565.371646642685
Training Loss : 0.45471101999282837
Done logging...




********** Iteration 5851000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5852000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5853000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5854000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5855000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5856000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5857000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5858000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5859000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5860000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5860001
mean reward (100 episodes) 1820.500000
best mean reward 1881.200000
running time 25608.260555
Train_EnvstepsSoFar : 5860001
Train_AverageReturn : 1820.5
Train_BestReturn : 1881.2
TimeSinceStart : 25608.260555028915
Training Loss : 0.4163669943809509
Done logging...




********** Iteration 5861000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5862000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5863000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5864000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5865000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5866000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5867000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5868000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5869000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5870000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5870001
mean reward (100 episodes) 1838.400000
best mean reward 1881.200000
running time 25651.063201
Train_EnvstepsSoFar : 5870001
Train_AverageReturn : 1838.4
Train_BestReturn : 1881.2
TimeSinceStart : 25651.06320118904
Training Loss : 0.24504218995571136
Done logging...




********** Iteration 5871000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5872000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5873000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5874000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5875000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5876000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5877000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5878000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5879000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5880000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5880001
mean reward (100 episodes) 1792.000000
best mean reward 1881.200000
running time 25693.909096
Train_EnvstepsSoFar : 5880001
Train_AverageReturn : 1792.0
Train_BestReturn : 1881.2
TimeSinceStart : 25693.90909600258
Training Loss : 0.11219443380832672
Done logging...




********** Iteration 5881000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5882000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5883000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5884000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5885000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5886000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5887000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5888000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5889000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5890000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5890001
mean reward (100 episodes) 1755.700000
best mean reward 1881.200000
running time 25736.852975
Train_EnvstepsSoFar : 5890001
Train_AverageReturn : 1755.7
Train_BestReturn : 1881.2
TimeSinceStart : 25736.8529753685
Training Loss : 0.20024146139621735
Done logging...




********** Iteration 5891000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5892000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5893000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5894000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5895000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5896000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5897000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5898000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5899000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5900000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5900001
mean reward (100 episodes) 1716.200000
best mean reward 1881.200000
running time 25779.625248
Train_EnvstepsSoFar : 5900001
Train_AverageReturn : 1716.2
Train_BestReturn : 1881.2
TimeSinceStart : 25779.62524843216
Training Loss : 1.1483274698257446
Done logging...




********** Iteration 5901000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5902000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5903000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5904000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5905000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5906000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5907000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5908000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5909000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5910000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5910001
mean reward (100 episodes) 1785.000000
best mean reward 1881.200000
running time 25822.369992
Train_EnvstepsSoFar : 5910001
Train_AverageReturn : 1785.0
Train_BestReturn : 1881.2
TimeSinceStart : 25822.369992017746
Training Loss : 0.113218292593956
Done logging...




********** Iteration 5911000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5912000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5913000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5914000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5915000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5916000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5917000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5918000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5919000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5920000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5920001
mean reward (100 episodes) 1734.800000
best mean reward 1881.200000
running time 25865.132006
Train_EnvstepsSoFar : 5920001
Train_AverageReturn : 1734.8
Train_BestReturn : 1881.2
TimeSinceStart : 25865.132005929947
Training Loss : 0.1483992636203766
Done logging...




********** Iteration 5921000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5922000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5923000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5924000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5925000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5926000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5927000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5928000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5929000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5930000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5930001
mean reward (100 episodes) 1773.400000
best mean reward 1881.200000
running time 25907.965619
Train_EnvstepsSoFar : 5930001
Train_AverageReturn : 1773.4
Train_BestReturn : 1881.2
TimeSinceStart : 25907.965618610382
Training Loss : 1.031760573387146
Done logging...




********** Iteration 5931000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5932000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5933000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5934000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5935000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5936000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5937000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5938000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5939000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5940000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5940001
mean reward (100 episodes) 1736.400000
best mean reward 1881.200000
running time 25950.526625
Train_EnvstepsSoFar : 5940001
Train_AverageReturn : 1736.4
Train_BestReturn : 1881.2
TimeSinceStart : 25950.52662539482
Training Loss : 1.6874961853027344
Done logging...




********** Iteration 5941000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5942000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5943000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5944000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5945000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5946000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5947000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5948000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5949000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5950000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5950001
mean reward (100 episodes) 1746.900000
best mean reward 1881.200000
running time 25993.548262
Train_EnvstepsSoFar : 5950001
Train_AverageReturn : 1746.9
Train_BestReturn : 1881.2
TimeSinceStart : 25993.548261642456
Training Loss : 0.22295032441616058
Done logging...




********** Iteration 5951000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5952000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5953000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5954000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5955000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5956000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5957000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5958000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5959000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5960000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5960001
mean reward (100 episodes) 1702.100000
best mean reward 1881.200000
running time 26036.359154
Train_EnvstepsSoFar : 5960001
Train_AverageReturn : 1702.1
Train_BestReturn : 1881.2
TimeSinceStart : 26036.359153985977
Training Loss : 0.18857675790786743
Done logging...




********** Iteration 5961000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5962000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5963000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5964000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5965000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5966000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5967000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5968000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5969000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5970000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5970001
mean reward (100 episodes) 1669.000000
best mean reward 1881.200000
running time 26079.333377
Train_EnvstepsSoFar : 5970001
Train_AverageReturn : 1669.0
Train_BestReturn : 1881.2
TimeSinceStart : 26079.333376646042
Training Loss : 0.5333858728408813
Done logging...




********** Iteration 5971000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5972000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5973000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5974000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5975000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5976000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5977000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5978000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5979000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5980000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5980001
mean reward (100 episodes) 1654.900000
best mean reward 1881.200000
running time 26122.128196
Train_EnvstepsSoFar : 5980001
Train_AverageReturn : 1654.9
Train_BestReturn : 1881.2
TimeSinceStart : 26122.12819647789
Training Loss : 0.2287563979625702
Done logging...




********** Iteration 5981000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5982000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5983000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5984000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5985000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5986000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5987000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5988000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5989000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5990000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 5990001
mean reward (100 episodes) 1695.600000
best mean reward 1881.200000
running time 26165.095557
Train_EnvstepsSoFar : 5990001
Train_AverageReturn : 1695.6
Train_BestReturn : 1881.2
TimeSinceStart : 26165.09555721283
Training Loss : 0.05444728210568428
Done logging...




********** Iteration 5991000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5992000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5993000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5994000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5995000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5996000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5997000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5998000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 5999000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6000000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6000001
mean reward (100 episodes) 1765.700000
best mean reward 1881.200000
running time 26210.662988
Train_EnvstepsSoFar : 6000001
Train_AverageReturn : 1765.7
Train_BestReturn : 1881.2
TimeSinceStart : 26210.662987709045
Training Loss : 0.21158035099506378
Done logging...




********** Iteration 6001000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6002000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6003000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6004000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6005000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6006000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6007000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6008000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6009000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6010000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6010001
mean reward (100 episodes) 1749.400000
best mean reward 1881.200000
running time 26253.098869
Train_EnvstepsSoFar : 6010001
Train_AverageReturn : 1749.4
Train_BestReturn : 1881.2
TimeSinceStart : 26253.09886932373
Training Loss : 0.9225443601608276
Done logging...




********** Iteration 6011000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6012000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6013000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6014000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6015000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6016000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6017000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6018000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6019000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6020000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6020001
mean reward (100 episodes) 1715.700000
best mean reward 1881.200000
running time 26296.091501
Train_EnvstepsSoFar : 6020001
Train_AverageReturn : 1715.7
Train_BestReturn : 1881.2
TimeSinceStart : 26296.091500759125
Training Loss : 0.7960567474365234
Done logging...




********** Iteration 6021000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6022000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6023000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6024000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6025000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6026000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6027000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6028000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6029000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6030000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6030001
mean reward (100 episodes) 1673.800000
best mean reward 1881.200000
running time 26338.905792
Train_EnvstepsSoFar : 6030001
Train_AverageReturn : 1673.8
Train_BestReturn : 1881.2
TimeSinceStart : 26338.90579199791
Training Loss : 0.12587222456932068
Done logging...




********** Iteration 6031000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6032000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6033000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6034000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6035000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6036000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6037000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6038000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6039000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6040000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6040001
mean reward (100 episodes) 1669.700000
best mean reward 1881.200000
running time 26381.848414
Train_EnvstepsSoFar : 6040001
Train_AverageReturn : 1669.7
Train_BestReturn : 1881.2
TimeSinceStart : 26381.848413705826
Training Loss : 0.20115362107753754
Done logging...




********** Iteration 6041000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6042000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6043000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6044000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6045000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6046000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6047000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6048000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6049000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6050000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6050001
mean reward (100 episodes) 1716.100000
best mean reward 1881.200000
running time 26424.722896
Train_EnvstepsSoFar : 6050001
Train_AverageReturn : 1716.1
Train_BestReturn : 1881.2
TimeSinceStart : 26424.722895860672
Training Loss : 2.660456895828247
Done logging...




********** Iteration 6051000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6052000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6053000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6054000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6055000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6056000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6057000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6058000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6059000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6060000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6060001
mean reward (100 episodes) 1830.900000
best mean reward 1881.200000
running time 26467.321755
Train_EnvstepsSoFar : 6060001
Train_AverageReturn : 1830.9
Train_BestReturn : 1881.2
TimeSinceStart : 26467.321754693985
Training Loss : 0.2045353651046753
Done logging...




********** Iteration 6061000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6062000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6063000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6064000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6065000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6066000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6067000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6068000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6069000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6070000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6070001
mean reward (100 episodes) 1839.500000
best mean reward 1881.200000
running time 26510.030205
Train_EnvstepsSoFar : 6070001
Train_AverageReturn : 1839.5
Train_BestReturn : 1881.2
TimeSinceStart : 26510.03020477295
Training Loss : 0.2005617618560791
Done logging...




********** Iteration 6071000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6072000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6073000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6074000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6075000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6076000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6077000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6078000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6079000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6080000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6080001
mean reward (100 episodes) 1750.900000
best mean reward 1881.200000
running time 26552.949452
Train_EnvstepsSoFar : 6080001
Train_AverageReturn : 1750.9
Train_BestReturn : 1881.2
TimeSinceStart : 26552.94945168495
Training Loss : 1.3572357892990112
Done logging...




********** Iteration 6081000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6082000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6083000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6084000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6085000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6086000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6087000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6088000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6089000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6090000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6090001
mean reward (100 episodes) 1644.600000
best mean reward 1881.200000
running time 26595.704890
Train_EnvstepsSoFar : 6090001
Train_AverageReturn : 1644.6
Train_BestReturn : 1881.2
TimeSinceStart : 26595.704889535904
Training Loss : 2.1283273696899414
Done logging...




********** Iteration 6091000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6092000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6093000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6094000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6095000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6096000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6097000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6098000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6099000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6100000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6100001
mean reward (100 episodes) 1595.300000
best mean reward 1881.200000
running time 26638.419508
Train_EnvstepsSoFar : 6100001
Train_AverageReturn : 1595.3
Train_BestReturn : 1881.2
TimeSinceStart : 26638.419507741928
Training Loss : 0.8079763054847717
Done logging...




********** Iteration 6101000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6102000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6103000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6104000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6105000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6106000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6107000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6108000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6109000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6110000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6110001
mean reward (100 episodes) 1580.500000
best mean reward 1881.200000
running time 26681.309639
Train_EnvstepsSoFar : 6110001
Train_AverageReturn : 1580.5
Train_BestReturn : 1881.2
TimeSinceStart : 26681.309639453888
Training Loss : 0.22856847941875458
Done logging...




********** Iteration 6111000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6112000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6113000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6114000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6115000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6116000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6117000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6118000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6119000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6120000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6120001
mean reward (100 episodes) 1632.900000
best mean reward 1881.200000
running time 26724.005515
Train_EnvstepsSoFar : 6120001
Train_AverageReturn : 1632.9
Train_BestReturn : 1881.2
TimeSinceStart : 26724.005514860153
Training Loss : 0.5287299752235413
Done logging...




********** Iteration 6121000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6122000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6123000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6124000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6125000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6126000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6127000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6128000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6129000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6130000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6130001
mean reward (100 episodes) 1682.600000
best mean reward 1881.200000
running time 26766.960137
Train_EnvstepsSoFar : 6130001
Train_AverageReturn : 1682.6
Train_BestReturn : 1881.2
TimeSinceStart : 26766.96013689041
Training Loss : 0.2139003723859787
Done logging...




********** Iteration 6131000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6132000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6133000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6134000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6135000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6136000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6137000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6138000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6139000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6140000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6140001
mean reward (100 episodes) 1620.500000
best mean reward 1881.200000
running time 26809.482637
Train_EnvstepsSoFar : 6140001
Train_AverageReturn : 1620.5
Train_BestReturn : 1881.2
TimeSinceStart : 26809.482637405396
Training Loss : 0.23070630431175232
Done logging...




********** Iteration 6141000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6142000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6143000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6144000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6145000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6146000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6147000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6148000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6149000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6150000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6150001
mean reward (100 episodes) 1663.300000
best mean reward 1881.200000
running time 26852.422704
Train_EnvstepsSoFar : 6150001
Train_AverageReturn : 1663.3
Train_BestReturn : 1881.2
TimeSinceStart : 26852.4227039814
Training Loss : 0.7177855968475342
Done logging...




********** Iteration 6151000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6152000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6153000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6154000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6155000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6156000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6157000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6158000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6159000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6160000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6160001
mean reward (100 episodes) 1670.900000
best mean reward 1881.200000
running time 26895.253050
Train_EnvstepsSoFar : 6160001
Train_AverageReturn : 1670.9
Train_BestReturn : 1881.2
TimeSinceStart : 26895.253050088882
Training Loss : 0.8661872148513794
Done logging...




********** Iteration 6161000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6162000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6163000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6164000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6165000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6166000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6167000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6168000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6169000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6170000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6170001
mean reward (100 episodes) 1749.500000
best mean reward 1881.200000
running time 26938.025566
Train_EnvstepsSoFar : 6170001
Train_AverageReturn : 1749.5
Train_BestReturn : 1881.2
TimeSinceStart : 26938.025565624237
Training Loss : 1.2819944620132446
Done logging...




********** Iteration 6171000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6172000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6173000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6174000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6175000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6176000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6177000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6178000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6179000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6180000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6180001
mean reward (100 episodes) 1673.500000
best mean reward 1881.200000
running time 26980.958665
Train_EnvstepsSoFar : 6180001
Train_AverageReturn : 1673.5
Train_BestReturn : 1881.2
TimeSinceStart : 26980.958664894104
Training Loss : 0.3479742109775543
Done logging...




********** Iteration 6181000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6182000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6183000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6184000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6185000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6186000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6187000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6188000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6189000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6190000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6190001
mean reward (100 episodes) 1650.000000
best mean reward 1881.200000
running time 27023.714463
Train_EnvstepsSoFar : 6190001
Train_AverageReturn : 1650.0
Train_BestReturn : 1881.2
TimeSinceStart : 27023.71446299553
Training Loss : 0.09048732370138168
Done logging...




********** Iteration 6191000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6192000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6193000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6194000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6195000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6196000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6197000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6198000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6199000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6200000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6200001
mean reward (100 episodes) 1682.500000
best mean reward 1881.200000
running time 27066.450304
Train_EnvstepsSoFar : 6200001
Train_AverageReturn : 1682.5
Train_BestReturn : 1881.2
TimeSinceStart : 27066.450303792953
Training Loss : 0.6224547028541565
Done logging...




********** Iteration 6201000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6202000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6203000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6204000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6205000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6206000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6207000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6208000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6209000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6210000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6210001
mean reward (100 episodes) 1736.400000
best mean reward 1881.200000
running time 27109.055348
Train_EnvstepsSoFar : 6210001
Train_AverageReturn : 1736.4
Train_BestReturn : 1881.2
TimeSinceStart : 27109.055347919464
Training Loss : 0.17608517408370972
Done logging...




********** Iteration 6211000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6212000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6213000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6214000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6215000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6216000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6217000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6218000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6219000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6220000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6220001
mean reward (100 episodes) 1728.300000
best mean reward 1881.200000
running time 27151.937135
Train_EnvstepsSoFar : 6220001
Train_AverageReturn : 1728.3
Train_BestReturn : 1881.2
TimeSinceStart : 27151.937135457993
Training Loss : 0.4329793453216553
Done logging...




********** Iteration 6221000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6222000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6223000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6224000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6225000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6226000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6227000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6228000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6229000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6230000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6230001
mean reward (100 episodes) 1668.000000
best mean reward 1881.200000
running time 27194.941284
Train_EnvstepsSoFar : 6230001
Train_AverageReturn : 1668.0
Train_BestReturn : 1881.2
TimeSinceStart : 27194.94128394127
Training Loss : 1.1271638870239258
Done logging...




********** Iteration 6231000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6232000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6233000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6234000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6235000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6236000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6237000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6238000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6239000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6240000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6240001
mean reward (100 episodes) 1675.700000
best mean reward 1881.200000
running time 27237.895185
Train_EnvstepsSoFar : 6240001
Train_AverageReturn : 1675.7
Train_BestReturn : 1881.2
TimeSinceStart : 27237.895185232162
Training Loss : 0.8195368051528931
Done logging...




********** Iteration 6241000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6242000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6243000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6244000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6245000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6246000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6247000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6248000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6249000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6250000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6250001
mean reward (100 episodes) 1629.800000
best mean reward 1881.200000
running time 27280.872513
Train_EnvstepsSoFar : 6250001
Train_AverageReturn : 1629.8
Train_BestReturn : 1881.2
TimeSinceStart : 27280.872512817383
Training Loss : 0.09645912051200867
Done logging...




********** Iteration 6251000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6252000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6253000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6254000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6255000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6256000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6257000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6258000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6259000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6260000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6260001
mean reward (100 episodes) 1696.200000
best mean reward 1881.200000
running time 27323.800319
Train_EnvstepsSoFar : 6260001
Train_AverageReturn : 1696.2
Train_BestReturn : 1881.2
TimeSinceStart : 27323.800319194794
Training Loss : 0.25921303033828735
Done logging...




********** Iteration 6261000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6262000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6263000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6264000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6265000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6266000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6267000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6268000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6269000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6270000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6270001
mean reward (100 episodes) 1683.800000
best mean reward 1881.200000
running time 27366.636710
Train_EnvstepsSoFar : 6270001
Train_AverageReturn : 1683.8
Train_BestReturn : 1881.2
TimeSinceStart : 27366.636709690094
Training Loss : 0.2945801615715027
Done logging...




********** Iteration 6271000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6272000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6273000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6274000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6275000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6276000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6277000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6278000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6279000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6280000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6280001
mean reward (100 episodes) 1768.600000
best mean reward 1881.200000
running time 27409.566390
Train_EnvstepsSoFar : 6280001
Train_AverageReturn : 1768.6
Train_BestReturn : 1881.2
TimeSinceStart : 27409.5663895607
Training Loss : 1.677046775817871
Done logging...




********** Iteration 6281000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6282000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6283000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6284000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6285000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6286000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6287000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6288000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6289000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6290000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6290001
mean reward (100 episodes) 1720.400000
best mean reward 1881.200000
running time 27452.604627
Train_EnvstepsSoFar : 6290001
Train_AverageReturn : 1720.4
Train_BestReturn : 1881.2
TimeSinceStart : 27452.604626893997
Training Loss : 0.17507585883140564
Done logging...




********** Iteration 6291000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6292000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6293000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6294000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6295000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6296000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6297000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6298000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6299000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6300000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6300001
mean reward (100 episodes) 1707.500000
best mean reward 1881.200000
running time 27495.699730
Train_EnvstepsSoFar : 6300001
Train_AverageReturn : 1707.5
Train_BestReturn : 1881.2
TimeSinceStart : 27495.699729919434
Training Loss : 0.1627577245235443
Done logging...




********** Iteration 6301000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6302000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6303000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6304000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6305000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6306000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6307000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6308000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6309000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6310000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6310001
mean reward (100 episodes) 1662.400000
best mean reward 1881.200000
running time 27538.859360
Train_EnvstepsSoFar : 6310001
Train_AverageReturn : 1662.4
Train_BestReturn : 1881.2
TimeSinceStart : 27538.859360218048
Training Loss : 0.9008733034133911
Done logging...




********** Iteration 6311000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6312000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6313000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6314000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6315000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6316000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6317000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6318000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6319000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6320000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6320001
mean reward (100 episodes) 1643.200000
best mean reward 1881.200000
running time 27582.121298
Train_EnvstepsSoFar : 6320001
Train_AverageReturn : 1643.2
Train_BestReturn : 1881.2
TimeSinceStart : 27582.12129831314
Training Loss : 0.29850631952285767
Done logging...




********** Iteration 6321000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6322000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6323000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6324000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6325000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6326000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6327000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6328000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6329000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6330000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6330001
mean reward (100 episodes) 1691.700000
best mean reward 1881.200000
running time 27625.060284
Train_EnvstepsSoFar : 6330001
Train_AverageReturn : 1691.7
Train_BestReturn : 1881.2
TimeSinceStart : 27625.060284137726
Training Loss : 0.20323944091796875
Done logging...




********** Iteration 6331000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6332000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6333000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6334000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6335000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6336000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6337000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6338000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6339000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6340000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6340001
mean reward (100 episodes) 1681.400000
best mean reward 1881.200000
running time 27668.354408
Train_EnvstepsSoFar : 6340001
Train_AverageReturn : 1681.4
Train_BestReturn : 1881.2
TimeSinceStart : 27668.354407787323
Training Loss : 1.0791083574295044
Done logging...




********** Iteration 6341000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6342000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6343000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6344000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6345000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6346000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6347000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6348000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6349000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6350000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6350001
mean reward (100 episodes) 1711.900000
best mean reward 1881.200000
running time 27711.774991
Train_EnvstepsSoFar : 6350001
Train_AverageReturn : 1711.9
Train_BestReturn : 1881.2
TimeSinceStart : 27711.774990797043
Training Loss : 0.0900641456246376
Done logging...




********** Iteration 6351000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6352000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6353000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6354000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6355000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6356000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6357000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6358000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6359000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6360000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6360001
mean reward (100 episodes) 1702.600000
best mean reward 1881.200000
running time 27754.987871
Train_EnvstepsSoFar : 6360001
Train_AverageReturn : 1702.6
Train_BestReturn : 1881.2
TimeSinceStart : 27754.987870693207
Training Loss : 0.08995847404003143
Done logging...




********** Iteration 6361000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6362000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6363000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6364000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6365000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6366000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6367000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6368000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6369000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6370000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6370001
mean reward (100 episodes) 1713.500000
best mean reward 1881.200000
running time 27798.422661
Train_EnvstepsSoFar : 6370001
Train_AverageReturn : 1713.5
Train_BestReturn : 1881.2
TimeSinceStart : 27798.422661066055
Training Loss : 0.08111011981964111
Done logging...




********** Iteration 6371000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6372000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6373000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6374000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6375000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6376000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6377000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6378000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6379000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6380000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6380001
mean reward (100 episodes) 1668.600000
best mean reward 1881.200000
running time 27841.681246
Train_EnvstepsSoFar : 6380001
Train_AverageReturn : 1668.6
Train_BestReturn : 1881.2
TimeSinceStart : 27841.681245565414
Training Loss : 1.1896525621414185
Done logging...




********** Iteration 6381000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6382000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6383000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6384000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6385000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6386000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6387000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6388000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6389000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6390000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6390001
mean reward (100 episodes) 1674.800000
best mean reward 1881.200000
running time 27885.124859
Train_EnvstepsSoFar : 6390001
Train_AverageReturn : 1674.8
Train_BestReturn : 1881.2
TimeSinceStart : 27885.124858617783
Training Loss : 1.4660059213638306
Done logging...




********** Iteration 6391000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6392000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6393000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6394000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6395000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6396000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6397000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6398000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6399000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6400000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6400001
mean reward (100 episodes) 1692.800000
best mean reward 1881.200000
running time 27928.390057
Train_EnvstepsSoFar : 6400001
Train_AverageReturn : 1692.8
Train_BestReturn : 1881.2
TimeSinceStart : 27928.390057086945
Training Loss : 0.30036449432373047
Done logging...




********** Iteration 6401000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6402000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6403000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6404000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6405000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6406000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6407000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6408000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6409000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6410000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6410001
mean reward (100 episodes) 1728.500000
best mean reward 1881.200000
running time 27971.625850
Train_EnvstepsSoFar : 6410001
Train_AverageReturn : 1728.5
Train_BestReturn : 1881.2
TimeSinceStart : 27971.625850200653
Training Loss : 0.5271646976470947
Done logging...




********** Iteration 6411000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6412000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6413000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6414000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6415000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6416000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6417000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6418000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6419000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6420000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6420001
mean reward (100 episodes) 1778.800000
best mean reward 1881.200000
running time 28014.835560
Train_EnvstepsSoFar : 6420001
Train_AverageReturn : 1778.8
Train_BestReturn : 1881.2
TimeSinceStart : 28014.83555984497
Training Loss : 1.1561568975448608
Done logging...




********** Iteration 6421000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6422000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6423000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6424000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6425000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6426000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6427000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6428000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6429000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6430000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6430001
mean reward (100 episodes) 1807.200000
best mean reward 1881.200000
running time 28058.127033
Train_EnvstepsSoFar : 6430001
Train_AverageReturn : 1807.2
Train_BestReturn : 1881.2
TimeSinceStart : 28058.127032518387
Training Loss : 0.3645597994327545
Done logging...




********** Iteration 6431000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6432000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6433000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6434000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6435000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6436000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6437000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6438000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6439000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6440000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6440001
mean reward (100 episodes) 1815.400000
best mean reward 1881.200000
running time 28101.435625
Train_EnvstepsSoFar : 6440001
Train_AverageReturn : 1815.4
Train_BestReturn : 1881.2
TimeSinceStart : 28101.435625314713
Training Loss : 1.0187277793884277
Done logging...




********** Iteration 6441000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6442000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6443000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6444000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6445000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6446000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6447000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6448000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6449000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6450000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6450001
mean reward (100 episodes) 1811.500000
best mean reward 1881.200000
running time 28144.706678
Train_EnvstepsSoFar : 6450001
Train_AverageReturn : 1811.5
Train_BestReturn : 1881.2
TimeSinceStart : 28144.706678152084
Training Loss : 0.44706523418426514
Done logging...




********** Iteration 6451000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6452000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6453000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6454000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6455000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6456000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6457000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6458000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6459000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6460000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6460001
mean reward (100 episodes) 1786.500000
best mean reward 1881.200000
running time 28187.956777
Train_EnvstepsSoFar : 6460001
Train_AverageReturn : 1786.5
Train_BestReturn : 1881.2
TimeSinceStart : 28187.956777334213
Training Loss : 0.09929855167865753
Done logging...




********** Iteration 6461000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6462000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6463000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6464000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6465000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6466000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6467000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6468000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6469000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6470000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6470001
mean reward (100 episodes) 1780.900000
best mean reward 1881.200000
running time 28231.175707
Train_EnvstepsSoFar : 6470001
Train_AverageReturn : 1780.9
Train_BestReturn : 1881.2
TimeSinceStart : 28231.175706624985
Training Loss : 1.5116336345672607
Done logging...




********** Iteration 6471000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6472000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6473000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6474000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6475000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6476000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6477000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6478000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6479000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6480000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6480001
mean reward (100 episodes) 1739.300000
best mean reward 1881.200000
running time 28274.549380
Train_EnvstepsSoFar : 6480001
Train_AverageReturn : 1739.3
Train_BestReturn : 1881.2
TimeSinceStart : 28274.549379587173
Training Loss : 0.1362629532814026
Done logging...




********** Iteration 6481000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6482000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6483000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6484000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6485000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6486000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6487000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6488000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6489000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6490000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6490001
mean reward (100 episodes) 1692.000000
best mean reward 1881.200000
running time 28318.127331
Train_EnvstepsSoFar : 6490001
Train_AverageReturn : 1692.0
Train_BestReturn : 1881.2
TimeSinceStart : 28318.127331018448
Training Loss : 0.4492987394332886
Done logging...




********** Iteration 6491000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6492000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6493000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6494000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6495000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6496000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6497000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6498000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6499000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6500000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6500001
mean reward (100 episodes) 1725.500000
best mean reward 1881.200000
running time 28363.920986
Train_EnvstepsSoFar : 6500001
Train_AverageReturn : 1725.5
Train_BestReturn : 1881.2
TimeSinceStart : 28363.9209856987
Training Loss : 1.1209282875061035
Done logging...




********** Iteration 6501000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6502000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6503000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6504000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6505000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6506000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6507000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6508000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6509000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6510000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6510001
mean reward (100 episodes) 1728.200000
best mean reward 1881.200000
running time 28407.126295
Train_EnvstepsSoFar : 6510001
Train_AverageReturn : 1728.2
Train_BestReturn : 1881.2
TimeSinceStart : 28407.12629532814
Training Loss : 0.18142548203468323
Done logging...




********** Iteration 6511000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6512000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6513000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6514000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6515000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6516000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6517000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6518000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6519000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6520000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6520001
mean reward (100 episodes) 1768.000000
best mean reward 1881.200000
running time 28452.275075
Train_EnvstepsSoFar : 6520001
Train_AverageReturn : 1768.0
Train_BestReturn : 1881.2
TimeSinceStart : 28452.27507543564
Training Loss : 0.36613929271698
Done logging...




********** Iteration 6521000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6522000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6523000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6524000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6525000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6526000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6527000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6528000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6529000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6530000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6530001
mean reward (100 episodes) 1745.100000
best mean reward 1881.200000
running time 28499.112339
Train_EnvstepsSoFar : 6530001
Train_AverageReturn : 1745.1
Train_BestReturn : 1881.2
TimeSinceStart : 28499.112339496613
Training Loss : 0.16942492127418518
Done logging...




********** Iteration 6531000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6532000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6533000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6534000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6535000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6536000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6537000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6538000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6539000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6540000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6540001
mean reward (100 episodes) 1667.400000
best mean reward 1881.200000
running time 28548.158599
Train_EnvstepsSoFar : 6540001
Train_AverageReturn : 1667.4
Train_BestReturn : 1881.2
TimeSinceStart : 28548.158598661423
Training Loss : 0.9290441870689392
Done logging...




********** Iteration 6541000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6542000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6543000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6544000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6545000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6546000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6547000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6548000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6549000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6550000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6550001
mean reward (100 episodes) 1663.000000
best mean reward 1881.200000
running time 28596.898633
Train_EnvstepsSoFar : 6550001
Train_AverageReturn : 1663.0
Train_BestReturn : 1881.2
TimeSinceStart : 28596.898633003235
Training Loss : 0.669719934463501
Done logging...




********** Iteration 6551000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6552000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6553000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6554000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6555000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6556000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6557000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6558000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6559000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6560000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6560001
mean reward (100 episodes) 1688.200000
best mean reward 1881.200000
running time 28646.338648
Train_EnvstepsSoFar : 6560001
Train_AverageReturn : 1688.2
Train_BestReturn : 1881.2
TimeSinceStart : 28646.338647842407
Training Loss : 0.24025361239910126
Done logging...




********** Iteration 6561000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6562000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6563000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6564000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6565000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6566000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6567000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6568000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6569000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6570000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6570001
mean reward (100 episodes) 1714.100000
best mean reward 1881.200000
running time 28696.023718
Train_EnvstepsSoFar : 6570001
Train_AverageReturn : 1714.1
Train_BestReturn : 1881.2
TimeSinceStart : 28696.023718357086
Training Loss : 0.18933318555355072
Done logging...




********** Iteration 6571000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6572000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6573000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6574000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6575000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6576000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6577000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6578000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6579000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6580000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6580001
mean reward (100 episodes) 1756.700000
best mean reward 1881.200000
running time 28742.679070
Train_EnvstepsSoFar : 6580001
Train_AverageReturn : 1756.7
Train_BestReturn : 1881.2
TimeSinceStart : 28742.67906999588
Training Loss : 0.49906450510025024
Done logging...




********** Iteration 6581000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6582000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6583000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6584000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6585000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6586000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6587000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6588000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6589000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6590000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6590001
mean reward (100 episodes) 1791.200000
best mean reward 1881.200000
running time 28788.137199
Train_EnvstepsSoFar : 6590001
Train_AverageReturn : 1791.2
Train_BestReturn : 1881.2
TimeSinceStart : 28788.137199163437
Training Loss : 0.2604236602783203
Done logging...




********** Iteration 6591000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6592000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6593000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6594000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6595000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6596000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6597000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6598000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6599000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6600000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6600001
mean reward (100 episodes) 1771.400000
best mean reward 1881.200000
running time 28833.883669
Train_EnvstepsSoFar : 6600001
Train_AverageReturn : 1771.4
Train_BestReturn : 1881.2
TimeSinceStart : 28833.883668661118
Training Loss : 0.16732843220233917
Done logging...




********** Iteration 6601000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6602000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6603000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6604000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6605000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6606000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6607000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6608000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6609000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6610000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6610001
mean reward (100 episodes) 1712.700000
best mean reward 1881.200000
running time 28880.338166
Train_EnvstepsSoFar : 6610001
Train_AverageReturn : 1712.7
Train_BestReturn : 1881.2
TimeSinceStart : 28880.33816599846
Training Loss : 0.17553281784057617
Done logging...




********** Iteration 6611000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6612000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6613000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6614000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6615000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6616000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6617000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6618000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6619000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6620000 ************

Training agent...

Training agent using sampled data from replay buffer...

Beginning logging procedure...
Timestep 6620001
mean reward (100 episodes) 1691.800000
best mean reward 1881.200000
running time 28926.574944
Train_EnvstepsSoFar : 6620001
Train_AverageReturn : 1691.8
Train_BestReturn : 1881.2
TimeSinceStart : 28926.57494354248
Training Loss : 0.24353580176830292
Done logging...




********** Iteration 6621000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6622000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6623000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6624000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6625000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6626000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6627000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6628000 ************

Training agent...

Training agent using sampled data from replay buffer...


********** Iteration 6629000 ************

Training agent...

Training agent using sampled data from replay buffer...
